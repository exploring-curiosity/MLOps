name: train_cluster

services:
  api:
    build:
      context: .
      dockerfile: Dockerfile.fast_api
    container_name: fastapi-server
    working_dir: /app
    volumes:
      - ../TrainingScripts:/app
      - features_data:/app/Features
    ports:
      - "9090:9090"
    environment:
      MLFLOW_TRACKING_URI: "http://129.114.26.212:8000/"
    command: >
      python main.py --host 0.0.0.0 --port 9090

  ray-head:
    build:
      context: .
      dockerfile: Dockerfile.ray_head
    image: ray-head:custom
    container_name: ray-head
    command: >
      /bin/sh -c "
        mkdir -p /tmp/ray && \
        chown -R root:root /tmp/ray && \
        ray start --head --port=6379 --dashboard-host=0.0.0.0 --block --metrics-export-port=8080
      "
    ports:
      - "6379:6379"
      - "8265:8265"
      - "8080:8080"
      - "8090:8090"
    shm_size: '24g'
    volumes:
      - ray_tmp:/tmp/ray
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_ENDPOINT_URL=${MINIO_ENDPOINT}
      - MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI}
    user: root

  ray-worker-0:
    build:
      context: ..
      dockerfile: docker/Dockerfile.ray_worker
    image: ray-worker:gpu 
    container_name: ray-worker-0
    command: ["ray", "start", "--address=ray-head:6379", "--num-cpus=32", "--num-gpus=1", "--block"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: "nvidia"
              capabilities: ["gpu"]
              device_ids: ["0"]
    shm_size: '24g'
    environment:
      - RAY_RUNTIME_ENV_CONTAINER_PROVIDER=docker
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_ENDPOINT_URL=${MINIO_ENDPOINT}
      - MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI}
    volumes:
      - features_data:/mnt
    depends_on:
      - ray-head

volumes:
  features_data:
    name: features_data
  grafana_storage:
  ray_tmp:
