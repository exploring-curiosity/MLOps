{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b12d4846-6311-41cd-a942-05357f9ccc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm                         # ← add tqdm import\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from pynvml import (\n",
    "    nvmlInit, nvmlDeviceGetHandleByIndex,\n",
    "    nvmlDeviceGetUtilizationRates, nvmlDeviceGetMemoryInfo,\n",
    "    nvmlDeviceGetTemperature, NVML_TEMPERATURE_GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6790a6b2-e34a-4aa9-ac5f-7a65064bcd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/07 00:25:56 INFO mlflow.tracking.fluent: Experiment with name 'PannsMLP' does not exist. Creating a new experiment.\n",
      "2025/05/07 00:25:57 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "mlflow.set_experiment(\"PannsMLP\")\n",
    "try:\n",
    "    mlflow.end_run()\n",
    "except:\n",
    "    pass\n",
    "mlflow.start_run(log_system_metrics=True)\n",
    "\n",
    "# log GPU info\n",
    "gpu_info = next(\n",
    "    (subprocess.run(cmd, capture_output=True, text=True).stdout\n",
    "        for cmd in [\"nvidia-smi\", \"rocm-smi\"]\n",
    "        if subprocess.run(f\"command -v {cmd}\", shell=True,\n",
    "                          capture_output=True).returncode == 0),\n",
    "    \"No GPU found.\"\n",
    ")\n",
    "mlflow.log_text(gpu_info, \"gpu-info.txt\")\n",
    "\n",
    "nvmlInit()\n",
    "gpu_handle = nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "def log_system_metrics_mlflow(step=None):\n",
    "    mlflow.log_metric(\"system.cpu.utilization\", psutil.cpu_percent(), step=step)\n",
    "    mem = psutil.virtual_memory()\n",
    "    mlflow.log_metric(\"system.memory.used\", mem.used, step=step)\n",
    "    mlflow.log_metric(\"system.memory.percent\", mem.percent, step=step)\n",
    "    gpu_util = nvmlDeviceGetUtilizationRates(gpu_handle).gpu\n",
    "    mlflow.log_metric(\"system.gpu.0.utilization\", gpu_util, step=step)\n",
    "    gpu_mem = nvmlDeviceGetMemoryInfo(gpu_handle)\n",
    "    mlflow.log_metric(\"system.gpu.0.memory.used\", gpu_mem.used, step=step)\n",
    "    mlflow.log_metric(\"system.gpu.0.memory.percent\",\n",
    "                      (gpu_mem.used / gpu_mem.total) * 100, step=step)\n",
    "    gpu_temp = nvmlDeviceGetTemperature(gpu_handle, NVML_TEMPERATURE_GPU)\n",
    "    mlflow.log_metric(\"system.gpu.0.temperature\", gpu_temp, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae864dcf-4efe-4b67-ad54-20843a4e0b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 manifest_csv: str,\n",
    "                 metadata_csv: str,\n",
    "                 feature_base: str,\n",
    "                 classes: list,\n",
    "                 embeddings_key: str = \"embedding\"):\n",
    "        \"\"\"\n",
    "        manifest_csv: path to manifest_train.csv or manifest_test.csv\n",
    "        metadata_csv: path to train.csv (for secondary_labels)\n",
    "        feature_base: e.g. \"/home/jovyan/Features\"\n",
    "        classes: global sorted list of all primary_label codes\n",
    "        \"\"\"\n",
    "        m_df = pd.read_csv(manifest_csv)\n",
    "        # build full path under Features/embeddings/\n",
    "        m_df[\"emb_path\"] = (\n",
    "            m_df[\"emb_path\"].astype(str)\n",
    "                       .str.lstrip(os.sep)\n",
    "                       .apply(lambda p: os.path.join(feature_base, \"embeddings\", p))\n",
    "        )\n",
    "\n",
    "        meta = pd.read_csv(metadata_csv, usecols=[\"filename\",\"secondary_labels\"])\n",
    "        meta[\"recording_id\"]   = meta.filename.str.replace(r\"\\.ogg$\", \"\", regex=True)\n",
    "        meta[\"secondary_list\"] = meta.secondary_labels.fillna(\"\").str.split()\n",
    "        sec_map = dict(zip(meta.recording_id, meta.secondary_list))\n",
    "\n",
    "        self.rows = []\n",
    "        for _, row in tqdm(m_df.iterrows(),\n",
    "                          total=len(m_df),\n",
    "                          desc=f\"Building {os.path.basename(manifest_csv)}\"):\n",
    "            rid  = row.chunk_id.split(\"_chk\")[0]\n",
    "            labs = [row.primary_label] + sec_map.get(rid, [])\n",
    "            self.rows.append({\n",
    "                \"emb_path\": row.emb_path,\n",
    "                \"labels\":   labs\n",
    "            })\n",
    "\n",
    "        self.classes       = classes\n",
    "        self.label2idx     = {lab:i for i, lab in enumerate(self.classes)}\n",
    "        self.num_classes   = len(self.classes)\n",
    "        self.embeddings_key = embeddings_key\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        r   = self.rows[idx]\n",
    "        arr = np.load(r[\"emb_path\"])[self.embeddings_key]   # (n_windows, emb_dim)\n",
    "        x   = arr.mean(axis=0).astype(np.float32)           # (emb_dim,)\n",
    "        y   = np.zeros(self.num_classes, dtype=np.float32)\n",
    "        for lab in r[\"labels\"]:\n",
    "            # ignore any labels not in taxonomy\n",
    "            if lab in self.label2idx:\n",
    "                y[self.label2idx[lab]] = 1.0\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3701ee26-3f0d-4ca0-afa4-54de061b071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingClassifier(nn.Module):\n",
    "    def __init__(self, emb_dim, num_classes, hidden_dims=[512,256], dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(emb_dim, hidden_dims[0])\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dims[0])\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dims[1])\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(hidden_dims[1], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.drop2(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "891ef36c-e086-4faf-8712-7b2ea7b28fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building manifest_train.csv: 100%|██████████| 108451/108451 [00:05<00:00, 21554.23it/s]\n",
      "Building manifest_test.csv: 100%|██████████| 11022/11022 [00:00<00:00, 22569.60it/s]\n"
     ]
    }
   ],
   "source": [
    "TRAIN_MANIFEST = \"/home/jovyan/Features/manifest_train.csv\"\n",
    "TEST_MANIFEST  = \"/home/jovyan/Features/manifest_test.csv\"\n",
    "TRAIN_CSV      = \"/home/jovyan/Data/birdclef-2025/train.csv\"\n",
    "FEATURE_BASE   = \"/home/jovyan/Features\"\n",
    "\n",
    "BATCH_SIZE  = 32\n",
    "LR          = 1e-3\n",
    "EPOCHS      = 20\n",
    "HIDDEN_DIMS = [512, 256]\n",
    "DROPOUT     = 0.3\n",
    "\n",
    "TAXONOMY_CSV = \"/home/jovyan/Data/birdclef-2025/taxonomy.csv\"\n",
    "tax_df = pd.read_csv(TAXONOMY_CSV)\n",
    "classes = sorted(tax_df['primary_label'].astype(str).tolist())\n",
    "\n",
    "train_ds = EmbeddingDataset(\n",
    "    manifest_csv = TRAIN_MANIFEST,\n",
    "    metadata_csv = TRAIN_CSV,\n",
    "    feature_base = FEATURE_BASE,\n",
    "    classes      = classes\n",
    ")\n",
    "test_ds = EmbeddingDataset(\n",
    "    manifest_csv = TEST_MANIFEST,\n",
    "    metadata_csv = TRAIN_CSV,\n",
    "    feature_base = FEATURE_BASE,\n",
    "    classes      = classes\n",
    ")\n",
    "\n",
    "assert train_ds.num_classes == test_ds.num_classes == len(classes)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "sample_x, _ = train_ds[0]\n",
    "emb_dim = sample_x.shape[0]\n",
    "\n",
    "model   = EmbeddingClassifier(emb_dim, train_ds.num_classes, HIDDEN_DIMS, DROPOUT).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "mlflow.log_params({\n",
    "    \"input_dim\": emb_dim,\n",
    "    \"hidden_dims\": HIDDEN_DIMS,\n",
    "    \"dropout\": DROPOUT,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"lr\": LR,\n",
    "    \"epochs\": EPOCHS\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc3dd16e-d40b-4818-854c-5b5bb8804f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train: 100%|██████████| 3390/3390 [00:30<00:00, 110.71batch/s, batch_loss=0.0231, acc=0.0992]\n",
      "Epoch 1 Test : 100%|██████████| 345/345 [00:02<00:00, 163.97batch/s, batch_loss=0.0231, acc=0.1381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 1/20  Train loss=0.0243, acc=0.0992 │ Test loss=0.0196, acc=0.1381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Train: 100%|██████████| 3390/3390 [00:32<00:00, 104.77batch/s, batch_loss=0.0168, acc=0.1990]\n",
      "Epoch 2 Test : 100%|██████████| 345/345 [00:02<00:00, 166.55batch/s, batch_loss=0.0208, acc=0.1764]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 2/20  Train loss=0.0175, acc=0.1990 │ Test loss=0.0184, acc=0.1764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Train: 100%|██████████| 3390/3390 [00:30<00:00, 112.31batch/s, batch_loss=0.0354, acc=0.2394]\n",
      "Epoch 3 Test : 100%|██████████| 345/345 [00:02<00:00, 161.09batch/s, batch_loss=0.0245, acc=0.2050]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 3/20  Train loss=0.0163, acc=0.2394 │ Test loss=0.0178, acc=0.2050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Train: 100%|██████████| 3390/3390 [00:31<00:00, 106.30batch/s, batch_loss=0.0328, acc=0.2657]\n",
      "Epoch 4 Test : 100%|██████████| 345/345 [00:02<00:00, 159.93batch/s, batch_loss=0.0231, acc=0.2102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 4/20  Train loss=0.0155, acc=0.2657 │ Test loss=0.0175, acc=0.2102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Train: 100%|██████████| 3390/3390 [00:31<00:00, 106.74batch/s, batch_loss=0.0257, acc=0.2858]\n",
      "Epoch 5 Test : 100%|██████████| 345/345 [00:02<00:00, 167.31batch/s, batch_loss=0.0221, acc=0.2261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 5/20  Train loss=0.0150, acc=0.2858 │ Test loss=0.0173, acc=0.2261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Train: 100%|██████████| 3390/3390 [00:28<00:00, 117.39batch/s, batch_loss=0.0138, acc=0.2994]\n",
      "Epoch 6 Test : 100%|██████████| 345/345 [00:02<00:00, 169.59batch/s, batch_loss=0.0215, acc=0.2373]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 6/20  Train loss=0.0147, acc=0.2994 │ Test loss=0.0172, acc=0.2373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Train: 100%|██████████| 3390/3390 [00:29<00:00, 113.77batch/s, batch_loss=0.0354, acc=0.3119]\n",
      "Epoch 7 Test : 100%|██████████| 345/345 [00:02<00:00, 158.32batch/s, batch_loss=0.0221, acc=0.2477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 7/20  Train loss=0.0143, acc=0.3119 │ Test loss=0.0169, acc=0.2477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Train: 100%|██████████| 3390/3390 [00:29<00:00, 115.18batch/s, batch_loss=0.0226, acc=0.3212]\n",
      "Epoch 8 Test : 100%|██████████| 345/345 [00:02<00:00, 158.71batch/s, batch_loss=0.0218, acc=0.2530]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 8/20  Train loss=0.0141, acc=0.3212 │ Test loss=0.0169, acc=0.2530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 Train: 100%|██████████| 3390/3390 [00:29<00:00, 116.88batch/s, batch_loss=0.0277, acc=0.3323]\n",
      "Epoch 9 Test : 100%|██████████| 345/345 [00:02<00:00, 163.74batch/s, batch_loss=0.0215, acc=0.2657]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 9/20  Train loss=0.0138, acc=0.3323 │ Test loss=0.0168, acc=0.2657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 Train: 100%|██████████| 3390/3390 [00:29<00:00, 116.10batch/s, batch_loss=0.0279, acc=0.3382]\n",
      "Epoch 10 Test : 100%|██████████| 345/345 [00:02<00:00, 159.15batch/s, batch_loss=0.0210, acc=0.2641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 10/20  Train loss=0.0136, acc=0.3382 │ Test loss=0.0167, acc=0.2641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 Train: 100%|██████████| 3390/3390 [00:29<00:00, 115.45batch/s, batch_loss=0.0206, acc=0.3452]\n",
      "Epoch 11 Test : 100%|██████████| 345/345 [00:02<00:00, 144.60batch/s, batch_loss=0.0203, acc=0.2770]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 11/20  Train loss=0.0135, acc=0.3452 │ Test loss=0.0165, acc=0.2770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 Train: 100%|██████████| 3390/3390 [00:29<00:00, 115.11batch/s, batch_loss=0.0424, acc=0.3531]\n",
      "Epoch 12 Test : 100%|██████████| 345/345 [00:02<00:00, 138.61batch/s, batch_loss=0.0199, acc=0.2775]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 12/20  Train loss=0.0133, acc=0.3531 │ Test loss=0.0164, acc=0.2775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 Train: 100%|██████████| 3390/3390 [00:29<00:00, 116.76batch/s, batch_loss=0.0236, acc=0.3580]\n",
      "Epoch 13 Test : 100%|██████████| 345/345 [00:02<00:00, 120.97batch/s, batch_loss=0.0200, acc=0.2811]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 13/20  Train loss=0.0132, acc=0.3580 │ Test loss=0.0165, acc=0.2811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 Train: 100%|██████████| 3390/3390 [00:29<00:00, 116.51batch/s, batch_loss=0.0370, acc=0.3637]\n",
      "Epoch 14 Test : 100%|██████████| 345/345 [00:02<00:00, 141.31batch/s, batch_loss=0.0205, acc=0.2766]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 14/20  Train loss=0.0130, acc=0.3637 │ Test loss=0.0166, acc=0.2766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 Train: 100%|██████████| 3390/3390 [00:31<00:00, 108.61batch/s, batch_loss=0.0588, acc=0.3686]\n",
      "Epoch 15 Test : 100%|██████████| 345/345 [00:02<00:00, 136.35batch/s, batch_loss=0.0215, acc=0.2823]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 15/20  Train loss=0.0129, acc=0.3686 │ Test loss=0.0164, acc=0.2823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 Train: 100%|██████████| 3390/3390 [00:29<00:00, 116.84batch/s, batch_loss=0.0308, acc=0.3732]\n",
      "Epoch 16 Test : 100%|██████████| 345/345 [00:02<00:00, 141.55batch/s, batch_loss=0.0192, acc=0.2824]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 16/20  Train loss=0.0128, acc=0.3732 │ Test loss=0.0164, acc=0.2824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 Train: 100%|██████████| 3390/3390 [00:29<00:00, 115.58batch/s, batch_loss=0.0292, acc=0.3779]\n",
      "Epoch 17 Test : 100%|██████████| 345/345 [00:02<00:00, 144.16batch/s, batch_loss=0.0219, acc=0.2873]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 17/20  Train loss=0.0126, acc=0.3779 │ Test loss=0.0164, acc=0.2873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 Train: 100%|██████████| 3390/3390 [00:28<00:00, 119.66batch/s, batch_loss=0.0418, acc=0.3828]\n",
      "Epoch 18 Test : 100%|██████████| 345/345 [00:02<00:00, 169.07batch/s, batch_loss=0.0190, acc=0.2917]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 18/20  Train loss=0.0126, acc=0.3828 │ Test loss=0.0163, acc=0.2917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 Train: 100%|██████████| 3390/3390 [00:28<00:00, 119.82batch/s, batch_loss=0.0264, acc=0.3865]\n",
      "Epoch 19 Test : 100%|██████████| 345/345 [00:02<00:00, 167.83batch/s, batch_loss=0.0187, acc=0.2946]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 19/20  Train loss=0.0125, acc=0.3865 │ Test loss=0.0162, acc=0.2946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 Train: 100%|██████████| 3390/3390 [00:29<00:00, 114.60batch/s, batch_loss=0.0147, acc=0.3891]\n",
      "Epoch 20 Test : 100%|██████████| 345/345 [00:02<00:00, 167.96batch/s, batch_loss=0.0192, acc=0.2866]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 20/20  Train loss=0.0124, acc=0.3891 │ Test loss=0.0163, acc=0.2866\n"
     ]
    }
   ],
   "source": [
    "best_test_acc = 0.0\n",
    "best_ckpt     = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # — Train —\n",
    "    model.train()\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch} Train\", unit=\"batch\")\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for xb, yb in train_bar:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "        preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "        correct += (preds == yb).all(dim=1).sum().item()\n",
    "        total   += xb.size(0)\n",
    "\n",
    "        train_bar.set_postfix({\n",
    "            \"batch_loss\": f\"{loss.item():.4f}\",\n",
    "            \"acc\":         f\"{correct/total:.4f}\"\n",
    "        })\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc  = correct / total\n",
    "\n",
    "    # — Test —\n",
    "    model.eval()\n",
    "    test_bar = tqdm(test_loader, desc=f\"Epoch {epoch} Test \", unit=\"batch\")\n",
    "    test_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_bar:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "\n",
    "            test_loss += loss.item() * xb.size(0)\n",
    "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "            correct += (preds == yb).all(dim=1).sum().item()\n",
    "            total   += xb.size(0)\n",
    "\n",
    "            test_bar.set_postfix({\n",
    "                \"batch_loss\": f\"{loss.item():.4f}\",\n",
    "                \"acc\":         f\"{correct/total:.4f}\"\n",
    "            })\n",
    "\n",
    "    test_loss /= total\n",
    "    test_acc   = correct / total\n",
    "\n",
    "    # checkpoint\n",
    "    ckpt = f\"ckpt_epoch_{epoch}.pt\"\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optim_state\": optimizer.state_dict(),\n",
    "        \"train_loss\": train_loss,\n",
    "        \"test_loss\": test_loss\n",
    "    }, ckpt)\n",
    "\n",
    "    # MLflow logging\n",
    "    mlflow.log_metrics({\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_accuracy\": train_acc,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_accuracy\": test_acc\n",
    "    }, step=epoch)\n",
    "    log_system_metrics_mlflow(step=epoch)\n",
    "    mlflow.log_artifact(ckpt, artifact_path=\"checkpoints\")\n",
    "\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        best_ckpt     = ckpt\n",
    "\n",
    "    print(f\"→ Epoch {epoch}/{EPOCHS}  \"\n",
    "          f\"Train loss={train_loss:.4f}, acc={train_acc:.4f} │ \"\n",
    "          f\"Test loss={test_loss:.4f}, acc={test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "468bc3b1-bada-4248-8244-87b24eff8c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/07 00:57:21 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2025/05/07 00:57:21 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run rare-stoat-788 at: http://192.5.87.49:8000/#/experiments/1/runs/89cdd90cdd88426eb2fb80fcff474108\n",
      "🧪 View experiment at: http://192.5.87.49:8000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "mlflow.log_metric(\"best_test_accuracy\", best_test_acc)\n",
    "mlflow.log_artifact(best_ckpt, artifact_path=\"model\")\n",
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
