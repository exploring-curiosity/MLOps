{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b12d4846-6311-41cd-a942-05357f9ccc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from pynvml import (\n",
    "    nvmlInit, nvmlDeviceGetHandleByIndex,\n",
    "    nvmlDeviceGetUtilizationRates, nvmlDeviceGetMemoryInfo,\n",
    "    nvmlDeviceGetTemperature, NVML_TEMPERATURE_GPU\n",
    ")\n",
    "from sklearn.metrics import f1_score, average_precision_score, precision_recall_curve\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import OneCycleLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6790a6b2-e34a-4aa9-ac5f-7a65064bcd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/07 15:59:58 INFO mlflow.tracking.fluent: Experiment with name 'PannsMLP_PrimaryLabel' does not exist. Creating a new experiment.\n",
      "2025/05/07 15:59:58 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "mlflow.set_experiment(\"PannsMLP_PrimaryLabel\")\n",
    "if mlflow.active_run(): \n",
    "    mlflow.end_run()\n",
    "mlflow.start_run(log_system_metrics=True)\n",
    "\n",
    "# log GPU/CPU info\n",
    "gpu_info = next(\n",
    "    (subprocess.run(cmd, capture_output=True, text=True).stdout\n",
    "     for cmd in [\"nvidia-smi\", \"rocm-smi\"]\n",
    "     if subprocess.run(f\"command -v {cmd}\", shell=True, capture_output=True).returncode == 0),\n",
    "    \"No GPU found.\"\n",
    ")\n",
    "mlflow.log_text(gpu_info, \"gpu-info.txt\")\n",
    "\n",
    "nvmlInit()\n",
    "gpu_handle = nvmlDeviceGetHandleByIndex(0)\n",
    "def log_sys(step=None):\n",
    "    mlflow.log_metric(\"system.cpu.utilization\", psutil.cpu_percent(), step=step)\n",
    "    mem = psutil.virtual_memory()\n",
    "    mlflow.log_metric(\"system.memory.used\", mem.used, step=step)\n",
    "    mlflow.log_metric(\"system.memory.percent\", mem.percent, step=step)\n",
    "    util = nvmlDeviceGetUtilizationRates(gpu_handle)\n",
    "    mlflow.log_metric(\"system.gpu.utilization\", util.gpu, step=step)\n",
    "    gpu_mem = nvmlDeviceGetMemoryInfo(gpu_handle)\n",
    "    mlflow.log_metric(\"system.gpu.mem.used\", gpu_mem.used, step=step)\n",
    "    mlflow.log_metric(\"system.gpu.mem.percent\", (gpu_mem.used/gpu_mem.total)*100, step=step)\n",
    "    temp = nvmlDeviceGetTemperature(gpu_handle, NVML_TEMPERATURE_GPU)\n",
    "    mlflow.log_metric(\"system.gpu.temperature\", temp, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04b712c0-1a86-45b0-8be2-5e1fc0f83da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE    = 64\n",
    "LR            = 1e-3\n",
    "WEIGHT_DECAY  = 1e-4\n",
    "EPOCHS        = 30\n",
    "HIDDEN_DIMS   = [2048, 1024, 512]\n",
    "DROPOUT       = 0.5\n",
    "MIXUP_ALPHA   = 0.4\n",
    "FOCAL_GAMMA   = 2.0\n",
    "BEST_CKPT     = \"best_emb_mlp.pt\"\n",
    "\n",
    "TAXONOMY_CSV  = \"/home/jovyan/Data/birdclef-2025/taxonomy.csv\"\n",
    "TRAIN_MAN     = \"/home/jovyan/Features/manifest_train.csv\"\n",
    "TEST_MAN      = \"/home/jovyan/Features/manifest_test.csv\"\n",
    "TRAIN_META    = \"/home/jovyan/Data/birdclef-2025/train.csv\"\n",
    "FEATURE_BASE  = \"/home/jovyan/Features\"\n",
    "\n",
    "tax_df     = pd.read_csv(TAXONOMY_CSV)\n",
    "CLASSES    = sorted(tax_df[\"primary_label\"].astype(str).tolist())\n",
    "NUM_CLASSES= len(CLASSES)\n",
    "\n",
    "mlflow.log_params({\n",
    "    \"batch_size\":   BATCH_SIZE,\n",
    "    \"lr\":           LR,\n",
    "    \"weight_decay\": WEIGHT_DECAY,\n",
    "    \"epochs\":       EPOCHS,\n",
    "    \"hidden_dims\":  HIDDEN_DIMS,\n",
    "    \"dropout\":      DROPOUT,\n",
    "    \"mixup_alpha\":  MIXUP_ALPHA,\n",
    "    \"focal_gamma\":  FOCAL_GAMMA\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae864dcf-4efe-4b67-ad54-20843a4e0b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, manifest, meta_csv, base, classes, key=\"embedding\"):\n",
    "        m = pd.read_csv(manifest)\n",
    "        m[\"emb_path\"] = (\n",
    "            m[\"emb_path\"].astype(str)\n",
    "             .str.lstrip(os.sep)\n",
    "             .apply(lambda p: os.path.join(base, \"embeddings\", p))\n",
    "        )\n",
    "        meta = pd.read_csv(meta_csv, usecols=[\"filename\",\"secondary_labels\"])\n",
    "        meta[\"rid\"]  = meta.filename.str.replace(r\"\\.ogg$\",\"\",regex=True)\n",
    "        meta[\"secs\"] = meta.secondary_labels.fillna(\"\").str.split()\n",
    "        sec_map = dict(zip(meta.rid, meta.secs))\n",
    "\n",
    "        self.rows = []\n",
    "        for _, row in tqdm(m.iterrows(), total=len(m), desc=\"Building dataset\"):\n",
    "            rid         = row.chunk_id.split(\"_chk\")[0]\n",
    "            labs        = [row.primary_label] + sec_map.get(rid, [])\n",
    "            primary_cls = row.primary_label\n",
    "            self.rows.append((row.emb_path, labs, primary_cls))\n",
    "\n",
    "        self.idx_map = {c:i for i,c in enumerate(classes)}\n",
    "        self.num_cls = len(classes)\n",
    "        self.key     = key\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        path, labs, primary_cls = self.rows[i]\n",
    "        arr = np.load(path)[self.key]                # (n_windows, emb_dim)\n",
    "        x   = arr.mean(axis=0).astype(np.float32)    # (emb_dim,)\n",
    "        y   = np.zeros(self.num_cls, dtype=np.float32)\n",
    "        for c in labs:\n",
    "            idx = self.idx_map.get(c)\n",
    "            if idx is not None:\n",
    "                y[idx] = 1.0\n",
    "        prim_idx = self.idx_map.get(primary_cls, -1)\n",
    "        return x, y, prim_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "255031df-47ac-46a0-a15b-3f59a82c1097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building dataset: 100%|██████████| 108451/108451 [00:04<00:00, 22735.91it/s]\n",
      "Building dataset: 100%|██████████| 11022/11022 [00:00<00:00, 24214.38it/s]\n"
     ]
    }
   ],
   "source": [
    "def mixup(x, y, alpha=MIXUP_ALPHA):\n",
    "    lam = np.random.beta(alpha, alpha) if alpha>0 else 1.0\n",
    "    idx = torch.randperm(x.size(0), device=x.device)\n",
    "    return lam*x + (1-lam)*x[idx], y, y[idx], lam\n",
    "\n",
    "train_ds = EmbeddingDataset(TRAIN_MAN, TRAIN_META, FEATURE_BASE, CLASSES)\n",
    "test_ds  = EmbeddingDataset(TEST_MAN,  TRAIN_META, FEATURE_BASE, CLASSES)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,  num_workers=4, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE,\n",
    "                          shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f9436fe-137f-4648-8e56-bbe297079ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingClassifier(nn.Module):\n",
    "    def __init__(self, emb_dim, num_cls):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(emb_dim,  HIDDEN_DIMS[0]),\n",
    "            nn.BatchNorm1d(HIDDEN_DIMS[0]), nn.ReLU(), nn.Dropout(DROPOUT),\n",
    "            nn.Linear(HIDDEN_DIMS[0],HIDDEN_DIMS[1]),\n",
    "            nn.BatchNorm1d(HIDDEN_DIMS[1]), nn.ReLU(), nn.Dropout(DROPOUT),\n",
    "            nn.Linear(HIDDEN_DIMS[1],HIDDEN_DIMS[2]),\n",
    "            nn.BatchNorm1d(HIDDEN_DIMS[2]), nn.ReLU(), nn.Dropout(DROPOUT),\n",
    "            nn.Linear(HIDDEN_DIMS[2],num_cls)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3701ee26-3f0d-4ca0-afa4-54de061b071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x, _, _ = train_ds[0]\n",
    "emb_dim        = sample_x.shape[0]\n",
    "model          = EmbeddingClassifier(emb_dim, NUM_CLASSES).to(DEVICE)\n",
    "mlflow.log_param(\"input_dim\", emb_dim)\n",
    "\n",
    "counts = np.zeros(NUM_CLASSES, dtype=int)\n",
    "for _, labs, _ in train_ds.rows:\n",
    "    for c in labs:\n",
    "        idx = train_ds.idx_map.get(c)\n",
    "        if idx is not None:\n",
    "            counts[idx] += 1\n",
    "n = len(train_ds)\n",
    "neg = n - counts\n",
    "pw  = np.ones(NUM_CLASSES, dtype=np.float32)\n",
    "mask= counts>0\n",
    "pw[mask] = neg[mask]/counts[mask]\n",
    "pos_weight = torch.from_numpy(pw).to(DEVICE)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, pos_weight=None):\n",
    "        super().__init__()\n",
    "        self.gamma      = gamma\n",
    "        self.pos_weight = pos_weight\n",
    "    def forward(self, logits, targets):\n",
    "        bce = F.binary_cross_entropy_with_logits(\n",
    "            logits, targets, pos_weight=self.pos_weight, reduction=\"none\"\n",
    "        )\n",
    "        p_t = torch.exp(-bce)\n",
    "        return ((1 - p_t)**self.gamma * bce).mean()\n",
    "\n",
    "criterion = FocalLoss(gamma=FOCAL_GAMMA, pos_weight=pos_weight)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=LR,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    epochs=EPOCHS,\n",
    "    pct_start=0.1,\n",
    "    div_factor=10\n",
    ")\n",
    "scaler    = GradScaler()\n",
    "\n",
    "mlflow.log_params({\n",
    "    \"optimizer\":     \"AdamW\",\n",
    "    \"weight_decay\":  WEIGHT_DECAY,\n",
    "    \"criterion\":     \"FocalLoss\",\n",
    "    \"focal_gamma\":   FOCAL_GAMMA,\n",
    "    \"scheduler\":     \"OneCycleLR\",\n",
    "    \"mixup_alpha\":   MIXUP_ALPHA\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc3dd16e-d40b-4818-854c-5b5bb8804f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1/30] Train: 100%|██████████| 1695/1695 [00:23<00:00, 71.13batch/s, loss=0.7414]\n",
      "[1/30] Eval : 100%|██████████| 173/173 [00:01<00:00, 87.77batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 1/30  F1=0.0391  AP=0.0795  PrimAcc=0.1404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2/30] Train: 100%|██████████| 1695/1695 [00:22<00:00, 74.29batch/s, loss=0.6014]\n",
      "[2/30] Eval : 100%|██████████| 173/173 [00:02<00:00, 85.39batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 2/30  F1=0.0610  AP=0.1000  PrimAcc=0.1663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3/30] Train: 100%|██████████| 1695/1695 [00:23<00:00, 72.16batch/s, loss=0.5645]\n",
      "[3/30] Eval : 100%|██████████| 173/173 [00:01<00:00, 86.94batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 3/30  F1=0.0704  AP=0.1163  PrimAcc=0.1783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[4/30] Train: 100%|██████████| 1695/1695 [00:23<00:00, 71.66batch/s, loss=0.5324]\n",
      "[4/30] Eval : 100%|██████████| 173/173 [00:02<00:00, 84.80batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 4/30  F1=0.0877  AP=0.1381  PrimAcc=0.2132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[5/30] Train: 100%|██████████| 1695/1695 [00:24<00:00, 69.22batch/s, loss=0.5105]\n",
      "[5/30] Eval : 100%|██████████| 173/173 [00:01<00:00, 86.89batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 5/30  F1=0.1006  AP=0.1512  PrimAcc=0.2226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[6/30] Train: 100%|██████████| 1695/1695 [00:23<00:00, 72.98batch/s, loss=0.5044]\n",
      "[6/30] Eval : 100%|██████████| 173/173 [00:01<00:00, 86.61batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 6/30  F1=0.0932  AP=0.1509  PrimAcc=0.2373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[7/30] Train: 100%|██████████| 1695/1695 [00:24<00:00, 68.04batch/s, loss=0.4939]\n",
      "[7/30] Eval : 100%|██████████| 173/173 [00:02<00:00, 86.34batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 7/30  F1=0.1031  AP=0.1810  PrimAcc=0.2539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[8/30] Train: 100%|██████████| 1695/1695 [00:25<00:00, 67.66batch/s, loss=0.4816]\n",
      "[8/30] Eval : 100%|██████████| 173/173 [00:01<00:00, 88.04batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 8/30  F1=0.1162  AP=0.1798  PrimAcc=0.2580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[9/30] Train: 100%|██████████| 1695/1695 [00:24<00:00, 69.00batch/s, loss=0.4762]\n",
      "[9/30] Eval : 100%|██████████| 173/173 [00:01<00:00, 86.86batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 9/30  F1=0.1144  AP=0.1965  PrimAcc=0.2717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/30] Train: 100%|██████████| 1695/1695 [00:24<00:00, 69.69batch/s, loss=0.4671]\n",
      "[10/30] Eval : 100%|██████████| 173/173 [00:02<00:00, 86.48batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 10/30  F1=0.1246  AP=0.1952  PrimAcc=0.2727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11/30] Train: 100%|██████████| 1695/1695 [00:24<00:00, 69.67batch/s, loss=0.4592]\n",
      "[11/30] Eval : 100%|██████████| 173/173 [00:02<00:00, 86.42batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 11/30  F1=0.1411  AP=0.2128  PrimAcc=0.2916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12/30] Train: 100%|██████████| 1695/1695 [00:24<00:00, 69.24batch/s, loss=0.4524]\n",
      "[12/30] Eval : 100%|██████████| 173/173 [00:02<00:00, 85.90batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 12/30  F1=0.1418  AP=0.2062  PrimAcc=0.2886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13/30] Train: 100%|██████████| 1695/1695 [00:24<00:00, 69.23batch/s, loss=0.4481]\n",
      "[13/30] Eval : 100%|██████████| 173/173 [00:01<00:00, 86.77batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 13/30  F1=0.1435  AP=0.2188  PrimAcc=0.2942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14/30] Train: 100%|██████████| 1695/1695 [00:23<00:00, 72.05batch/s, loss=0.4431]\n",
      "[14/30] Eval : 100%|██████████| 173/173 [00:01<00:00, 87.08batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 14/30  F1=0.1451  AP=0.2182  PrimAcc=0.2925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15/30] Train: 100%|██████████| 1695/1695 [00:25<00:00, 67.70batch/s, loss=0.4398]\n",
      "[15/30] Eval : 100%|██████████| 173/173 [00:02<00:00, 85.73batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 15/30  F1=0.1493  AP=0.2243  PrimAcc=0.3006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16/30] Train: 100%|██████████| 1695/1695 [00:24<00:00, 68.04batch/s, loss=0.4341]\n",
      "[16/30] Eval : 100%|██████████| 173/173 [00:01<00:00, 87.25batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 16/30  F1=0.1573  AP=0.2298  PrimAcc=0.3033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17/30] Train: 100%|██████████| 1695/1695 [00:24<00:00, 68.97batch/s, loss=0.4312]\n",
      "[17/30] Eval : 100%|██████████| 173/173 [00:02<00:00, 86.40batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 17/30  F1=0.1650  AP=0.2351  PrimAcc=0.3089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18/30] Train: 100%|██████████| 1695/1695 [00:23<00:00, 72.02batch/s, loss=0.4249]\n",
      "[18/30] Eval : 100%|██████████| 173/173 [00:01<00:00, 87.83batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 18/30  F1=0.1608  AP=0.2454  PrimAcc=0.3113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19/30] Train: 100%|██████████| 1695/1695 [00:24<00:00, 68.23batch/s, loss=0.4252]\n",
      "[19/30] Eval : 100%|██████████| 173/173 [00:01<00:00, 87.28batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 19/30  F1=0.1596  AP=0.2401  PrimAcc=0.3110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20/30] Train: 100%|██████████| 1695/1695 [00:24<00:00, 68.98batch/s, loss=0.4157]\n",
      "[20/30] Eval : 100%|██████████| 173/173 [00:01<00:00, 88.83batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 20/30  F1=0.1685  AP=0.2486  PrimAcc=0.3149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21/30] Train: 100%|██████████| 1695/1695 [00:25<00:00, 67.29batch/s, loss=0.4138]\n",
      "[21/30] Eval : 100%|██████████| 173/173 [00:02<00:00, 86.11batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 21/30  F1=0.1688  AP=0.2535  PrimAcc=0.3218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[22/30] Train: 100%|██████████| 1695/1695 [00:24<00:00, 69.05batch/s, loss=0.4099]\n",
      "[22/30] Eval : 100%|██████████| 173/173 [00:01<00:00, 86.66batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 22/30  F1=0.1645  AP=0.2493  PrimAcc=0.3207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23/30] Train: 100%|██████████| 1695/1695 [00:25<00:00, 66.89batch/s, loss=0.4116]\n",
      "[23/30] Eval : 100%|██████████| 173/173 [00:02<00:00, 70.24batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 23/30  F1=0.1708  AP=0.2579  PrimAcc=0.3275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[24/30] Train: 100%|██████████| 1695/1695 [00:24<00:00, 69.12batch/s, loss=0.4051]\n",
      "[24/30] Eval : 100%|██████████| 173/173 [00:02<00:00, 71.75batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 24/30  F1=0.1847  AP=0.2632  PrimAcc=0.3291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[25/30] Train: 100%|██████████| 1695/1695 [00:24<00:00, 69.89batch/s, loss=0.4056]\n",
      "[25/30] Eval : 100%|██████████| 173/173 [00:02<00:00, 72.98batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 25/30  F1=0.1806  AP=0.2627  PrimAcc=0.3300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[26/30] Train: 100%|██████████| 1695/1695 [00:26<00:00, 64.54batch/s, loss=0.4053]\n",
      "[26/30] Eval : 100%|██████████| 173/173 [00:02<00:00, 75.17batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 26/30  F1=0.1780  AP=0.2641  PrimAcc=0.3299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27/30] Train: 100%|██████████| 1695/1695 [00:22<00:00, 73.76batch/s, loss=0.4006]\n",
      "[27/30] Eval : 100%|██████████| 173/173 [00:01<00:00, 87.78batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 27/30  F1=0.1784  AP=0.2633  PrimAcc=0.3276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[28/30] Train: 100%|██████████| 1695/1695 [00:24<00:00, 69.39batch/s, loss=0.4036]\n",
      "[28/30] Eval : 100%|██████████| 173/173 [00:02<00:00, 86.48batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 28/30  F1=0.1813  AP=0.2651  PrimAcc=0.3297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[29/30] Train: 100%|██████████| 1695/1695 [00:24<00:00, 70.47batch/s, loss=0.4011]\n",
      "[29/30] Eval : 100%|██████████| 173/173 [00:02<00:00, 86.39batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 29/30  F1=0.1809  AP=0.2666  PrimAcc=0.3331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30/30] Train: 100%|██████████| 1695/1695 [00:24<00:00, 68.40batch/s, loss=0.4005]\n",
      "[30/30] Eval : 100%|██████████| 173/173 [00:01<00:00, 88.06batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 30/30  F1=0.1721  AP=0.2569  PrimAcc=0.3243\n"
     ]
    }
   ],
   "source": [
    "best_f1, best_ap = 0.0, 0.0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    # — Train —\n",
    "    model.train()\n",
    "    train_bar = tqdm(train_loader, desc=f\"[{epoch}/{EPOCHS}] Train\", unit=\"batch\")\n",
    "    running_loss, total = 0.0, 0\n",
    "    for xb, yb, _ in train_bar:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        xb_m, ya, yb_m, lam = mixup(xb, yb)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(device_type=\"cuda\"):\n",
    "            logits = model(xb_m)\n",
    "            loss   = lam*criterion(logits, ya) + (1-lam)*criterion(logits, yb_m)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        bs = xb.size(0)\n",
    "        running_loss += loss.item()*bs\n",
    "        total       += bs\n",
    "        train_bar.set_postfix({\"loss\": f\"{running_loss/total:.4f}\"})\n",
    "    train_loss = running_loss/total\n",
    "\n",
    "    # — Eval —\n",
    "    model.eval()\n",
    "    all_scores, all_tgts, all_prims = [], [], []\n",
    "    val_loss, total = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb, prim_idx in tqdm(test_loader, desc=f\"[{epoch}/{EPOCHS}] Eval \", unit=\"batch\"):\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            with autocast(device_type=\"cuda\"):\n",
    "                logits = model(xb)\n",
    "                val_loss += criterion(logits, yb).item()*xb.size(0)\n",
    "                scores = torch.sigmoid(logits).cpu().numpy()\n",
    "            all_scores.append(scores)\n",
    "            all_tgts.append(yb.cpu().numpy())\n",
    "            all_prims.extend(prim_idx.tolist())\n",
    "            total += xb.size(0)\n",
    "\n",
    "    val_loss /= total\n",
    "    scores   = np.vstack(all_scores)\n",
    "    tgts     = np.vstack(all_tgts)\n",
    "    prims    = np.array(all_prims, dtype=int)\n",
    "\n",
    "    # threshold calibration\n",
    "    thresholds = np.full(NUM_CLASSES, 0.5, dtype=np.float32)\n",
    "    for i in range(NUM_CLASSES):\n",
    "        y_true = tgts[:, i]\n",
    "        if 0 < y_true.sum() < len(y_true):\n",
    "            prec, rec, th = precision_recall_curve(y_true, scores[:, i])\n",
    "            f1_vals = 2 * prec * rec / (prec + rec + 1e-8)\n",
    "            best    = np.nanargmax(f1_vals[:-1])\n",
    "            thresholds[i] = th[best]\n",
    "\n",
    "    preds     = (scores >= thresholds).astype(int)\n",
    "    micro_f1  = f1_score(tgts, preds, average=\"micro\", zero_division=0)\n",
    "    micro_ap  = average_precision_score(tgts, scores, average=\"micro\")\n",
    "\n",
    "    # primary-label top-1 accuracy\n",
    "    top1      = scores.argmax(axis=1)\n",
    "    primary_acc = (top1 == prims).mean()\n",
    "\n",
    "    # checkpoint best\n",
    "    if micro_f1 > best_f1:\n",
    "        best_f1, best_ap = micro_f1, micro_ap\n",
    "        torch.save(model.state_dict(), BEST_CKPT)\n",
    "        mlflow.log_artifact(BEST_CKPT, artifact_path=\"model\")\n",
    "\n",
    "    # log metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"train_loss\":   train_loss,\n",
    "        \"val_loss\":     val_loss,\n",
    "        \"micro_f1\":     micro_f1,\n",
    "        \"micro_ap\":     micro_ap,\n",
    "        \"primary_acc\":  primary_acc\n",
    "    }, step=epoch)\n",
    "    log_sys(step=epoch)\n",
    "\n",
    "    print(f\"→ Epoch {epoch}/{EPOCHS}  \"\n",
    "          f\"F1={micro_f1:.4f}  AP={micro_ap:.4f}  PrimAcc={primary_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "468bc3b1-bada-4248-8244-87b24eff8c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/07 16:14:20 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2025/05/07 16:14:20 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run masked-tern-496 at: http://192.5.87.49:8000/#/experiments/5/runs/1cc334b1a71e43d4bc495f23034b9765\n",
      "🧪 View experiment at: http://192.5.87.49:8000/#/experiments/5\n"
     ]
    }
   ],
   "source": [
    "mlflow.log_metric(\"best_micro_f1\", best_f1)\n",
    "mlflow.log_metric(\"best_micro_ap\", best_ap)\n",
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
