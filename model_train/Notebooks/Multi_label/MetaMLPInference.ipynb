{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13bc88ec-08de-4350-8195-b0c9bf797eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import timm\n",
    "from peft import get_peft_model, LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10ebc686-6510-4234-abc7-5c2bc5df760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "FEATURE_BASE  = \"/Users/harish/Desktop/MLOps Project/features_sampled\"\n",
    "TEST_MANIFEST = os.path.join(FEATURE_BASE, \"manifest_test.csv\")\n",
    "TAXONOMY_CSV  = \"/Users/harish/Desktop/MLOps Project/Data/birdclef-2025/taxonomy.csv\"\n",
    "TRAIN_META    = \"/Users/harish/Desktop/MLOps Project/Data/birdclef-2025/train.csv\"\n",
    "DROPOUT        = 0.3\n",
    "CKPT_EMB    = \"/Users/harish/Desktop/MLOps Project/Models/best_emb_mlp.pt\"\n",
    "CKPT_RES    = \"/Users/harish/Desktop/MLOps Project/Models/best_resnet50.pt\"\n",
    "CKPT_EFF    = \"/Users/harish/Desktop/MLOps Project/Models/best_effb3_lora.pt\"\n",
    "CKPT_RAW    = \"/Users/harish/Desktop/MLOps Project/Models/best_rawcnn.pt\"\n",
    "CKPT_META   = \"/Users/harish/Desktop/MLOps Project/Models/best_meta_mlp.pt\"\n",
    "HIDDEN_DIMS    = [1024, 512]\n",
    "THRESHOLD   = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c88ada-8813-46a3-b6a7-9e3ed7364fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tax = pd.read_csv(TAXONOMY_CSV)\n",
    "CLASSES = sorted(tax[\"primary_label\"].astype(str).tolist())\n",
    "NUM_CLASSES = len(CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "986a81d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dims, dropout):\n",
    "        super().__init__()\n",
    "        layers, dims = [], [in_dim]+hidden_dims\n",
    "        for i in range(len(hidden_dims)):\n",
    "            layers += [\n",
    "                nn.Linear(dims[i], dims[i+1]),\n",
    "                nn.BatchNorm1d(dims[i+1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ]\n",
    "        layers.append(nn.Linear(dims[-1], NUM_CLASSES))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5812ec1c-2753-4325-b205-a99c5923b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EmbeddingClassifier(nn.Module):\n",
    "    def __init__(self, emb_dim, num_cls):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 2048), nn.BatchNorm1d(2048), nn.ReLU(), nn.Dropout(DROPOUT),\n",
    "            nn.Linear(2048, 1024),    nn.BatchNorm1d(1024), nn.ReLU(), nn.Dropout(DROPOUT),\n",
    "            nn.Linear(1024, 512),     nn.BatchNorm1d(512),  nn.ReLU(), nn.Dropout(DROPOUT),\n",
    "            nn.Linear(512, num_cls)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "def get_resnet50_multilabel(num_classes):\n",
    "    m = torch.hub.load('pytorch/vision:v0.14.0', 'resnet50', pretrained=False)\n",
    "    m.conv1 = nn.Conv2d(1, m.conv1.out_channels,\n",
    "                        kernel_size=m.conv1.kernel_size,\n",
    "                        stride=m.conv1.stride,\n",
    "                        padding=m.conv1.padding,\n",
    "                        bias=False)\n",
    "    m.fc    = nn.Linear(m.fc.in_features, num_classes)\n",
    "    return m\n",
    "\n",
    "TARGET_MODULES  = [\"conv_pw\",\"conv_dw\",\"conv_pwl\",\"conv_head\"]\n",
    "MODULES_TO_SAVE = [\"classifier\"]\n",
    "def build_efficientnetb3_lora(num_classes):\n",
    "    base = timm.create_model(\"efficientnet_b3\", pretrained=True)\n",
    "    # patch forward\n",
    "    orig_fwd = base.forward\n",
    "    def forward_patch(*args, input_ids=None, **kwargs):\n",
    "        x = input_ids if input_ids is not None else args[0]\n",
    "        return orig_fwd(x)\n",
    "    base.forward = forward_patch\n",
    "    # adapt stem & head\n",
    "    stem = base.conv_stem\n",
    "    base.conv_stem = nn.Conv2d(1, stem.out_channels,\n",
    "                               kernel_size=stem.kernel_size,\n",
    "                               stride=stem.stride,\n",
    "                               padding=stem.padding,\n",
    "                               bias=False)\n",
    "    base.classifier = nn.Linear(base.classifier.in_features, num_classes)\n",
    "    # LoRA\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=12, lora_alpha=24,\n",
    "        target_modules=TARGET_MODULES,\n",
    "        lora_dropout=0.1, bias=\"none\",\n",
    "        modules_to_save=MODULES_TO_SAVE,\n",
    "        task_type=\"FEATURE_EXTRACTION\",\n",
    "        inference_mode=False\n",
    "    )\n",
    "    return get_peft_model(base, lora_cfg)\n",
    "\n",
    "class RawAudioCNN(nn.Module):\n",
    "    def __init__(self, num_cls):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16,  kernel_size=15, stride=4, padding=7)\n",
    "        self.bn1   = nn.BatchNorm1d(16)\n",
    "        self.pool  = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(16,32,  kernel_size=15, stride=2, padding=7)\n",
    "        self.bn2   = nn.BatchNorm1d(32)\n",
    "        self.conv3 = nn.Conv1d(32,64,  kernel_size=15, stride=2, padding=7)\n",
    "        self.bn3   = nn.BatchNorm1d(64)\n",
    "        self.conv4 = nn.Conv1d(64,128, kernel_size=15, stride=2, padding=7)\n",
    "        self.bn4   = nn.BatchNorm1d(128)\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc          = nn.Linear(128, num_cls)\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # [B,T]→[B,1,T]\n",
    "        x = F.relu(self.bn1(self.conv1(x))); x = self.pool(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.global_pool(x).squeeze(-1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0d61ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8k/4_jk4hk138z6lhcftv5hb9nm0000gn/T/ipykernel_97662/33657588.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  emb_model.load_state_dict(torch.load(CKPT_EMB,map_location=torch.device('cpu')))\n",
      "Using cache found in /Users/harish/.cache/torch/hub/pytorch_vision_v0.14.0\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/var/folders/8k/4_jk4hk138z6lhcftv5hb9nm0000gn/T/ipykernel_97662/33657588.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  m.load_state_dict(torch.load(ckpt, map_location=torch.device('cpu')))\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingDatasetForDim:\n",
    "    def __init__(self, manifest, meta_csv, base, classes, key=\"embedding\"):\n",
    "        import pandas as pd, os\n",
    "        df = pd.read_csv(manifest)\n",
    "        # assume emb_path column already points to your .npz under base/embeddings\n",
    "        df[\"emb_path\"] = df[\"emb_path\"].astype(str) \\\n",
    "            .apply(lambda p: os.path.join(base, \"embeddings\", p.lstrip(os.sep)))\n",
    "        row = df.iloc[0]\n",
    "        arr = np.load(row.emb_path)[key]      # (n_windows, emb_dim)\n",
    "        self.emb_dim = arr.shape[1]\n",
    "\n",
    "# use it to grab emb_dim\n",
    "_emb_ds = EmbeddingDatasetForDim(TEST_MANIFEST, TRAIN_META, FEATURE_BASE, CLASSES)\n",
    "emb_dim = _emb_ds.emb_dim\n",
    "\n",
    "# now build and load your embedding model correctly:\n",
    "emb_model = EmbeddingClassifier(emb_dim=emb_dim, num_cls=NUM_CLASSES).to(DEVICE)\n",
    "emb_model.load_state_dict(torch.load(CKPT_EMB,map_location=torch.device('cpu')))\n",
    "emb_model.eval()\n",
    "\n",
    "\n",
    "# load weights & freeze\n",
    "for m, ckpt in [(emb_model,CKPT_EMB),(res_model,CKPT_RES),\n",
    "               (eff_model,CKPT_EFF),(raw_model,CKPT_RAW)]:\n",
    "    m.load_state_dict(torch.load(ckpt, map_location=torch.device('cpu')))\n",
    "    m.eval()\n",
    "    for p in m.parameters(): p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086602eb-667e-4d0a-b567-f63ff5230239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8k/4_jk4hk138z6lhcftv5hb9nm0000gn/T/ipykernel_97662/2716882300.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  meta_model.load_state_dict(torch.load(CKPT_META, map_location=DEVICE))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MetaMLP(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=824, out_features=1024, bias=True)\n",
       "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "    (4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): Dropout(p=0.3, inplace=False)\n",
       "    (8): Linear(in_features=512, out_features=206, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Embedding MLP\n",
    "# we need emb_dim: grab from one sample\n",
    "test_manifest = pd.read_csv(TEST_MANIFEST)\n",
    "sample = test_manifest.iloc[0]\n",
    "\n",
    "#1 ) Embedding MLP\n",
    "emb_path = os.path.join(FEATURE_BASE, \"embeddings\", sample.emb_path.lstrip(os.sep))\n",
    "emb_arr  = np.load(emb_path)[\"embedding\"].mean(axis=0).astype(np.float32)\n",
    "\n",
    "# 2 )ResNet50\n",
    "res_model = get_resnet50_multilabel(NUM_CLASSES).to(DEVICE)\n",
    "# 3) EffNet\n",
    "eff_model = build_efficientnetb3_lora(NUM_CLASSES).to(DEVICE)\n",
    "# 4) RawCNN\n",
    "raw_model = RawAudioCNN(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "\n",
    "# 5) Meta supervisor\n",
    "meta_model = MetaMLP(NUM_CLASSES*4, HIDDEN_DIMS, DROPOUT).to(DEVICE)\n",
    "#meta_model = MetaMLP().to(DEVICE)\n",
    "meta_model.load_state_dict(torch.load(CKPT_META, map_location=DEVICE))\n",
    "meta_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13c85fe8-4fe1-4ad4-a2b9-dc830fa8f779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding\n",
    "emb = torch.from_numpy(emb_arr).unsqueeze(0).to(DEVICE)  # [1,emb_dim]\n",
    "\n",
    "# mel‑aug (ResNet50)\n",
    "ma_path = os.path.join(FEATURE_BASE, \"mel_aug\", sample.mel_aug_path.lstrip(os.sep))\n",
    "ma_arr  = np.load(ma_path)[\"mel\"].astype(np.float32)\n",
    "ma = torch.from_numpy(ma_arr).unsqueeze(0).unsqueeze(0).to(DEVICE)  # [1,1,n_mels,n_frames]\n",
    "\n",
    "# mel (EffNetB3)\n",
    "m_path = os.path.join(FEATURE_BASE, \"mel\", sample.mel_path.lstrip(os.sep))\n",
    "m_arr  = np.load(m_path)[\"mel\"].astype(np.float32)\n",
    "m = torch.from_numpy(m_arr).unsqueeze(0).unsqueeze(0).to(DEVICE)       # [1,1,n_mels,n_frames]\n",
    "\n",
    "# raw audio\n",
    "wav_path = os.path.join(FEATURE_BASE, \"denoised\", sample.audio_path.lstrip(os.sep))\n",
    "wav, sr   = torchaudio.load(wav_path)   # [1,T]\n",
    "wav       = wav.squeeze(0)\n",
    "T         = sr * 10\n",
    "if wav.size(0)<T:\n",
    "    wav = F.pad(wav, (0, T-wav.size(0)))\n",
    "else:\n",
    "    wav = wav[:T]\n",
    "wav = (wav - wav.mean())/wav.std().clamp_min(1e-6)\n",
    "wav = wav.unsqueeze(0).to(DEVICE)       # [1,T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d206c30-8429-42a5-bd26-14c62d3e5393",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    p1 = torch.sigmoid(emb_model(emb))     # [1,NUM_CLASSES]\n",
    "    p2 = torch.sigmoid(res_model(ma))      # [1,NUM_CLASSES]\n",
    "    p3 = torch.sigmoid(eff_model(m))       # [1,NUM_CLASSES]\n",
    "    p4 = torch.sigmoid(raw_model(wav))     # [1,NUM_CLASSES]\n",
    "\n",
    "    feat   = torch.cat([p1,p2,p3,p4], dim=1)\n",
    "    logits = meta_model(feat)\n",
    "    probs  = torch.sigmoid(logits)[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f84749a1-daab-401f-89ec-36d3919bf0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multi-label predictions (prob ≥ 0.5):\n",
      "  • 1564122: 0.766\n"
     ]
    }
   ],
   "source": [
    "ml_preds = [(CLASSES[i], float(probs[i]))\n",
    "            for i in range(NUM_CLASSES) if probs[i] >= THRESHOLD]\n",
    "\n",
    "print(f\"\\nMulti-label predictions (prob ≥ {THRESHOLD}):\")\n",
    "if ml_preds:\n",
    "    for lab, sc in ml_preds:\n",
    "        print(f\"  • {lab}: {sc:.3f}\")\n",
    "else:\n",
    "    print(\"  • <none>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbcd23fa-a8c6-4383-8473-c336a12a63d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Primary‑label (top‑1) prediction:\n",
      "  → 1564122: 0.766\n"
     ]
    }
   ],
   "source": [
    "primary_idx   = int(probs.argmax())\n",
    "primary_label = CLASSES[primary_idx]\n",
    "primary_score = float(probs[primary_idx])\n",
    "\n",
    "print(f\"\\nPrimary‑label (top‑1) prediction:\")\n",
    "print(f\"  → {primary_label}: {primary_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09f5537",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
