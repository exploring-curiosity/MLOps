{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bc88ec-08de-4350-8195-b0c9bf797eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebc686-6510-4234-abc7-5c2bc5df760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "FEATURE_BASE  = \"/home/jovyan/Features\"\n",
    "TEST_MANIFEST = os.path.join(FEATURE_BASE, \"manifest_test.csv\")\n",
    "TAXONOMY_CSV  = \"/home/jovyan/Data/birdclef-2025/taxonomy.csv\"\n",
    "\n",
    "CKPT_EMB    = \"best_emb_mlp.pt\"\n",
    "CKPT_RES    = \"best_resnet50.pt\"\n",
    "CKPT_EFF    = \"best_effb3_lora.pt\"\n",
    "CKPT_RAW    = \"best_rawcnn.pt\"\n",
    "CKPT_META   = \"best_meta_mlp.pt\"\n",
    "\n",
    "THRESHOLD   = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c88ada-8813-46a3-b6a7-9e3ed7364fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tax = pd.read_csv(TAXONOMY_CSV)\n",
    "CLASSES = sorted(tax[\"primary_label\"].astype(str).tolist())\n",
    "NUM_CLASSES = len(CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5812ec1c-2753-4325-b205-a99c5923b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingClassifier(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 2048), nn.BatchNorm1d(2048), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(2048, 1024),   nn.BatchNorm1d(1024), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512),    nn.BatchNorm1d(512),  nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(512, NUM_CLASSES)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "def get_resnet50_multilabel():\n",
    "    from torchvision.models import resnet50\n",
    "    m = resnet50(weights=None)\n",
    "    m.conv1 = nn.Conv2d(1, m.conv1.out_channels,\n",
    "                        kernel_size=m.conv1.kernel_size,\n",
    "                        stride=m.conv1.stride,\n",
    "                        padding=m.conv1.padding,\n",
    "                        bias=False)\n",
    "    m.fc = nn.Linear(m.fc.in_features, NUM_CLASSES)\n",
    "    return m\n",
    "\n",
    "def build_efficientnetb3_lora():\n",
    "    import timm\n",
    "    from peft import get_peft_model, LoraConfig\n",
    "    base = timm.create_model(\"efficientnet_b3\", pretrained=True)\n",
    "    # patch forward\n",
    "    orig = base.forward\n",
    "    def fw(x, **kwargs): return orig(x)\n",
    "    base.forward = fw\n",
    "    # adapt channel & head\n",
    "    stem = base.conv_stem\n",
    "    base.conv_stem = nn.Conv2d(1, stem.out_channels, stem.kernel_size, stem.stride, stem.padding, bias=False)\n",
    "    base.classifier = nn.Linear(base.classifier.in_features, NUM_CLASSES)\n",
    "    # LoRA config must match training – but at inference it’s baked in\n",
    "    return base\n",
    "\n",
    "class RawAudioCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1,16,15,4,7); self.bn1 = nn.BatchNorm1d(16); self.pool = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(16,32,15,2,7); self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.conv3 = nn.Conv1d(32,64,15,2,7); self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.conv4 = nn.Conv1d(64,128,15,2,7);self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(128, NUM_CLASSES)\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # [B,T]→[B,1,T]\n",
    "        x = F.relu(self.bn1(self.conv1(x))); x = self.pool(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.global_pool(x).squeeze(-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class MetaMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        input_dim = 4 * NUM_CLASSES\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),       nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(256, NUM_CLASSES)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086602eb-667e-4d0a-b567-f63ff5230239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Embedding MLP\n",
    "# we need emb_dim: grab from one sample\n",
    "test_manifest = pd.read_csv(TEST_MANIFEST)\n",
    "sample = test_manifest.iloc[0]\n",
    "emb_path = os.path.join(FEATURE_BASE, \"embeddings\", sample.emb_path.lstrip(os.sep))\n",
    "emb_arr  = np.load(emb_path)[\"embedding\"].mean(axis=0).astype(np.float32)\n",
    "emb_model = EmbeddingClassifier(emb_dim=emb_arr.shape[0]).to(DEVICE)\n",
    "emb_model.load_state_dict(torch.load(CKPT_EMB, map_location=DEVICE))\n",
    "emb_model.eval()\n",
    "\n",
    "# 2) ResNet50\n",
    "res_model = get_resnet50_multilabel().to(DEVICE)\n",
    "res_model.load_state_dict(torch.load(CKPT_RES, map_location=DEVICE))\n",
    "res_model.eval()\n",
    "\n",
    "# 3) EfficientNet‑B3 LoRA (at inference just use as plain module)\n",
    "eff_model = build_efficientnetb3_lora().to(DEVICE)\n",
    "eff_model.load_state_dict(torch.load(CKPT_EFF, map_location=DEVICE))\n",
    "eff_model.eval()\n",
    "\n",
    "# 4) RawAudioCNN\n",
    "raw_model = RawAudioCNN().to(DEVICE)\n",
    "raw_model.load_state_dict(torch.load(CKPT_RAW, map_location=DEVICE))\n",
    "raw_model.eval()\n",
    "\n",
    "# 5) Meta supervisor\n",
    "meta_model = MetaMLP().to(DEVICE)\n",
    "meta_model.load_state_dict(torch.load(CKPT_META, map_location=DEVICE))\n",
    "meta_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c85fe8-4fe1-4ad4-a2b9-dc830fa8f779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding\n",
    "emb = torch.from_numpy(emb_arr).unsqueeze(0).to(DEVICE)  # [1,emb_dim]\n",
    "\n",
    "# mel‑aug (ResNet50)\n",
    "ma_path = os.path.join(FEATURE_BASE, \"mel_aug\", sample.mel_aug_path.lstrip(os.sep))\n",
    "ma_arr  = np.load(ma_path)[\"mel\"].astype(np.float32)\n",
    "ma = torch.from_numpy(ma_arr).unsqueeze(0).unsqueeze(0).to(DEVICE)  # [1,1,n_mels,n_frames]\n",
    "\n",
    "# mel (EffNetB3)\n",
    "m_path = os.path.join(FEATURE_BASE, \"mel\", sample.mel_path.lstrip(os.sep))\n",
    "m_arr  = np.load(m_path)[\"mel\"].astype(np.float32)\n",
    "m = torch.from_numpy(m_arr).unsqueeze(0).unsqueeze(0).to(DEVICE)       # [1,1,n_mels,n_frames]\n",
    "\n",
    "# raw audio\n",
    "wav_path = os.path.join(FEATURE_BASE, \"denoised\", sample.audio_path.lstrip(os.sep))\n",
    "wav, sr   = torchaudio.load(wav_path)   # [1,T]\n",
    "wav       = wav.squeeze(0)\n",
    "T         = sr * 10\n",
    "if wav.size(0)<T:\n",
    "    wav = F.pad(wav, (0, T-wav.size(0)))\n",
    "else:\n",
    "    wav = wav[:T]\n",
    "wav = (wav - wav.mean())/wav.std().clamp_min(1e-6)\n",
    "wav = wav.unsqueeze(0).to(DEVICE)       # [1,T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d206c30-8429-42a5-bd26-14c62d3e5393",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    p1 = torch.sigmoid(emb_model(emb))     # [1,NUM_CLASSES]\n",
    "    p2 = torch.sigmoid(res_model(ma))      # [1,NUM_CLASSES]\n",
    "    p3 = torch.sigmoid(eff_model(m))       # [1,NUM_CLASSES]\n",
    "    p4 = torch.sigmoid(raw_model(wav))     # [1,NUM_CLASSES]\n",
    "\n",
    "    feat   = torch.cat([p1,p2,p3,p4], dim=1)\n",
    "    logits = meta_model(feat)\n",
    "    probs  = torch.sigmoid(logits)[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84749a1-daab-401f-89ec-36d3919bf0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_preds = [(CLASSES[i], float(probs[i]))\n",
    "            for i in range(NUM_CLASSES) if probs[i] >= THRESHOLD]\n",
    "\n",
    "print(f\"\\nMulti‑label predictions (prob ≥ {THRESHOLD}):\")\n",
    "if ml_preds:\n",
    "    for lab, sc in ml_preds:\n",
    "        print(f\"  • {lab}: {sc:.3f}\")\n",
    "else:\n",
    "    print(\"  • <none>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcd23fa-a8c6-4383-8473-c336a12a63d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_idx   = int(probs.argmax())\n",
    "primary_label = CLASSES[primary_idx]\n",
    "primary_score = float(probs[primary_idx])\n",
    "\n",
    "print(f\"\\nPrimary‑label (top‑1) prediction:\")\n",
    "print(f\"  → {primary_label}: {primary_score:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
