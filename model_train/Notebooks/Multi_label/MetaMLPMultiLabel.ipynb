{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1832a709-7e15-403c-9a34-9a2569219bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from sklearn.metrics import f1_score, average_precision_score, precision_recall_curve\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from pynvml import (\n",
    "    nvmlInit, nvmlDeviceGetHandleByIndex,\n",
    "    nvmlDeviceGetUtilizationRates, nvmlDeviceGetMemoryInfo,\n",
    "    nvmlDeviceGetTemperature, NVML_TEMPERATURE_GPU\n",
    ")\n",
    "import timm\n",
    "from peft import get_peft_model, LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8cbe066-a50f-413f-bb79-36cd1a9c6690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/08 05:29:48 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n"
     ]
    }
   ],
   "source": [
    "DEVICE         = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE     = 64\n",
    "LR             = 1e-3\n",
    "WEIGHT_DECAY   = 1e-4\n",
    "EPOCHS         = 20\n",
    "HIDDEN_DIMS    = [1024, 512]\n",
    "DROPOUT        = 0.3\n",
    "FOCAL_GAMMA    = 2.0\n",
    "SAVE_EPOCH_CK  = False\n",
    "BEST_CKPT      = \"best_meta_mlp.pt\"\n",
    "THRESHOLD_INIT = 0.5\n",
    "\n",
    "FEATURE_BASE   = \"/home/jovyan/Features\"\n",
    "TRAIN_MANIFEST = os.path.join(FEATURE_BASE, \"manifest_train.csv\")\n",
    "TEST_MANIFEST  = os.path.join(FEATURE_BASE, \"manifest_test.csv\")\n",
    "TAXONOMY_CSV   = \"/home/jovyan/Data/birdclef-2025/taxonomy.csv\"\n",
    "TRAIN_META     = \"/home/jovyan/Data/birdclef-2025/train.csv\"\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ MLflow & SYSTEM METRICS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "mlflow.set_experiment(\"MetaMLP_Supervisor\")\n",
    "if mlflow.active_run(): mlflow.end_run()\n",
    "mlflow.start_run(log_system_metrics=True)\n",
    "\n",
    "gpu_info = next(\n",
    "    (subprocess.run(cmd, capture_output=True, text=True).stdout\n",
    "     for cmd in [\"nvidia-smi\",\"rocm-smi\"]\n",
    "     if subprocess.run(f\"command -v {cmd}\", shell=True,\n",
    "                       capture_output=True).returncode==0),\n",
    "    \"No GPU found.\"\n",
    ")\n",
    "mlflow.log_text(gpu_info, \"gpu-info.txt\")\n",
    "nvmlInit(); gpu_handle = nvmlDeviceGetHandleByIndex(0)\n",
    "def log_sys(step=None):\n",
    "    mlflow.log_metric(\"cpu_pct\", psutil.cpu_percent(), step=step)\n",
    "    m = psutil.virtual_memory()\n",
    "    mlflow.log_metric(\"mem_used\", m.used, step=step)\n",
    "    mlflow.log_metric(\"mem_pct\", m.percent, step=step)\n",
    "    u = nvmlDeviceGetUtilizationRates(gpu_handle)\n",
    "    mlflow.log_metric(\"gpu_util\", u.gpu, step=step)\n",
    "    gm = nvmlDeviceGetMemoryInfo(gpu_handle)\n",
    "    mlflow.log_metric(\"gpu_mem_used\", gm.used, step=step)\n",
    "    mlflow.log_metric(\"gpu_mem_pct\", (gm.used/gm.total)*100, step=step)\n",
    "    t = nvmlDeviceGetTemperature(gpu_handle, NVML_TEMPERATURE_GPU)\n",
    "    mlflow.log_metric(\"gpu_temp\", t, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03cce013-e4ca-45c6-9939-48d4bd60c559",
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_df   = pd.read_csv(TAXONOMY_CSV)\n",
    "CLASSES  = sorted(tax_df[\"primary_label\"].astype(str).tolist())\n",
    "NUM_CLS  = len(CLASSES)\n",
    "IDX_MAP  = {c:i for i,c in enumerate(CLASSES)}\n",
    "\n",
    "mlflow.log_params({\n",
    "    \"batch_size\":   BATCH_SIZE,\n",
    "    \"lr\":           LR,\n",
    "    \"weight_decay\": WEIGHT_DECAY,\n",
    "    \"epochs\":       EPOCHS,\n",
    "    \"hidden_dims\":  HIDDEN_DIMS,\n",
    "    \"dropout\":      DROPOUT,\n",
    "    \"focal_gamma\":  FOCAL_GAMMA\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70063d59-1748-425e-8bb1-44f3b71aa846",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingClassifier(nn.Module):\n",
    "    def __init__(self, emb_dim, num_cls):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 2048), nn.BatchNorm1d(2048), nn.ReLU(), nn.Dropout(DROPOUT),\n",
    "            nn.Linear(2048, 1024),    nn.BatchNorm1d(1024), nn.ReLU(), nn.Dropout(DROPOUT),\n",
    "            nn.Linear(1024, 512),     nn.BatchNorm1d(512),  nn.ReLU(), nn.Dropout(DROPOUT),\n",
    "            nn.Linear(512, num_cls)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "def get_resnet50_multilabel(num_classes):\n",
    "    m = torch.hub.load('pytorch/vision:v0.14.0', 'resnet50', pretrained=False)\n",
    "    m.conv1 = nn.Conv2d(1, m.conv1.out_channels,\n",
    "                        kernel_size=m.conv1.kernel_size,\n",
    "                        stride=m.conv1.stride,\n",
    "                        padding=m.conv1.padding,\n",
    "                        bias=False)\n",
    "    m.fc    = nn.Linear(m.fc.in_features, num_classes)\n",
    "    return m\n",
    "\n",
    "TARGET_MODULES  = [\"conv_pw\",\"conv_dw\",\"conv_pwl\",\"conv_head\"]\n",
    "MODULES_TO_SAVE = [\"classifier\"]\n",
    "def build_efficientnetb3_lora(num_classes):\n",
    "    base = timm.create_model(\"efficientnet_b3\", pretrained=True)\n",
    "    # patch forward\n",
    "    orig_fwd = base.forward\n",
    "    def forward_patch(*args, input_ids=None, **kwargs):\n",
    "        x = input_ids if input_ids is not None else args[0]\n",
    "        return orig_fwd(x)\n",
    "    base.forward = forward_patch\n",
    "    # adapt stem & head\n",
    "    stem = base.conv_stem\n",
    "    base.conv_stem = nn.Conv2d(1, stem.out_channels,\n",
    "                               kernel_size=stem.kernel_size,\n",
    "                               stride=stem.stride,\n",
    "                               padding=stem.padding,\n",
    "                               bias=False)\n",
    "    base.classifier = nn.Linear(base.classifier.in_features, num_classes)\n",
    "    # LoRA\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=12, lora_alpha=24,\n",
    "        target_modules=TARGET_MODULES,\n",
    "        lora_dropout=0.1, bias=\"none\",\n",
    "        modules_to_save=MODULES_TO_SAVE,\n",
    "        task_type=\"FEATURE_EXTRACTION\",\n",
    "        inference_mode=False\n",
    "    )\n",
    "    return get_peft_model(base, lora_cfg)\n",
    "\n",
    "class RawAudioCNN(nn.Module):\n",
    "    def __init__(self, num_cls):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16,  kernel_size=15, stride=4, padding=7)\n",
    "        self.bn1   = nn.BatchNorm1d(16)\n",
    "        self.pool  = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(16,32,  kernel_size=15, stride=2, padding=7)\n",
    "        self.bn2   = nn.BatchNorm1d(32)\n",
    "        self.conv3 = nn.Conv1d(32,64,  kernel_size=15, stride=2, padding=7)\n",
    "        self.bn3   = nn.BatchNorm1d(64)\n",
    "        self.conv4 = nn.Conv1d(64,128, kernel_size=15, stride=2, padding=7)\n",
    "        self.bn4   = nn.BatchNorm1d(128)\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc          = nn.Linear(128, num_cls)\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # [B,T]‚Üí[B,1,T]\n",
    "        x = F.relu(self.bn1(self.conv1(x))); x = self.pool(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.global_pool(x).squeeze(-1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3703ac5c-b3a2-463a-8cc5-144988a52481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jovyan/.cache/torch/hub/pytorch_vision_v0.14.0\n",
      "/opt/conda/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "CKPT_EMB = \"best_emb_mlp.pt\"\n",
    "CKPT_RES = \"best_resnet50.pt\"\n",
    "CKPT_EFF = \"best_effb3_lora.pt\"\n",
    "CKPT_RAW = \"best_rawcnn.pt\"\n",
    "\n",
    "class EmbeddingDatasetForDim:\n",
    "    def __init__(self, manifest, meta_csv, base, classes, key=\"embedding\"):\n",
    "        import pandas as pd, os\n",
    "        df = pd.read_csv(manifest)\n",
    "        # assume emb_path column already points to your .npz under base/embeddings\n",
    "        df[\"emb_path\"] = df[\"emb_path\"].astype(str) \\\n",
    "            .apply(lambda p: os.path.join(base, \"embeddings\", p.lstrip(os.sep)))\n",
    "        row = df.iloc[0]\n",
    "        arr = np.load(row.emb_path)[key]      # (n_windows, emb_dim)\n",
    "        self.emb_dim = arr.shape[1]\n",
    "\n",
    "# use it to grab emb_dim\n",
    "_emb_ds = EmbeddingDatasetForDim(TRAIN_MANIFEST, TRAIN_META, FEATURE_BASE, CLASSES)\n",
    "emb_dim = _emb_ds.emb_dim\n",
    "\n",
    "# now build and load your embedding model correctly:\n",
    "emb_model = EmbeddingClassifier(emb_dim=emb_dim, num_cls=NUM_CLS).to(DEVICE)\n",
    "emb_model.load_state_dict(torch.load(CKPT_EMB))\n",
    "emb_model.eval()\n",
    "\n",
    "# ResNet50\n",
    "res_model = get_resnet50_multilabel(NUM_CLS).to(DEVICE)\n",
    "# EffNet\n",
    "eff_model = build_efficientnetb3_lora(NUM_CLS).to(DEVICE)\n",
    "# RawCNN\n",
    "raw_model = RawAudioCNN(NUM_CLS).to(DEVICE)\n",
    "\n",
    "# load weights & freeze\n",
    "for m, ckpt in [(emb_model,CKPT_EMB),(res_model,CKPT_RES),\n",
    "               (eff_model,CKPT_EFF),(raw_model,CKPT_RAW)]:\n",
    "    m.load_state_dict(torch.load(ckpt))\n",
    "    m.eval()\n",
    "    for p in m.parameters(): p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f847a974-db06-4f73-adf4-3f9a2e29b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleDataset(Dataset):\n",
    "    def __init__(self, manifest, meta_csv, base):\n",
    "        m = pd.read_csv(manifest)\n",
    "        meta = pd.read_csv(meta_csv, usecols=[\"filename\",\"secondary_labels\"])\n",
    "        meta[\"rid\"]  = meta.filename.str.replace(r\"\\.ogg$\",\"\",regex=True)\n",
    "        meta[\"secs\"] = meta.secondary_labels.fillna(\"\").str.split()\n",
    "        sec_map = dict(zip(meta.rid, meta.secs))\n",
    "\n",
    "        self.rows = []\n",
    "        for _,r in m.iterrows():\n",
    "            rid  = r.chunk_id.split(\"_chk\")[0]\n",
    "            labs = [r.primary_label] + sec_map.get(rid,[])\n",
    "            labs = [l for l in labs if l in IDX_MAP]\n",
    "            prim = IDX_MAP[r.primary_label]\n",
    "\n",
    "            emb_p = os.path.join(base,\"embeddings\", r.emb_path.lstrip(os.sep))\n",
    "            ma_p  = os.path.join(base,\"mel_aug\",    r.mel_aug_path.lstrip(os.sep))\n",
    "            m_p   = os.path.join(base,\"mel\",        r.mel_path.lstrip(os.sep))\n",
    "            wav_p = os.path.join(base,\"denoised\",   r.audio_path.lstrip(os.sep))\n",
    "\n",
    "            yvec = np.zeros(NUM_CLS, np.float32)\n",
    "            for l in labs:\n",
    "                yvec[IDX_MAP[l]] = 1.0\n",
    "\n",
    "            self.rows.append((emb_p, ma_p, m_p, wav_p, yvec, prim))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        emb_p, ma_p, m_p, wav_p, yvec, prim = self.rows[i]\n",
    "\n",
    "        # 1) Embedding\n",
    "        emb_arr = np.load(emb_p)[\"embedding\"].mean(axis=0).astype(np.float32)\n",
    "        emb = torch.from_numpy(emb_arr)\n",
    "\n",
    "        # 2) Mel‚Äëaug\n",
    "        ma_arr = np.load(ma_p)[\"mel\"].astype(np.float32)\n",
    "        ma = torch.from_numpy(ma_arr).unsqueeze(0)  # [1, n_mels, n_frames]\n",
    "\n",
    "        # 3) Clean mel\n",
    "        m_arr = np.load(m_p)[\"mel\"].astype(np.float32)\n",
    "        m = torch.from_numpy(m_arr).unsqueeze(0)    # [1, n_mels, n_frames]\n",
    "\n",
    "        # 4) Raw waveform\n",
    "        wav, _ = torchaudio.load(wav_p)   # [1, samples]\n",
    "        wav = wav.squeeze(0)              # [samples]\n",
    "        T   = 32000 * 10\n",
    "        if wav.size(0) < T:\n",
    "            wav = F.pad(wav, (0, T - wav.size(0)))\n",
    "        else:\n",
    "            wav = wav[:T]\n",
    "        wav = (wav - wav.mean()) / wav.std().clamp_min(1e-6)  # normalize\n",
    "\n",
    "        # 5) Label vector\n",
    "        y = torch.from_numpy(yvec)\n",
    "\n",
    "        return emb, ma, m, wav, y, prim\n",
    "\n",
    "\n",
    "def ensemble_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: list of tuples (emb, mel_aug, mel, wav, y, prim)\n",
    "     - emb: [emb_dim]\n",
    "     - mel_aug, mel: [1, n_mels, n_frames]\n",
    "     - wav: [T]\n",
    "     - y: [num_classes]\n",
    "     - prim: int\n",
    "    \"\"\"\n",
    "    embs, mas, ms, wavs, ys, prims = zip(*batch)\n",
    "\n",
    "    embs = torch.stack(embs, dim=0)   # [B, emb_dim]\n",
    "    mas  = torch.stack(mas,  dim=0)   # [B, 1, n_mels, n_frames]\n",
    "    ms   = torch.stack(ms,   dim=0)   # [B, 1, n_mels, n_frames]\n",
    "    # wavs is a list of [T], stack into [B, T]\n",
    "    wavs = torch.stack(wavs, dim=0)   # [B, T]\n",
    "    ys   = torch.stack(ys,   dim=0)   # [B, num_classes]\n",
    "    prims = torch.tensor(prims, dtype=torch.long)  # [B]\n",
    "\n",
    "    return embs, mas, ms, wavs, ys, prims\n",
    "\n",
    "\n",
    "train_ds = EnsembleDataset(TRAIN_MANIFEST, TRAIN_META, FEATURE_BASE)\n",
    "test_ds  = EnsembleDataset(TEST_MANIFEST,  TRAIN_META, FEATURE_BASE)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=4, pin_memory=True,\n",
    "    collate_fn=ensemble_collate_fn\n",
    ")\n",
    "test_loader  = DataLoader(\n",
    "    test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=4, pin_memory=True,\n",
    "    collate_fn=ensemble_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72f1527f-ae5a-4d1a-8609-ded43d373d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "824"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MetaMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dims, dropout):\n",
    "        super().__init__()\n",
    "        layers, dims = [], [in_dim]+hidden_dims\n",
    "        for i in range(len(hidden_dims)):\n",
    "            layers += [\n",
    "                nn.Linear(dims[i], dims[i+1]),\n",
    "                nn.BatchNorm1d(dims[i+1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ]\n",
    "        layers.append(nn.Linear(dims[-1], NUM_CLS))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "meta_model = MetaMLP(NUM_CLS*4, HIDDEN_DIMS, DROPOUT).to(DEVICE)\n",
    "mlflow.log_param(\"meta_in_dim\", NUM_CLS*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "183d5ae2-4ea6-4d60-b57b-671124a6669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0):\n",
    "        super().__init__(); self.gamma=gamma\n",
    "    def forward(self, logits, targets):\n",
    "        bce = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\")\n",
    "        p_t = torch.exp(-bce)\n",
    "        return ((1-p_t)**self.gamma * bce).mean()\n",
    "\n",
    "criterion = FocalLoss(FOCAL_GAMMA)\n",
    "optimizer = torch.optim.AdamW(meta_model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer, max_lr=LR,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    epochs=EPOCHS, pct_start=0.1, div_factor=10\n",
    ")\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "184a3260-1de0-43d2-a5c2-f037acaea0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1/20] Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1695/1695 [04:51<00:00,  5.82batch/s, loss=0.0069]\n",
      "[1/20] Eval : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 173/173 [00:31<00:00,  5.55batch/s, loss=0.0039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Üí Epoch 1/20  F1=0.4990  AP=0.6726  PrimAcc=0.6515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2/20] Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1695/1695 [04:51<00:00,  5.81batch/s, loss=0.0009]\n",
      "[2/20] Eval : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 173/173 [00:33<00:00,  5.16batch/s, loss=0.0043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Üí Epoch 2/20  F1=0.4595  AP=0.6603  PrimAcc=0.6454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3/20] Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1695/1695 [05:14<00:00,  5.40batch/s, loss=0.0008]\n",
      "[3/20] Eval : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 173/173 [00:30<00:00,  5.76batch/s, loss=0.0047]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Üí Epoch 3/20  F1=0.5286  AP=0.6540  PrimAcc=0.6436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[4/20] Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1695/1695 [05:00<00:00,  5.65batch/s, loss=0.0007]\n",
      "[4/20] Eval : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 173/173 [00:30<00:00,  5.72batch/s, loss=0.0050]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Üí Epoch 4/20  F1=0.5010  AP=0.6615  PrimAcc=0.6504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[5/20] Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1695/1695 [05:00<00:00,  5.64batch/s, loss=0.0007]\n",
      "[5/20] Eval : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 173/173 [00:31<00:00,  5.43batch/s, loss=0.0052]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Üí Epoch 5/20  F1=0.5204  AP=0.6568  PrimAcc=0.6494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[6/20] Train:   2%|‚ñè         | 34/1695 [00:07<05:18,  5.22batch/s, loss=0.0005]Exception in thread Thread-16 (_pin_memory_loop):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/conda/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py\", line 61, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py\", line 37, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torch/multiprocessing/reductions.py\", line 541, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/multiprocessing/connection.py\", line 519, in Client\n",
      "    c = SocketClient(address)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/multiprocessing/connection.py\", line 647, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "[6/20] Train:   2%|‚ñè         | 34/1695 [00:07<06:13,  4.45batch/s, loss=0.0005]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m     loss   = criterion(logits, yb)\n\u001b[32m     26\u001b[39m scaler.scale(loss).backward()\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m scaler.update()\n\u001b[32m     29\u001b[39m scheduler.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/amp/grad_scaler.py:461\u001b[39m, in \u001b[36mGradScaler.step\u001b[39m\u001b[34m(self, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    455\u001b[39m     \u001b[38;5;28mself\u001b[39m.unscale_(optimizer)\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    458\u001b[39m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m]) > \u001b[32m0\u001b[39m\n\u001b[32m    459\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m retval = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    463\u001b[39m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m] = OptState.STEPPED\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/amp/grad_scaler.py:355\u001b[39m, in \u001b[36mGradScaler._maybe_opt_step\u001b[39m\u001b[34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    349\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m     **kwargs: Any,\n\u001b[32m    353\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    354\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf_per_device\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    356\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/amp/grad_scaler.py:355\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    349\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m     **kwargs: Any,\n\u001b[32m    353\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    354\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m    356\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "best_f1, best_ap, best_acc = 0.0, 0.0, 0.0\n",
    "thresholds = np.full(NUM_CLS, THRESHOLD_INIT, dtype=np.float32)\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    # ‚Äî Train ‚Äî\n",
    "    meta_model.train()\n",
    "    train_bar = tqdm(train_loader, desc=f\"[{epoch}/{EPOCHS}] Train\", unit=\"batch\")\n",
    "    run_loss = total = 0\n",
    "    for emb, ma, m, wav, yb, prim in train_bar:\n",
    "        emb, ma, m, wav, yb = [t.to(DEVICE) for t in (emb, ma, m, wav, yb)]\n",
    "\n",
    "        # get frozen‚Äëbase outputs\n",
    "        with torch.no_grad():\n",
    "            p1 = torch.sigmoid(emb_model(emb))\n",
    "            p2 = torch.sigmoid(res_model(ma))\n",
    "            p3 = torch.sigmoid(eff_model(m))\n",
    "            p4 = torch.sigmoid(raw_model(wav))\n",
    "\n",
    "        feat = torch.cat([p1, p2, p3, p4], dim=1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(device_type=\"cuda\"):\n",
    "            logits = meta_model(feat)\n",
    "            loss   = criterion(logits, yb)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        bs = emb.size(0)\n",
    "        run_loss += loss.item() * bs\n",
    "        total    += bs\n",
    "\n",
    "        # <-- update tqdm postfix with latest running avg loss -->\n",
    "        train_bar.set_postfix({\"loss\": f\"{run_loss/total:.4f}\"})\n",
    "\n",
    "    train_loss = run_loss / total\n",
    "\n",
    "    # ‚Äî Eval ‚Äî\n",
    "    meta_model.eval()\n",
    "    eval_bar = tqdm(test_loader, desc=f\"[{epoch}/{EPOCHS}] Eval \", unit=\"batch\")\n",
    "    val_loss = total = 0\n",
    "    all_scores, all_tgts, all_prims = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for emb, ma, m, wav, yb, prim in eval_bar:\n",
    "            emb, ma, m, wav, yb = [t.to(DEVICE) for t in (emb, ma, m, wav, yb)]\n",
    "            p1 = torch.sigmoid(emb_model(emb))\n",
    "            p2 = torch.sigmoid(res_model(ma))\n",
    "            p3 = torch.sigmoid(eff_model(m))\n",
    "            p4 = torch.sigmoid(raw_model(wav))\n",
    "            feat   = torch.cat([p1, p2, p3, p4], dim=1)\n",
    "\n",
    "            logits = meta_model(feat)\n",
    "            loss   = criterion(logits, yb)\n",
    "\n",
    "            bs = emb.size(0)\n",
    "            val_loss += loss.item() * bs\n",
    "            total    += bs\n",
    "\n",
    "            scores = logits.sigmoid().cpu().numpy()\n",
    "            all_scores.append(scores)\n",
    "            all_tgts.append(yb.cpu().numpy())\n",
    "            all_prims.extend(prim.tolist())\n",
    "\n",
    "            # <-- update tqdm postfix with latest running eval loss -->\n",
    "            eval_bar.set_postfix({\"loss\": f\"{val_loss/total:.4f}\"})\n",
    "\n",
    "    val_loss = val_loss / total\n",
    "\n",
    "    scores = np.vstack(all_scores)\n",
    "    tgts   = np.vstack(all_tgts)\n",
    "    prims  = np.array(all_prims, dtype=int)\n",
    "\n",
    "    # calibrate thresholds\n",
    "    for i in range(NUM_CLS):\n",
    "        y_true = tgts[:,i]\n",
    "        if 0<y_true.sum()<len(y_true):\n",
    "            prec,rec,th = precision_recall_curve(y_true, scores[:,i])\n",
    "            f1s = 2*prec*rec/(prec+rec+1e-8)\n",
    "            thresholds[i] = th[np.nanargmax(f1s[:-1])]\n",
    "\n",
    "    preds     = (scores>=thresholds).astype(int)\n",
    "    micro_f1  = f1_score(tgts, preds, average=\"micro\", zero_division=0)\n",
    "    micro_ap  = average_precision_score(tgts, scores, average=\"micro\")\n",
    "    primary_acc = (scores.argmax(axis=1)==prims).mean()\n",
    "\n",
    "    # checkpoint best\n",
    "    if micro_f1>best_f1:\n",
    "        best_f1,best_ap,best_acc = micro_f1,micro_ap,primary_acc\n",
    "        torch.save(meta_model.state_dict(), BEST_CKPT)\n",
    "        mlflow.log_artifact(BEST_CKPT, artifact_path=\"model\")\n",
    "\n",
    "    mlflow.log_metrics({\n",
    "        \"train_loss\":  train_loss,\n",
    "        \"val_loss\":    val_loss,\n",
    "        \"micro_f1\":    micro_f1,\n",
    "        \"micro_ap\":    micro_ap,\n",
    "        \"prim_acc\":    primary_acc\n",
    "    }, step=epoch)\n",
    "    log_sys(step=epoch)\n",
    "\n",
    "    print(f\"‚Üí Epoch {epoch}/{EPOCHS}  \"\n",
    "          f\"F1={micro_f1:.4f}  AP={micro_ap:.4f}  PrimAcc={primary_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f72698f-fbe8-424c-a52c-217e1097ea83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/08 05:58:37 WARNING mlflow.utils.requirements_utils: Found torch version (2.7.0+cu128) contains a local version label (+cu128). MLflow logged a pip requirement for this package as 'torch==2.7.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/05/08 05:58:42 WARNING mlflow.utils.requirements_utils: Found torch version (2.7.0+cu128) contains a local version label (+cu128). MLflow logged a pip requirement for this package as 'torch==2.7.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/05/08 05:58:44 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2025/05/08 05:58:44 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run masked-loon-707 at: http://129.114.26.77:8000/#/experiments/3/runs/8c75ddd9e6c748528c473e4e88f31d0b\n",
      "üß™ View experiment at: http://129.114.26.77:8000/#/experiments/3\n"
     ]
    }
   ],
   "source": [
    "mlflow.log_metric(\"best_micro_f1\", best_f1)\n",
    "mlflow.log_metric(\"best_micro_ap\", best_ap)\n",
    "mlflow.log_metric(\"best_primary_acc\", best_acc)\n",
    "\n",
    "LOCAL_MODEL_DIR = \"Meta_MLP_model\"\n",
    "mlflow.pytorch.save_model(meta_model, LOCAL_MODEL_DIR)\n",
    "mlflow.log_artifacts(LOCAL_MODEL_DIR, artifact_path=\"meta_mlp_model\")\n",
    "\n",
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
