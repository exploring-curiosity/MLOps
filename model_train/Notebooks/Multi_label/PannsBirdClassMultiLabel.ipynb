{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "962c22c7-ac80-4358-a7a8-f952ecf8d0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from pynvml import (\n",
    "    nvmlInit,\n",
    "    nvmlDeviceGetHandleByIndex,\n",
    "    nvmlDeviceGetUtilizationRates,\n",
    "    nvmlDeviceGetMemoryInfo,\n",
    "    nvmlDeviceGetTemperature,\n",
    "    NVML_TEMPERATURE_GPU\n",
    ")\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc8aec0c-a041-46eb-9f25-26e1061958b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/07 12:53:37 INFO mlflow.tracking.fluent: Experiment with name 'Panns_CNN10_Finetune' does not exist. Creating a new experiment.\n",
      "2025/05/07 12:53:37 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "mlflow.set_experiment(\"Panns_CNN10_Finetune\")\n",
    "try: mlflow.end_run()\n",
    "except: pass\n",
    "mlflow.start_run(log_system_metrics=True)\n",
    "\n",
    "# log GPU/CPU info\n",
    "gpu_info = next(\n",
    "    (subprocess.run(cmd, capture_output=True, text=True).stdout\n",
    "     for cmd in [\"nvidia-smi\",\"rocm-smi\"]\n",
    "     if subprocess.run(f\"command -v {cmd}\", shell=True,\n",
    "                       capture_output=True).returncode == 0),\n",
    "    \"No GPU found.\"\n",
    ")\n",
    "mlflow.log_text(gpu_info, \"gpu-info.txt\")\n",
    "\n",
    "nvmlInit()\n",
    "gpu_handle = nvmlDeviceGetHandleByIndex(0)\n",
    "def log_system_metrics(step=None):\n",
    "    mlflow.log_metric(\"system.cpu.utilization\", psutil.cpu_percent(), step=step)\n",
    "    mem = psutil.virtual_memory()\n",
    "    mlflow.log_metric(\"system.memory.used\", mem.used, step=step)\n",
    "    mlflow.log_metric(\"system.memory.percent\", mem.percent, step=step)\n",
    "    g = nvmlDeviceGetUtilizationRates(gpu_handle)\n",
    "    mlflow.log_metric(\"system.gpu.0.utilization\", g.gpu, step=step)\n",
    "    m = nvmlDeviceGetMemoryInfo(gpu_handle)\n",
    "    mlflow.log_metric(\"system.gpu.0.memory.used\", m.used, step=step)\n",
    "    mlflow.log_metric(\"system.gpu.0.memory.percent\",\n",
    "                      (m.used/m.total)*100, step=step)\n",
    "    t = nvmlDeviceGetTemperature(gpu_handle, NVML_TEMPERATURE_GPU)\n",
    "    mlflow.log_metric(\"system.gpu.0.temperature\", t, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "661b90cc-a53f-483c-89c6-3f2207da964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXONOMY_CSV = \"/home/jovyan/Data/birdclef-2025/taxonomy.csv\"\n",
    "tax_df       = pd.read_csv(TAXONOMY_CSV)\n",
    "CLASSES      = sorted(tax_df[\"primary_label\"].astype(str).tolist())\n",
    "NUM_CLASSES  = len(CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45d64251-f022-4ad1-8aad-2ef4de4940c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoisedDataset(Dataset):\n",
    "    def __init__(self, manifest_csv, metadata_csv, feature_base, classes,\n",
    "                 sample_rate=32000, duration=10.0):\n",
    "        m_df = pd.read_csv(manifest_csv)\n",
    "        m_df[\"audio_path\"] = (\n",
    "            m_df[\"audio_path\"].str.lstrip(os.sep)\n",
    "                 .apply(lambda p: os.path.join(feature_base, \"denoised\", p))\n",
    "        )\n",
    "        meta = pd.read_csv(metadata_csv, usecols=[\"filename\",\"secondary_labels\"])\n",
    "        meta[\"rid\"]  = meta.filename.str.replace(r\"\\.ogg$\",\"\",regex=True)\n",
    "        meta[\"secs\"] = meta.secondary_labels.fillna(\"\").str.split()\n",
    "        sec_map = dict(zip(meta.rid, meta.secs))\n",
    "\n",
    "        self.rows        = []\n",
    "        self.label2idx   = {lab:i for i, lab in enumerate(classes)}\n",
    "        self.num_classes = len(classes)\n",
    "        self.wav_len     = int(sample_rate * duration)\n",
    "\n",
    "        for _, row in tqdm(m_df.iterrows(), total=len(m_df),\n",
    "                          desc=f\"Building {os.path.basename(manifest_csv)}\"):\n",
    "            rid  = row.chunk_id.split(\"_chk\")[0]\n",
    "            labs = [row.primary_label] + sec_map.get(rid, [])\n",
    "            labs = [l for l in labs if l in self.label2idx]\n",
    "            self.rows.append({\"path\": row.audio_path, \"labels\": labs})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rec = self.rows[idx]\n",
    "        wav, sr = torchaudio.load(rec[\"path\"])  # (channels, samples)\n",
    "        wav = torch.mean(wav, dim=0)             # mono\n",
    "        if wav.size(0) < self.wav_len:\n",
    "            wav = F.pad(wav, (0, self.wav_len - wav.size(0)))\n",
    "        else:\n",
    "            wav = wav[:self.wav_len]\n",
    "        y = torch.zeros(self.num_classes, dtype=torch.float32)\n",
    "        for l in rec[\"labels\"]:\n",
    "            y[self.label2idx[l]] = 1.0\n",
    "        return wav, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3c56378-ebf0-4351-a00f-b6f0c710c5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'audioset_tagging_cnn'...\n"
     ]
    }
   ],
   "source": [
    "REPO_DIR = \"audioset_tagging_cnn\"\n",
    "if not os.path.isdir(REPO_DIR):\n",
    "    os.system(\"git clone https://github.com/qiuqiangkong/audioset_tagging_cnn.git\")\n",
    "sys.path.insert(0, os.path.join(REPO_DIR, \"pytorch\"))\n",
    "from models import Cnn10\n",
    "\n",
    "def get_panns_cnn10(num_classes, device):\n",
    "    model = Cnn10(\n",
    "        sample_rate=32000,\n",
    "        window_size=1024,\n",
    "        hop_size=320,\n",
    "        mel_bins=64,\n",
    "        fmin=50,\n",
    "        fmax=14000,\n",
    "        classes_num=num_classes\n",
    "    )\n",
    "    return model.to(device)\n",
    "\n",
    "model     = get_panns_cnn10(NUM_CLASSES, DEVICE)\n",
    "criterion = nn.BCEWithLogitsLoss()  # will override with pos_weight below\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e3cc1e6-fd97-4169-a665-d279e76cabe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building manifest_train.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 108451/108451 [00:05<00:00, 20704.89it/s]\n",
      "Building manifest_test.csv: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11022/11022 [00:00<00:00, 23479.12it/s]\n"
     ]
    }
   ],
   "source": [
    "FEATURE_BASE   = \"/home/jovyan/Features\"\n",
    "TRAIN_MANIFEST = os.path.join(FEATURE_BASE, \"manifest_train.csv\")\n",
    "TEST_MANIFEST  = os.path.join(FEATURE_BASE, \"manifest_test.csv\")\n",
    "TRAIN_CSV      = \"/home/jovyan/Data/birdclef-2025/train.csv\"\n",
    "\n",
    "BATCH_SIZE, LR, EPOCHS = 32, 1e-4, 20\n",
    "\n",
    "train_ds = DenoisedDataset(TRAIN_MANIFEST, TRAIN_CSV, FEATURE_BASE, CLASSES)\n",
    "test_ds  = DenoisedDataset(TEST_MANIFEST,  TRAIN_CSV, FEATURE_BASE, CLASSES)\n",
    "\n",
    "# compute class-wise pos_weight (fast)\n",
    "counts = np.zeros(NUM_CLASSES, dtype=np.int64)\n",
    "for row in train_ds.rows:\n",
    "    for lab in row[\"labels\"]:\n",
    "        counts[ train_ds.label2idx[lab] ] += 1\n",
    "n = len(train_ds)\n",
    "neg = n - counts\n",
    "pos_weight = np.ones(NUM_CLASSES, dtype=np.float32)\n",
    "mask = counts > 0\n",
    "pos_weight[mask] = neg[mask] / counts[mask]\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.from_numpy(pos_weight).to(DEVICE))\n",
    "\n",
    "mlflow.log_params({\n",
    "    \"model\":        \"CNN10_panns\",\n",
    "    \"input\":        \"denoised_audio\",\n",
    "    \"num_classes\":  NUM_CLASSES,\n",
    "    \"batch_size\":   BATCH_SIZE,\n",
    "    \"lr\":           LR,\n",
    "    \"epochs\":       EPOCHS\n",
    "})\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE,\n",
    "    shuffle=True,  num_workers=4, pin_memory=True\n",
    ")\n",
    "test_loader  = DataLoader(\n",
    "    test_ds,  batch_size=BATCH_SIZE,\n",
    "    shuffle=False, num_workers=4, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73625f10-55a6-4571-8bfa-cfad81a402e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3390/3390 [12:38<00:00,  4.47batch/s, loss=1.3288]\n",
      "Epoch 1 Test : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 345/345 [00:31<00:00, 10.85batch/s, loss=1.2423]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Üí Epoch 1/20  micro‚ÄëF1=0.0097\n"
     ]
    }
   ],
   "source": [
    "best_f1, best_ckpt = 0.0, None\n",
    "\n",
    "for epoch in range(1, 1+1):\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch} Train\", unit=\"batch\")\n",
    "    run_loss, total = 0.0, 0\n",
    "    for xb, yb in train_bar:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out    = model(xb)                    \n",
    "        logits = out[\"clipwise_output\"] \n",
    "        loss   = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        run_loss += loss.item() * xb.size(0)\n",
    "        total   += xb.size(0)\n",
    "        train_bar.set_postfix({\"loss\": f\"{run_loss/total:.4f}\"})\n",
    "\n",
    "    # Test + micro‚ÄëF1\n",
    "    model.eval()\n",
    "    all_preds, all_tgts = [], []\n",
    "    test_bar = tqdm(test_loader, desc=f\"Epoch {epoch} Test \", unit=\"batch\")\n",
    "    test_loss, total = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_bar:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            out    = model(xb)                    \n",
    "            logits = out[\"clipwise_output\"]\n",
    "            loss   = criterion(logits, yb)\n",
    "\n",
    "            test_loss += loss.item() * xb.size(0)\n",
    "            total    += xb.size(0)\n",
    "\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            preds = (probs >= 0.5).astype(int)\n",
    "            all_preds.append(preds)\n",
    "            all_tgts.append(yb.cpu().numpy())\n",
    "            test_bar.set_postfix({\"loss\": f\"{test_loss/total:.4f}\"})\n",
    "\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_tgts  = np.vstack(all_tgts)\n",
    "    micro_f1  = f1_score(all_tgts, all_preds, average=\"micro\", zero_division=0)\n",
    "    test_loss = test_loss / total\n",
    "\n",
    "    # Checkpoint\n",
    "    ckpt = f\"cnn10_epoch_{epoch}.pt\"\n",
    "    torch.save({\n",
    "        \"epoch\":       epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optim_state\": optimizer.state_dict(),\n",
    "        \"train_loss\":  run_loss/len(train_ds),\n",
    "        \"test_loss\":   test_loss,\n",
    "        \"micro_f1\":    micro_f1\n",
    "    }, ckpt)\n",
    "\n",
    "    mlflow.log_metrics({\n",
    "        \"train_loss\": run_loss/len(train_ds),\n",
    "        \"test_loss\":  test_loss,\n",
    "        \"micro_f1\":   micro_f1\n",
    "    }, step=epoch)\n",
    "    log_system_metrics(step=epoch)\n",
    "    mlflow.log_artifact(ckpt, artifact_path=\"checkpoints\")\n",
    "\n",
    "    if micro_f1 > best_f1:\n",
    "        best_f1, best_ckpt = micro_f1, ckpt\n",
    "\n",
    "    print(f\"‚Üí Epoch {epoch}/{EPOCHS}  micro‚ÄëF1={micro_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72e3ab94-6087-46df-993d-ab070c741e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/07 13:08:37 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2025/05/07 13:08:38 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run skillful-crow-302 at: http://192.5.87.49:8000/#/experiments/1/runs/3ff9849b6b3747ca873dbcee6e2e66db\n",
      "üß™ View experiment at: http://192.5.87.49:8000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "mlflow.log_metric(\"best_micro_f1\", best_f1)\n",
    "mlflow.log_artifact(best_ckpt, artifact_path=\"model\")\n",
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
