{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3810d717-0ce7-43e3-934e-773ec5a34d63",
   "metadata": {},
   "source": [
    "## Model and application lifecycle - Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3707ed-e60d-44d4-af2f-3003ba05f42a",
   "metadata": {},
   "source": [
    "### Run a training and evaluation job\n",
    "\n",
    "In this we will manually trigger a model training and evaluation. This manual trigger could be for several reasons like an update in the model or change in production data. \n",
    "\n",
    "Through the previous workflow, we created a worflow template in Argo Workflow named [train-model](https://github.com/exploring-curiosity/MLOps/blob/main/continous_X_pipeline/workflows/train-model.yaml), which is responsible for both training and evaluating the model. \n",
    "\n",
    "Let's look at the code to understand the flow better\n",
    "\n",
    "This template accepts 3 Public Addresses as shown here : \n",
    "spec:\n",
    "  entrypoint: training-and-build\n",
    "  arguments:\n",
    "    parameters:\n",
    "    - name: train-ip\n",
    "    - name: eval-ip\n",
    "    - name: mlflow-ip\n",
    "    \n",
    "**train-ip** is the Public IP to trigger the Training Endpoint, which is responsible for Triggering the training process of the model, and logging the model artificats and status in ML Flow. \n",
    "\n",
    "**eval-ip** is the Public IP address to trigger the Model Evaluation Endpoint. This endpoint is responsible for Evaluating the model and registering it in the MLFlow with a Specific Name. \n",
    "\n",
    "**mlflow-ip** is the Public IP where the MLFlow is accessible. It will become more clear on why we are using the MLFlow here from the code below : \n",
    "\n",
    "Through this workflow, a Training Endpoint is triggered with the help of **train-ip**. This is an API call. We get a RUN ID of the Model that is logged in the MLFlow. The following code achieves this : \n",
    "```\n",
    "RESPONSE=$(curl -f -s -X POST \" http://{{inputs.parameters.train-ip}}:9090/train?model_name=resnet50&data_source=train\")\n",
    "        CURL_EXIT_CODE=$?\n",
    "        echo \"[INFO] Training endpoint response was: $RESPONSE\" >&2\n",
    "        if [ $CURL_EXIT_CODE -ne 0 ]; then\n",
    "          echo \"[ERROR] curl failed with code $CURL_EXIT_CODE\" >&2\n",
    "          exit $CURL_EXIT_CODE\n",
    "        fi\n",
    "        echo \"[INFO] Training endpoint response was: $RESPONSE\" >&2\n",
    "\n",
    "```\n",
    "\n",
    "Now, its possible that we Model Training could take several minutes if not hours togther to train a complex model on large datasets. And HTTP Endpoint calls have a timeout of just a few minutes. \n",
    "\n",
    "Hence, the Model Training Endpoint immediately return the RUN_ID of the model which is logs in the MLFlow. \n",
    "\n",
    "As Part of this workflow, we next keep polling the MLFlow to check if the process of training the model has completed. The following code achieves just this. We extract the RUN_ID and use it to poll the [MLFlow API](https://mlflow.org/docs/latest/api_reference/rest-api.html#get-run) to track the status. \n",
    "\n",
    "```\n",
    " RUN_ID=$(echo \"$RESPONSE\" | jq -r '.run_id')    \n",
    "        if [ -z \"$RUN_ID\" ]; then\n",
    "          echo \"[ERROR] run_id not found in response\" >&2\n",
    "          exit 1\n",
    "        fi\n",
    "        echo \"[INFO] MLflow run ID: $RUN_ID\" >&2\n",
    "        \n",
    "        #Polling MLFlow\n",
    "        TERMINAL=\"FINISHED|FAILED|KILLED\"\n",
    "        while true; do\n",
    "          STATUS=$(curl -s \"http://{{inputs.parameters.mlflow-ip}}:8000/api/2.0/mlflow/runs/get?run_id=${RUN_ID}\"| jq -r '.run.info.status')\n",
    "          echo \"[INFO] Run ${RUN_ID} status: ${STATUS}\" >&2\n",
    "          case \"$STATUS\" in\n",
    "            FINISHED|FAILED|KILLED)\n",
    "              echo \"[INFO] Terminal state reached: $STATUS\" >&2\n",
    "              break\n",
    "              ;;\n",
    "          esac\n",
    "          sleep 10\n",
    "        done\n",
    "```\n",
    "\n",
    "Now that the model is ready, we next have to evaluate it and register it. Since this could also happen outside the current Kubernetes we have to trigger another Endpoint. At the end of this process we get the version of a registered model (named as 'BirdClassificationModel'). The following code demonstrates this:\n",
    "\n",
    "```\n",
    "EVAL_RESPONSE=$(curl -f -s -X GET \"http://{{inputs.parameters.eval-ip}}:8080/get-version?run_id=${RUN_ID}\")\n",
    "        CURL_EXIT_CODE=$?\n",
    "        echo \"[INFO] Evaluation endpoint response was: EVAL_RESPONSE\" >&2\n",
    "        if [ $CURL_EXIT_CODE -ne 0 ]; then\n",
    "          echo \"[ERROR] curl failed with code $CURL_EXIT_CODE\" >&2\n",
    "          exit $CURL_EXIT_CODE\n",
    "        fi\n",
    "         \n",
    "        # Extracting model version\n",
    "        VERSION=$(echo \"EVAL_RESPONSE\" | jq -r '.new_model_version // empty')\n",
    "\n",
    "        if [ -z \"$VERSION\" ]; then\n",
    "          echo \"[WARN] 'new_model_version' not found in response.\" >&2\n",
    "          exit 1\n",
    "        fi\n",
    "\n",
    "        echo -n \"$VERSION\"\n",
    "```\n",
    "\n",
    "So, we triggered model train, model evaluation and at the end got the version of the registered model. With this the model's artifacts could be downloaded from MLFlow.\n",
    "\n",
    "To actually trigger this template, we have to go to Argo Workflows > Workflow Templates > Submit > Add the train-ip, eval-ip and mlflow-ip > Hit Submit.\n",
    "\n",
    "Now that we have a new registered model, we need a new container build! \n",
    "\n",
    "This is triggered *automatically* when a new model version is returned from a training job.\n",
    "\n",
    "One the build successful, and if we trigger http://A.B.C.D:8081 , we would be accessing the latest model which we just obtained.\n",
    "\n",
    "\n",
    "This completes the critical flow of obtaining a model. So now we have our FastAPI wrapper for this model, replace the existing model with this new model (with the name bird.pth). The FastAPI just loads this model and now, this model is available to users!\n",
    "\n",
    "Let's understand that flow better in the next section!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdeb4fe-1ba4-4cc1-94a4-590a38ca13b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
