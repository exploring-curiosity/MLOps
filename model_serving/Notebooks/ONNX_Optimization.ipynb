{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96fea4f2-76f0-498f-aca0-8e9c0b483ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import pandas as pd\n",
    "import import_ipynb\n",
    "\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaed8123-50b5-45e7-a8d7-3074d6506a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test manifest loaded:\n",
      "          chunk_id                   audio_path primary_label  \\\n",
      "0    CSA34200_chk0   /1564122/CSA34200_chk0.ogg       1564122   \n",
      "1  iNat320679_chk0  /126247/iNat320679_chk0.ogg        126247   \n",
      "2    CSA18793_chk0   /1346504/CSA18793_chk0.ogg       1346504   \n",
      "3    CSA34196_chk0   /1564122/CSA34196_chk0.ogg       1564122   \n",
      "4    CSA18792_chk0   /1346504/CSA18792_chk0.ogg       1346504   \n",
      "\n",
      "                      mel_path                         emb_path  \\\n",
      "0   /1564122/CSA34200_chk0.npz   /1564122/CSA34200_chk0_emb.npz   \n",
      "1  /126247/iNat320679_chk0.npz  /126247/iNat320679_chk0_emb.npz   \n",
      "2   /1346504/CSA18793_chk0.npz   /1346504/CSA18793_chk0_emb.npz   \n",
      "3   /1564122/CSA34196_chk0.npz   /1564122/CSA34196_chk0_emb.npz   \n",
      "4   /1346504/CSA18792_chk0.npz   /1346504/CSA18792_chk0_emb.npz   \n",
      "\n",
      "                  mel_aug_path  \n",
      "0   /1564122/CSA34200_chk0.npz  \n",
      "1  /126247/iNat320679_chk0.npz  \n",
      "2   /1346504/CSA18793_chk0.npz  \n",
      "3   /1564122/CSA34196_chk0.npz  \n",
      "4   /1346504/CSA18792_chk0.npz  \n",
      "Total samples: 11022\n"
     ]
    }
   ],
   "source": [
    "FEATURE_BASE  = \"/mnt/BirdCLEF/features_sampled\"\n",
    "TEST_MANIFEST = os.path.join(FEATURE_BASE, \"manifest_test.csv\")\n",
    "\n",
    "# Load the manifest file that lists test samples\n",
    "test_manifest = pd.read_csv(TEST_MANIFEST)\n",
    "\n",
    "# Inspect a few entries\n",
    "print(\"Test manifest loaded:\")\n",
    "print(test_manifest.head())\n",
    "print(f\"Total samples: {len(test_manifest)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f90dfd8f-2b6e-4cde-b6cc-a10b3bbc441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "# Define wrapper for ONNX export\n",
    "class FusionONNXWrapper(nn.Module):\n",
    "    def __init__(self, emb_model, res_model, eff_model, raw_model, meta_model):\n",
    "        super().__init__()\n",
    "        self.emb_model = emb_model\n",
    "        self.res_model = res_model\n",
    "        self.eff_model = eff_model\n",
    "        self.raw_model = raw_model\n",
    "        self.meta_model = meta_model\n",
    "\n",
    "    def forward(self, emb, ma, m, wav):\n",
    "        p1 = torch.sigmoid(self.emb_model(emb))\n",
    "        p2 = torch.sigmoid(self.res_model(ma))\n",
    "        p3 = torch.sigmoid(self.eff_model(m))\n",
    "        p4 = torch.sigmoid(self.raw_model(wav))\n",
    "        feat = torch.cat([p1, p2, p3, p4], dim=1)\n",
    "        return torch.sigmoid(self.meta_model(feat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2141a8b-c3a3-4a1c-b20b-3773a5897238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "# === Sub-models ===\n",
    "class EmbeddingClassifier(nn.Module):\n",
    "    def __init__(self, emb_dim, num_cls):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 2048), nn.BatchNorm1d(2048), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(2048, 1024),    nn.BatchNorm1d(1024), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),     nn.BatchNorm1d(512),  nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_cls)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "def get_resnet50_multilabel(num_classes):\n",
    "    m = torch.hub.load('pytorch/vision:v0.14.0', 'resnet50', pretrained=False)\n",
    "    m.conv1 = nn.Conv2d(1, m.conv1.out_channels,\n",
    "                        kernel_size=m.conv1.kernel_size,\n",
    "                        stride=m.conv1.stride,\n",
    "                        padding=m.conv1.padding,\n",
    "                        bias=False)\n",
    "    m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
    "    return m\n",
    "\n",
    "TARGET_MODULES  = [\"conv_pw\", \"conv_dw\", \"conv_pwl\", \"conv_head\"]\n",
    "MODULES_TO_SAVE = [\"classifier\"]\n",
    "def build_efficientnetb3_lora(num_classes):\n",
    "    base = timm.create_model(\"efficientnet_b3\", pretrained=True)\n",
    "    orig_fwd = base.forward\n",
    "    def forward_patch(*args, input_ids=None, **kwargs):\n",
    "        x = input_ids if input_ids is not None else args[0]\n",
    "        return orig_fwd(x)\n",
    "    base.forward = forward_patch\n",
    "    base.conv_stem = nn.Conv2d(1, base.conv_stem.out_channels,\n",
    "                                kernel_size=base.conv_stem.kernel_size,\n",
    "                                stride=base.conv_stem.stride,\n",
    "                                padding=base.conv_stem.padding,\n",
    "                                bias=False)\n",
    "    base.classifier = nn.Linear(base.classifier.in_features, num_classes)\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=12, lora_alpha=24,\n",
    "        target_modules=TARGET_MODULES,\n",
    "        lora_dropout=0.1, bias=\"none\",\n",
    "        modules_to_save=MODULES_TO_SAVE,\n",
    "        task_type=\"FEATURE_EXTRACTION\",\n",
    "        inference_mode=False\n",
    "    )\n",
    "    return get_peft_model(base, lora_cfg)\n",
    "\n",
    "class RawAudioCNN(nn.Module):\n",
    "    def __init__(self, num_cls):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=15, stride=4, padding=7)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.pool = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=15, stride=2, padding=7)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.conv3 = nn.Conv1d(32, 64, kernel_size=15, stride=2, padding=7)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.conv4 = nn.Conv1d(64, 128, kernel_size=15, stride=2, padding=7)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(128, num_cls)\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = F.relu(self.bn1(self.conv1(x))); x = self.pool(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.global_pool(x).squeeze(-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class MetaMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dims, dropout):\n",
    "        super().__init__()\n",
    "        layers, dims = [], [in_dim] + hidden_dims\n",
    "        for i in range(len(hidden_dims)):\n",
    "            layers += [\n",
    "                nn.Linear(dims[i], dims[i+1]),\n",
    "                nn.BatchNorm1d(dims[i+1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ]\n",
    "        layers.append(nn.Linear(dims[-1], NUM_CLASSES))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class FusionONNXWrapper(nn.Module):\n",
    "    def __init__(self, emb_model, res_model, eff_model, raw_model, meta_model):\n",
    "        super().__init__()\n",
    "        self.emb_model = emb_model\n",
    "        self.res_model = res_model\n",
    "        self.eff_model = eff_model\n",
    "        self.raw_model = raw_model\n",
    "        self.meta_model = meta_model\n",
    "    def forward(self, emb, ma, m, wav):\n",
    "        p1 = torch.sigmoid(self.emb_model(emb))\n",
    "        p2 = torch.sigmoid(self.res_model(ma))\n",
    "        p3 = torch.sigmoid(self.eff_model(m))\n",
    "        p4 = torch.sigmoid(self.raw_model(wav))\n",
    "        feat = torch.cat([p1, p2, p3, p4], dim=1)\n",
    "        return torch.sigmoid(self.meta_model(feat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfeffe1f-bd11-4012-9fb7-2d15e1766302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jovyan/.cache/torch/hub/pytorch_vision_v0.14.0\n",
      "/opt/conda/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/tmp/ipykernel_23694/2409270622.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  emb_model.load_state_dict(torch.load(CKPT_EMB, map_location=DEVICE))\n",
      "/tmp/ipykernel_23694/2409270622.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  res_model.load_state_dict(torch.load(CKPT_RES, map_location=DEVICE))\n",
      "/tmp/ipykernel_23694/2409270622.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  eff_model.load_state_dict(torch.load(CKPT_EFF, map_location=DEVICE))\n",
      "/tmp/ipykernel_23694/2409270622.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  raw_model.load_state_dict(torch.load(CKPT_RAW, map_location=DEVICE))\n",
      "/tmp/ipykernel_23694/2409270622.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  meta_model.load_state_dict(torch.load(CKPT_META, map_location=DEVICE))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load class taxonomy\n",
    "TAXONOMY_CSV = \"/mnt/BirdCLEF/taxonomy.csv\"\n",
    "tax = pd.read_csv(TAXONOMY_CSV)\n",
    "CLASSES = sorted(tax[\"primary_label\"].astype(str).tolist())\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "DEVICE = torch.device(\"cpu\")  # You can switch to \"cuda\" if desired\n",
    "\n",
    "# Checkpoint paths\n",
    "\n",
    "CKPT_EMB    = \"/mnt/BirdCLEF/Models/best_emb_mlp.pt\"\n",
    "CKPT_RES    = \"/mnt/BirdCLEF/Models/best_resnet50.pt\"\n",
    "CKPT_EFF    = \"/mnt/BirdCLEF/Models/best_effb3_lora.pt\"\n",
    "CKPT_RAW    = \"/mnt/BirdCLEF/Models/best_rawcnn.pt\"\n",
    "CKPT_META   = \"/mnt/BirdCLEF/Models/best_meta_mlp.pt\"\n",
    "\n",
    "# Instantiate models\n",
    "emb_model  = EmbeddingClassifier(2048, NUM_CLASSES).to(DEVICE)\n",
    "res_model  = get_resnet50_multilabel(NUM_CLASSES).to(DEVICE)\n",
    "eff_model  = build_efficientnetb3_lora(NUM_CLASSES).to(DEVICE)\n",
    "raw_model  = RawAudioCNN(NUM_CLASSES).to(DEVICE)\n",
    "meta_model = MetaMLP(NUM_CLASSES * 4, [1024, 512], dropout=0.3).to(DEVICE)\n",
    "\n",
    "# Load weights\n",
    "emb_model.load_state_dict(torch.load(CKPT_EMB, map_location=DEVICE))\n",
    "res_model.load_state_dict(torch.load(CKPT_RES, map_location=DEVICE))\n",
    "eff_model.load_state_dict(torch.load(CKPT_EFF, map_location=DEVICE))\n",
    "raw_model.load_state_dict(torch.load(CKPT_RAW, map_location=DEVICE))\n",
    "meta_model.load_state_dict(torch.load(CKPT_META, map_location=DEVICE))\n",
    "\n",
    "# Wrap fusion model\n",
    "fusion_model = FusionONNXWrapper(\n",
    "    emb_model, res_model, eff_model, raw_model, meta_model\n",
    ").to(DEVICE).eval()\n",
    "\n",
    "# Dummy inputs for ONNX export\n",
    "dummy_emb = torch.randn(1, 2048).to(DEVICE)             # Embedding vector\n",
    "dummy_ma  = torch.randn(1, 1, 64, 313).to(DEVICE)        # Mel-spectrogram (ResNet)\n",
    "dummy_m   = torch.randn(1, 1, 64, 313).to(DEVICE)        # Mel-spectrogram (EffNet)\n",
    "dummy_wav = torch.randn(1, 320000).to(DEVICE)           # Raw audio (20 sec @ 16kHz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16ffe1fd-ea1a-4f8b-bdd6-91e4c310c8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model exported to: fusion_birdclef.onnx\n"
     ]
    }
   ],
   "source": [
    "# === Cell 6: Export Fusion Model to ONNX ===\n",
    "\n",
    "onnx_model_path = \"fusion_birdclef.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    fusion_model,\n",
    "    (dummy_emb, dummy_ma, dummy_m, dummy_wav),\n",
    "    onnx_model_path,\n",
    "    input_names=[\"emb\", \"mel_aug\", \"mel\", \"wav\"],\n",
    "    output_names=[\"probabilities\"],\n",
    "    dynamic_axes={\n",
    "        \"emb\":          {0: \"batch\"},\n",
    "        \"mel_aug\":      {0: \"batch\"},\n",
    "        \"mel\":          {0: \"batch\"},\n",
    "        \"wav\":          {0: \"batch\"},\n",
    "        \"probabilities\": {0: \"batch\"}\n",
    "    },\n",
    "    export_params=True,\n",
    "    opset_version=17,\n",
    "    do_constant_folding=True\n",
    ")\n",
    "\n",
    "print(f\"ONNX model exported to: {onnx_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7288f86f-da5d-4d97-9e75-ee5cf866427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_fusion_onnx(ort_session):\n",
    "    print(f\"Execution provider: {ort_session.get_providers()}\")\n",
    "\n",
    "    # === Accuracy is not benchmarked here due to fusion input complexity ===\n",
    "\n",
    "    # === Benchmark latency on single sample ===\n",
    "    num_trials = 100\n",
    "    dummy_emb  = np.random.randn(1, 2048).astype(np.float32)\n",
    "    dummy_ma   = np.random.randn(1, 1, 64, 313).astype(np.float32)\n",
    "    dummy_m    = np.random.randn(1, 1, 64, 313).astype(np.float32)\n",
    "    dummy_wav  = np.random.randn(1, 320000).astype(np.float32)\n",
    "\n",
    "    # Warm-up\n",
    "    ort_session.run(None, {\n",
    "        \"emb\": dummy_emb,\n",
    "        \"mel_aug\": dummy_ma,\n",
    "        \"mel\": dummy_m,\n",
    "        \"wav\": dummy_wav\n",
    "    })\n",
    "\n",
    "    latencies = []\n",
    "    for _ in range(num_trials):\n",
    "        start = time.time()\n",
    "        ort_session.run(None, {\n",
    "            \"emb\": dummy_emb,\n",
    "            \"mel_aug\": dummy_ma,\n",
    "            \"mel\": dummy_m,\n",
    "            \"wav\": dummy_wav\n",
    "        })\n",
    "        latencies.append(time.time() - start)\n",
    "\n",
    "    latencies = np.array(latencies)\n",
    "    print(f\"Inference Latency (single sample, median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Latency (single sample, 95th percentile): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Latency (single sample, 99th percentile): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Throughput (single sample): {num_trials / np.sum(latencies):.2f} FPS\")\n",
    "\n",
    "    # === Benchmark batch throughput ===\n",
    "    num_batches = 50\n",
    "    batch_size = 32\n",
    "\n",
    "    dummy_emb  = np.random.randn(batch_size, 2048).astype(np.float32)\n",
    "    dummy_ma   = np.random.randn(batch_size, 1, 64, 313).astype(np.float32)\n",
    "    dummy_m    = np.random.randn(batch_size, 1, 64, 313).astype(np.float32)\n",
    "    dummy_wav  = np.random.randn(batch_size, 320000).astype(np.float32)\n",
    "\n",
    "    # Warm-up\n",
    "    ort_session.run(None, {\n",
    "        \"emb\": dummy_emb,\n",
    "        \"mel_aug\": dummy_ma,\n",
    "        \"mel\": dummy_m,\n",
    "        \"wav\": dummy_wav\n",
    "    })\n",
    "\n",
    "    batch_times = []\n",
    "    for _ in range(num_batches):\n",
    "        start = time.time()\n",
    "        ort_session.run(None, {\n",
    "            \"emb\": dummy_emb,\n",
    "            \"mel_aug\": dummy_ma,\n",
    "            \"mel\": dummy_m,\n",
    "            \"wav\": dummy_wav\n",
    "        })\n",
    "        batch_times.append(time.time() - start)\n",
    "\n",
    "    batch_fps = (batch_size * num_batches) / np.sum(batch_times)\n",
    "    print(f\"Batch Throughput: {batch_fps:.2f} FPS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9ea3c7e-63d7-43a4-92ab-9e5a514597ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution provider: ['CPUExecutionProvider']\n",
      "Inference Latency (single sample, median): 46.45 ms\n",
      "Inference Latency (single sample, 95th percentile): 46.66 ms\n",
      "Inference Latency (single sample, 99th percentile): 47.08 ms\n",
      "Inference Throughput (single sample): 21.52 FPS\n",
      "Batch Throughput: 75.84 FPS\n"
     ]
    }
   ],
   "source": [
    "onnx_path = \"fusion_birdclef.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_path, providers=['CPUExecutionProvider'])\n",
    "benchmark_fusion_onnx(ort_session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88bfbbe0-549b-4d61-b5a9-8367d38b17ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchFile",
     "evalue": "[ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from models/food11.onnx failed:Load model models/food11.onnx failed. File doesn't exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchFile\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m onnx_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/food11.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m ort_session \u001b[38;5;241m=\u001b[39m \u001b[43mort\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43monnx_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproviders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCUDAExecutionProvider\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m benchmark_session(ort_session)\n\u001b[1;32m      4\u001b[0m ort\u001b[38;5;241m.\u001b[39mget_device()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:472\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m disabled_optimizers \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 472\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_inference_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproviders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisabled_optimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:550\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[0;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_ep_custom_ops(session_options, providers, provider_options, available_providers)\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_path:\n\u001b[0;32m--> 550\u001b[0m     sess \u001b[38;5;241m=\u001b[39m \u001b[43mC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_config_from_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    552\u001b[0m     sess \u001b[38;5;241m=\u001b[39m C\u001b[38;5;241m.\u001b[39mInferenceSession(session_options, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_bytes, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_config_from_model)\n",
      "\u001b[0;31mNoSuchFile\u001b[0m: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from models/food11.onnx failed:Load model models/food11.onnx failed. File doesn't exist"
     ]
    }
   ],
   "source": [
    "onnx_model_path = \"models/food11.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CUDAExecutionProvider'])\n",
    "benchmark_session(ort_session)\n",
    "ort.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424650da-599b-442a-aa32-1690e3d01f47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
