{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "424650da-599b-442a-aa32-1690e3d01f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jovyan/.cache/torch/hub/pytorch_vision_v0.14.0\n",
      "/opt/conda/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported: embedding_classifier.onnx\n",
      "Exported: resnet50_multilabel.onnx\n",
      "Exported: efficientnet_b3_lora.onnx\n",
      "Exported: raw_audio_cnn.onnx\n",
      "Exported: meta_mlp.onnx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import onnx\n",
    "\n",
    "# ====== Constants ======\n",
    "NUM_CLASSES = 206\n",
    "EMB_DIM = 2048\n",
    "MEL_SHAPE = (1, 64, 313)\n",
    "WAV_LEN = 320000\n",
    "FUSION_DIM = NUM_CLASSES * 4\n",
    "DROPOUT = 0.3\n",
    "EXPORT_OPSET = 20\n",
    "onnx_dir = \"onnx_exports\"\n",
    "os.makedirs(onnx_dir, exist_ok=True)\n",
    "\n",
    "# ====== Model Definitions ======\n",
    "\n",
    "class EmbeddingClassifier(nn.Module):\n",
    "    def __init__(self, emb_dim, num_cls):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 2048), nn.BatchNorm1d(2048), nn.ReLU(), nn.Dropout(DROPOUT),\n",
    "            nn.Linear(2048, 1024),    nn.BatchNorm1d(1024), nn.ReLU(), nn.Dropout(DROPOUT),\n",
    "            nn.Linear(1024, 512),     nn.BatchNorm1d(512),  nn.ReLU(), nn.Dropout(DROPOUT),\n",
    "            nn.Linear(512, num_cls)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "def get_resnet50_multilabel(num_classes):\n",
    "    m = torch.hub.load('pytorch/vision:v0.14.0', 'resnet50', pretrained=False)\n",
    "    m.conv1 = nn.Conv2d(1, m.conv1.out_channels,\n",
    "                        kernel_size=m.conv1.kernel_size,\n",
    "                        stride=m.conv1.stride,\n",
    "                        padding=m.conv1.padding,\n",
    "                        bias=False)\n",
    "    m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
    "    return m\n",
    "\n",
    "TARGET_MODULES  = [\"conv_pw\", \"conv_dw\", \"conv_pwl\", \"conv_head\"]\n",
    "MODULES_TO_SAVE = [\"classifier\"]\n",
    "\n",
    "def build_efficientnetb3_lora(num_classes):\n",
    "    base = timm.create_model(\"efficientnet_b3\", pretrained=False)\n",
    "    orig_fwd = base.forward\n",
    "    def forward_patch(*args, input_ids=None, **kwargs):\n",
    "        x = input_ids if input_ids is not None else args[0]\n",
    "        return orig_fwd(x)\n",
    "    base.forward = forward_patch\n",
    "    base.conv_stem = nn.Conv2d(1, base.conv_stem.out_channels,\n",
    "                               kernel_size=base.conv_stem.kernel_size,\n",
    "                               stride=base.conv_stem.stride,\n",
    "                               padding=base.conv_stem.padding,\n",
    "                               bias=False)\n",
    "    base.classifier = nn.Linear(base.classifier.in_features, num_classes)\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=12, lora_alpha=24,\n",
    "        target_modules=TARGET_MODULES,\n",
    "        lora_dropout=0.1, bias=\"none\",\n",
    "        modules_to_save=MODULES_TO_SAVE,\n",
    "        task_type=\"FEATURE_EXTRACTION\",\n",
    "        inference_mode=False\n",
    "    )\n",
    "    return get_peft_model(base, lora_cfg)\n",
    "\n",
    "class RawAudioCNN(nn.Module):\n",
    "    def __init__(self, num_cls):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=15, stride=4, padding=7)\n",
    "        self.bn1   = nn.BatchNorm1d(16)\n",
    "        self.pool  = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=15, stride=2, padding=7)\n",
    "        self.bn2   = nn.BatchNorm1d(32)\n",
    "        self.conv3 = nn.Conv1d(32, 64, kernel_size=15, stride=2, padding=7)\n",
    "        self.bn3   = nn.BatchNorm1d(64)\n",
    "        self.conv4 = nn.Conv1d(64, 128, kernel_size=15, stride=2, padding=7)\n",
    "        self.bn4   = nn.BatchNorm1d(128)\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(128, num_cls)\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # [B, T] -> [B, 1, T]\n",
    "        x = F.relu(self.bn1(self.conv1(x))); x = self.pool(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.global_pool(x).squeeze(-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class MetaMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dims, dropout):\n",
    "        super().__init__()\n",
    "        layers, dims = [], [in_dim] + hidden_dims\n",
    "        for i in range(len(hidden_dims)):\n",
    "            layers += [\n",
    "                nn.Linear(dims[i], dims[i+1]),\n",
    "                nn.BatchNorm1d(dims[i+1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ]\n",
    "        layers.append(nn.Linear(dims[-1], NUM_CLASSES))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ====== Instantiate Models ======\n",
    "emb_model = EmbeddingClassifier(EMB_DIM, NUM_CLASSES)\n",
    "res_model = get_resnet50_multilabel(NUM_CLASSES)\n",
    "eff_model = build_efficientnetb3_lora(NUM_CLASSES)\n",
    "raw_model = RawAudioCNN(NUM_CLASSES)\n",
    "meta_model = MetaMLP(NUM_CLASSES * 4, [1024, 512], DROPOUT)\n",
    "\n",
    "# Ensure eval mode\n",
    "for m in [emb_model, res_model, eff_model, raw_model, meta_model]:\n",
    "    m.eval()\n",
    "\n",
    "# ====== Export to ONNX ======\n",
    "\n",
    "def export_model(model, dummy_input, filename, input_name, output_name):\n",
    "    torch.onnx.export(\n",
    "        model, dummy_input, os.path.join(onnx_dir, filename),\n",
    "        input_names=[input_name], output_names=[output_name],\n",
    "        dynamic_axes={input_name: {0: \"batch_size\"}, output_name: {0: \"batch_size\"}},\n",
    "        opset_version=EXPORT_OPSET, export_params=True, do_constant_folding=True\n",
    "    )\n",
    "    print(f\"Exported: {filename}\")\n",
    "\n",
    "# Embedding MLP\n",
    "export_model(emb_model, torch.randn(1, EMB_DIM), \"embedding_classifier.onnx\", \"embedding_input\", \"embedding_output\")\n",
    "\n",
    "# ResNet50 (mel_aug)\n",
    "export_model(res_model, torch.randn(1, *MEL_SHAPE), \"resnet50_multilabel.onnx\", \"mel_aug_input\", \"resnet_output\")\n",
    "\n",
    "# EfficientNetB3 + LoRA (mel)\n",
    "export_model(eff_model, torch.randn(1, *MEL_SHAPE), \"efficientnet_b3_lora.onnx\", \"mel_input\", \"effnet_output\")\n",
    "\n",
    "# RawAudioCNN (wav)\n",
    "export_model(raw_model, torch.randn(1, WAV_LEN), \"raw_audio_cnn.onnx\", \"wav_input\", \"raw_output\")\n",
    "\n",
    "# Meta MLP\n",
    "export_model(meta_model, torch.randn(1, FUSION_DIM), \"meta_mlp.onnx\", \"fusion_input\", \"meta_output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aa60baa-3728-4dc6-ab06-6f77bf3a7e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import onnxruntime as ort\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "# File paths\n",
    "FEATURE_BASE = \"/mnt/BirdCLEF/birdclef_dataset/features_sampled\"\n",
    "TEST_MANIFEST = os.path.join(FEATURE_BASE, \"manifest_test.csv\")\n",
    "TAXONOMY_CSV = os.path.join(FEATURE_BASE, \"taxonomy.csv\")\n",
    "\n",
    "# Constants\n",
    "THRESHOLD = 0.5\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "# Load taxonomy and test manifest\n",
    "tax = pd.read_csv(TAXONOMY_CSV)\n",
    "CLASSES = sorted(tax[\"primary_label\"].astype(str).tolist())\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "test_manifest = pd.read_csv(TEST_MANIFEST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee9c354e-ba75-42d7-add9-5400b549e3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sample(sample):\n",
    "    emb_path = os.path.join(FEATURE_BASE, \"embeddings\", sample.emb_path.lstrip(os.sep))\n",
    "    emb_arr = np.load(emb_path)[\"embedding\"].mean(axis=0).astype(np.float32)\n",
    "    emb = torch.from_numpy(emb_arr).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    ma_path = os.path.join(FEATURE_BASE, \"mel_aug\", sample.mel_aug_path.lstrip(os.sep))\n",
    "    ma_arr = np.load(ma_path)[\"mel\"].astype(np.float32)\n",
    "    ma = torch.from_numpy(ma_arr).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    m_path = os.path.join(FEATURE_BASE, \"mel\", sample.mel_path.lstrip(os.sep))\n",
    "    m_arr = np.load(m_path)[\"mel\"].astype(np.float32)\n",
    "    m = torch.from_numpy(m_arr).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    wav_path = os.path.join(FEATURE_BASE, \"denoised\", sample.audio_path.lstrip(os.sep))\n",
    "    wav, sr = torchaudio.load(wav_path)\n",
    "    wav = wav.float().squeeze(0)\n",
    "    T = sr * 10  # 20 seconds\n",
    "    if wav.size(0) < T:\n",
    "        wav = F.pad(wav, (0, T - wav.size(0)))\n",
    "    else:\n",
    "        wav = wav[:T]\n",
    "    wav = (wav - wav.mean()) / wav.std().clamp_min(1e-6)\n",
    "    wav = wav.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    return emb, ma, m, wav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffb11c90-d74b-4628-aa52-fd063173d125",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_dir = \"onnx_exports\"\n",
    "model_paths = {\n",
    "    \"embedding\": os.path.join(onnx_dir, \"embedding_classifier.onnx\"),\n",
    "    \"resnet\":    os.path.join(onnx_dir, \"resnet50_multilabel.onnx\"),\n",
    "    \"effnet\":    os.path.join(onnx_dir, \"efficientnet_b3_lora.onnx\"),\n",
    "    \"rawaudio\":  os.path.join(onnx_dir, \"raw_audio_cnn.onnx\"),\n",
    "    \"meta\":      os.path.join(onnx_dir, \"meta_mlp.onnx\")\n",
    "}\n",
    "\n",
    "sessions = {k: ort.InferenceSession(v, providers=[\"CPUExecutionProvider\"]) for k, v in model_paths.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5ac5e6f-e7e9-4206-877b-abf2c069b02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Sizes on Disk:\n",
      "embedding   : 27.73 MB\n",
      "resnet      : 95.52 MB\n",
      "effnet      : 63.34 MB\n",
      "rawaudio    : 0.76 MB\n",
      "meta        : 5.92 MB\n",
      "Total ONNX Model Size: 193.27 MB\n"
     ]
    }
   ],
   "source": [
    "# === Model Size on Disk ===\n",
    "print(\"\\nModel Sizes on Disk:\")\n",
    "total_size = 0\n",
    "for name, path in model_paths.items():\n",
    "    size = os.path.getsize(path)\n",
    "    total_size += size\n",
    "    print(f\"{name:<12}: {size / 1e6:.2f} MB\")\n",
    "print(f\"Total ONNX Model Size: {total_size / 1e6:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c413eb8-6320-4263-b301-fa7a2f73199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = test_manifest.iloc[0]\n",
    "emb, ma, m, wav = preprocess_sample(sample)\n",
    "\n",
    "# Convert to NumPy for ONNX\n",
    "emb_np = emb.cpu().numpy()\n",
    "ma_np = ma.cpu().numpy()\n",
    "m_np = m.cpu().numpy()\n",
    "wav_np = wav.cpu().numpy()\n",
    "\n",
    "# Run through all submodels\n",
    "p1 = sessions[\"embedding\"].run(None, {\"embedding_input\": emb_np})[0]\n",
    "p2 = sessions[\"resnet\"].run(None, {\"mel_aug_input\": ma_np})[0]\n",
    "p3 = sessions[\"effnet\"].run(None, {\"mel_input\": m_np})[0]\n",
    "p4 = sessions[\"rawaudio\"].run(None, {\"wav_input\": wav_np})[0]\n",
    "\n",
    "fusion_input = np.concatenate([p1, p2, p3, p4], axis=1)\n",
    "meta_out = sessions[\"meta\"].run(None, {\"fusion_input\": fusion_input})[0]\n",
    "probs = sigmoid(meta_out[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8114518-bfa2-4694-b44e-f811eb592b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Prediction: 1139490 (1.000)\n",
      "\n",
      "Multi-label Predictions (≥ 0.5):\n",
      "1139490         | Confidence: 1.000\n",
      "1192948         | Confidence: 1.000\n",
      "126247          | Confidence: 0.972\n",
      "134933          | Confidence: 1.000\n",
      "1462711         | Confidence: 0.999\n",
      "1462737         | Confidence: 1.000\n",
      "21038           | Confidence: 0.999\n",
      "21116           | Confidence: 1.000\n",
      "22333           | Confidence: 0.919\n",
      "22973           | Confidence: 1.000\n",
      "22976           | Confidence: 1.000\n",
      "24272           | Confidence: 1.000\n",
      "24322           | Confidence: 0.899\n",
      "41663           | Confidence: 1.000\n",
      "41970           | Confidence: 1.000\n",
      "42087           | Confidence: 1.000\n",
      "42113           | Confidence: 1.000\n",
      "46010           | Confidence: 0.997\n",
      "47067           | Confidence: 1.000\n",
      "476538          | Confidence: 1.000\n",
      "50186           | Confidence: 1.000\n",
      "517119          | Confidence: 1.000\n",
      "523060          | Confidence: 1.000\n",
      "528041          | Confidence: 0.969\n",
      "52884           | Confidence: 1.000\n",
      "548639          | Confidence: 0.975\n",
      "555142          | Confidence: 1.000\n",
      "566513          | Confidence: 0.856\n",
      "64862           | Confidence: 1.000\n",
      "65419           | Confidence: 0.999\n",
      "65547           | Confidence: 0.989\n",
      "66531           | Confidence: 1.000\n",
      "66893           | Confidence: 1.000\n",
      "67082           | Confidence: 1.000\n",
      "715170          | Confidence: 1.000\n",
      "868458          | Confidence: 1.000\n",
      "amakin1         | Confidence: 1.000\n",
      "bafibi1         | Confidence: 1.000\n",
      "banana          | Confidence: 0.869\n",
      "bkcdon          | Confidence: 1.000\n",
      "bkmtou1         | Confidence: 1.000\n",
      "blbgra1         | Confidence: 1.000\n",
      "blcant4         | Confidence: 1.000\n",
      "blchaw1         | Confidence: 1.000\n",
      "blcjay1         | Confidence: 1.000\n",
      "blctit1         | Confidence: 1.000\n",
      "blhpar1         | Confidence: 1.000\n",
      "blkvul          | Confidence: 1.000\n",
      "bobfly1         | Confidence: 1.000\n",
      "bubcur1         | Confidence: 0.805\n",
      "bubwre1         | Confidence: 1.000\n",
      "bucmot3         | Confidence: 1.000\n",
      "butsal1         | Confidence: 1.000\n",
      "cargra1         | Confidence: 1.000\n",
      "cinbec1         | Confidence: 1.000\n",
      "cocher1         | Confidence: 1.000\n",
      "cocwoo1         | Confidence: 1.000\n",
      "colara1         | Confidence: 1.000\n",
      "compau          | Confidence: 1.000\n",
      "cotfly1         | Confidence: 1.000\n",
      "crbtan1         | Confidence: 1.000\n",
      "crcwoo1         | Confidence: 1.000\n",
      "crebob1         | Confidence: 1.000\n",
      "cregua1         | Confidence: 1.000\n",
      "eardov1         | Confidence: 1.000\n",
      "grasal4         | Confidence: 1.000\n",
      "grbhaw1         | Confidence: 0.982\n",
      "greani1         | Confidence: 1.000\n",
      "greegr          | Confidence: 1.000\n",
      "greibi1         | Confidence: 1.000\n",
      "grekis          | Confidence: 1.000\n",
      "grysee1         | Confidence: 1.000\n",
      "gycwor1         | Confidence: 0.565\n",
      "laufal1         | Confidence: 1.000\n",
      "leagre          | Confidence: 0.998\n",
      "norscr1         | Confidence: 1.000\n",
      "piepuf1         | Confidence: 1.000\n",
      "pirfly1         | Confidence: 0.767\n",
      "purgal2         | Confidence: 1.000\n",
      "ragmac1         | Confidence: 1.000\n",
      "roahaw          | Confidence: 1.000\n",
      "rtlhum          | Confidence: 0.635\n",
      "rubsee1         | Confidence: 1.000\n",
      "ruther1         | Confidence: 1.000\n",
      "rutjac1         | Confidence: 1.000\n",
      "sahpar1         | Confidence: 1.000\n",
      "secfly1         | Confidence: 1.000\n",
      "shghum1         | Confidence: 1.000\n",
      "smbani          | Confidence: 1.000\n",
      "sobtyr1         | Confidence: 1.000\n",
      "socfly1         | Confidence: 1.000\n",
      "solsan          | Confidence: 1.000\n",
      "soulap1         | Confidence: 1.000\n",
      "spepar1         | Confidence: 0.856\n",
      "srwswa1         | Confidence: 1.000\n",
      "strfly1         | Confidence: 1.000\n",
      "thlsch3         | Confidence: 0.999\n",
      "turvul          | Confidence: 0.999\n",
      "wbwwre1         | Confidence: 1.000\n",
      "whbant1         | Confidence: 1.000\n",
      "whbman1         | Confidence: 0.988\n",
      "whfant1         | Confidence: 0.999\n",
      "whttro1         | Confidence: 1.000\n",
      "woosto          | Confidence: 1.000\n",
      "yebsee1         | Confidence: 1.000\n",
      "yecspi2         | Confidence: 0.956\n",
      "yeofly1         | Confidence: 0.948\n",
      "yercac1         | Confidence: 0.546\n",
      "ywcpar          | Confidence: 1.000\n"
     ]
    }
   ],
   "source": [
    "top_idx = int(np.argmax(probs))\n",
    "print(f\"Top-1 Prediction: {CLASSES[top_idx]} ({probs[top_idx]:.3f})\")\n",
    "\n",
    "print(\"\\nMulti-label Predictions (≥ 0.5):\")\n",
    "for i, score in enumerate(probs):\n",
    "    if score >= THRESHOLD:\n",
    "        print(f\"{CLASSES[i]:<15} | Confidence: {score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14a09844-91bd-41f7-b351-7ab726b0fbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.00% (0/100 correct)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "sample_indices = np.random.choice(len(test_manifest), size=100, replace=False)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    sample = test_manifest.iloc[idx]\n",
    "    emb, ma, m, wav = preprocess_sample(sample)\n",
    "\n",
    "    emb_np = emb.cpu().numpy()\n",
    "    ma_np  = ma.cpu().numpy()\n",
    "    m_np   = m.cpu().numpy()\n",
    "    wav_np = wav.cpu().numpy()\n",
    "\n",
    "    # Run inference\n",
    "    p1 = sessions[\"embedding\"].run(None, {\"embedding_input\": emb_np})[0]\n",
    "    p2 = sessions[\"resnet\"].run(None, {\"mel_aug_input\": ma_np})[0]\n",
    "    p3 = sessions[\"effnet\"].run(None, {\"mel_input\": m_np})[0]\n",
    "    p4 = sessions[\"rawaudio\"].run(None, {\"wav_input\": wav_np})[0]\n",
    "    fused = np.concatenate([p1, p2, p3, p4], axis=1)\n",
    "    meta_out = sessions[\"meta\"].run(None, {\"fusion_input\": fused})[0]\n",
    "\n",
    "    probs = sigmoid(meta_out[0])\n",
    "    pred = (probs >= THRESHOLD).astype(int)\n",
    "\n",
    "    gt = np.zeros(NUM_CLASSES, dtype=int)\n",
    "    labels = sample.primary_label if isinstance(sample.primary_label, list) else [sample.primary_label]\n",
    "    for lbl in labels:\n",
    "        if lbl in CLASSES:\n",
    "            gt[CLASSES.index(lbl)] = 1\n",
    "\n",
    "    all_preds.append(pred)\n",
    "    all_labels.append(gt)\n",
    "\n",
    "# Subset accuracy\n",
    "y_pred = np.stack(all_preds)\n",
    "y_true = np.stack(all_labels)\n",
    "correct = (y_pred == y_true).all(axis=1).sum()\n",
    "total = len(y_pred)\n",
    "accuracy = (correct / total) * 100\n",
    "print(f\"Accuracy: {accuracy:.2f}% ({correct}/{total} correct)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abb4c8ce-a65d-4547-a4e0-8ced61e9569f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding   : 27.73 MB\n",
      "resnet      : 95.52 MB\n",
      "effnet      : 63.34 MB\n",
      "rawaudio    : 0.76 MB\n",
      "meta        : 5.92 MB\n",
      "Total ONNX Model Size: 193.27 MB\n"
     ]
    }
   ],
   "source": [
    "total_size = 0\n",
    "for name, path in model_paths.items():\n",
    "    size = os.path.getsize(path)\n",
    "    total_size += size\n",
    "    print(f\"{name:<12}: {size / 1e6:.2f} MB\")\n",
    "\n",
    "print(f\"Total ONNX Model Size: {total_size / 1e6:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0952fb39-2e4c-4f54-9fef-c427d06e3b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Latency (single sample, median): 115.08 ms\n",
      "Inference Latency (single sample, 95th percentile): 143.71 ms\n",
      "Inference Latency (single sample, 99th percentile): 155.30 ms\n",
      "Inference Throughput (single sample): 8.62 FPS\n"
     ]
    }
   ],
   "source": [
    "# Use one sample for latency test\n",
    "sample = test_manifest.iloc[0]\n",
    "emb, ma, m, wav = preprocess_sample(sample)\n",
    "emb_np = emb.cpu().numpy()\n",
    "ma_np  = ma.cpu().numpy()\n",
    "m_np   = m.cpu().numpy()\n",
    "wav_np = wav.cpu().numpy()\n",
    "\n",
    "# Warm-up\n",
    "p1 = sessions[\"embedding\"].run(None, {\"embedding_input\": emb_np})[0]\n",
    "p2 = sessions[\"resnet\"].run(None, {\"mel_aug_input\": ma_np})[0]\n",
    "p3 = sessions[\"effnet\"].run(None, {\"mel_input\": m_np})[0]\n",
    "p4 = sessions[\"rawaudio\"].run(None, {\"wav_input\": wav_np})[0]\n",
    "fused = np.concatenate([p1, p2, p3, p4], axis=1)\n",
    "_ = sessions[\"meta\"].run(None, {\"fusion_input\": fused})[0]\n",
    "\n",
    "# Time it\n",
    "num_trials = 100\n",
    "latencies = []\n",
    "for _ in range(num_trials):\n",
    "    start_time = time.time()\n",
    "\n",
    "    p1 = sessions[\"embedding\"].run(None, {\"embedding_input\": emb_np})[0]\n",
    "    p2 = sessions[\"resnet\"].run(None, {\"mel_aug_input\": ma_np})[0]\n",
    "    p3 = sessions[\"effnet\"].run(None, {\"mel_input\": m_np})[0]\n",
    "    p4 = sessions[\"rawaudio\"].run(None, {\"wav_input\": wav_np})[0]\n",
    "    fused = np.concatenate([p1, p2, p3, p4], axis=1)\n",
    "    _ = sessions[\"meta\"].run(None, {\"fusion_input\": fused})[0]\n",
    "\n",
    "    latencies.append(time.time() - start_time)\n",
    "\n",
    "latencies = np.array(latencies)\n",
    "print(f\"Inference Latency (single sample, median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 95th percentile): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 99th percentile): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Inference Throughput (single sample): {num_trials/np.sum(latencies):.2f} FPS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb01ceef-4c6f-4d1d-803a-c3b617cebf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final BirdCLEF Inference Summary ===\n",
      "Accuracy: 0.00% (0/100 correct)\n",
      "Total ONNX Model Size: 193.27 MB\n",
      "Inference Latency (single sample, median): 115.08 ms\n",
      "Inference Latency (single sample, 95th percentile): 143.71 ms\n",
      "Inference Latency (single sample, 99th percentile): 155.30 ms\n",
      "Inference Throughput (single sample): 8.62 FPS\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Final BirdCLEF Inference Summary ===\")\n",
    "print(f\"Accuracy: {accuracy:.2f}% ({correct}/{total} correct)\")\n",
    "print(f\"Total ONNX Model Size: {total_size / 1e6:.2f} MB\")\n",
    "print(f\"Inference Latency (single sample, median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 95th percentile): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 99th percentile): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Inference Throughput (single sample): {num_trials/np.sum(latencies):.2f} FPS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5cdbdbb-ed51-4be4-b763-2df2560c2f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking batch throughput: 100%|██████████| 24/24 [00:13<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch Throughput (20x): 62.43 FPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 20\n",
    "num_batches = 25 # Total: 8 × 25 = 200 samples\n",
    "sample_indices = np.random.choice(len(test_manifest), size=batch_size * num_batches, replace=True)\n",
    "batches = [sample_indices[i:i + batch_size] for i in range(0, len(sample_indices), batch_size)]\n",
    "\n",
    "batch_times = []\n",
    "\n",
    "# ONNX Runtime returns [output_array], so fix shape safely\n",
    "def safe_squeeze(p_list):\n",
    "    p = p_list[0]\n",
    "    return np.squeeze(p, axis=1) if p.ndim == 3 else p\n",
    "\n",
    "# === Warm-up Batch ===\n",
    "warmup_batch = batches[0]\n",
    "embs, mas, ms, wavs = [], [], [], []\n",
    "\n",
    "for idx in warmup_batch:\n",
    "    sample = test_manifest.iloc[idx]\n",
    "    emb, ma, m, wav = preprocess_sample(sample)\n",
    "    embs.append(emb.cpu().numpy())\n",
    "    mas.append(ma.cpu().numpy())\n",
    "    ms.append(m.cpu().numpy())\n",
    "    wavs.append(wav.cpu().numpy())\n",
    "\n",
    "# Run warm-up inference\n",
    "p1 = safe_squeeze(sessions[\"embedding\"].run(None, {\"embedding_input\": np.concatenate(embs)}))\n",
    "p2 = safe_squeeze(sessions[\"resnet\"].run(None, {\"mel_aug_input\": np.concatenate(mas)}))\n",
    "p3 = safe_squeeze(sessions[\"effnet\"].run(None, {\"mel_input\": np.concatenate(ms)}))\n",
    "p4 = safe_squeeze(sessions[\"rawaudio\"].run(None, {\"wav_input\": np.concatenate(wavs)}))\n",
    "fusion_input = np.concatenate([p1, p2, p3, p4], axis=1)\n",
    "_ = sessions[\"meta\"].run(None, {\"fusion_input\": fusion_input})\n",
    "\n",
    "# === Timed Batches ===\n",
    "for batch in tqdm(batches[1:], desc=\"Benchmarking batch throughput\"):\n",
    "    embs, mas, ms, wavs = [], [], [], []\n",
    "    for idx in batch:\n",
    "        sample = test_manifest.iloc[idx]\n",
    "        emb, ma, m, wav = preprocess_sample(sample)\n",
    "        embs.append(emb.cpu().numpy())\n",
    "        mas.append(ma.cpu().numpy())\n",
    "        ms.append(m.cpu().numpy())\n",
    "        wavs.append(wav.cpu().numpy())\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    p1 = safe_squeeze(sessions[\"embedding\"].run(None, {\"embedding_input\": np.concatenate(embs)}))\n",
    "    p2 = safe_squeeze(sessions[\"resnet\"].run(None, {\"mel_aug_input\": np.concatenate(mas)}))\n",
    "    p3 = safe_squeeze(sessions[\"effnet\"].run(None, {\"mel_input\": np.concatenate(ms)}))\n",
    "    p4 = safe_squeeze(sessions[\"rawaudio\"].run(None, {\"wav_input\": np.concatenate(wavs)}))\n",
    "    fusion_input = np.concatenate([p1, p2, p3, p4], axis=1)\n",
    "    _ = sessions[\"meta\"].run(None, {\"fusion_input\": fusion_input})\n",
    "\n",
    "    batch_times.append(time.time() - start_time)\n",
    "\n",
    "# === Compute and print throughput ===\n",
    "total_samples = batch_size * (len(batches) - 1)\n",
    "batch_fps = total_samples / np.sum(batch_times)\n",
    "print(f\"\\nBatch Throughput ({batch_size}x): {batch_fps:.2f} FPS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f730e29-459e-442b-ba1a-84656d072aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final BirdCLEF ONNX Inference Summary ===\n",
      "Subset Accuracy:              0.00%\n",
      "Per-label Accuracy:           47.34%\n",
      "F1 Score (micro):             0.0093\n",
      "Precision (micro):            0.0047\n",
      "Recall (micro):               0.5100\n",
      "Total ONNX Model Size:        193.27 MB\n",
      "Latency (single sample):\n",
      "  Median:                   115.08 ms\n",
      "  95th percentile:          143.71 ms\n",
      "  99th percentile:          155.30 ms\n",
      "  Throughput (1x):          8.62 FPS\n",
      " Batch Throughput (20x):    62.43 FPS\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# Accuracy metrics\n",
    "subset_acc = (y_pred == y_true).all(axis=1).mean() * 100\n",
    "per_label_acc = (y_pred == y_true).mean() * 100\n",
    "\n",
    "# F1, Precision, Recall\n",
    "f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "precision = precision_score(y_true, y_pred, average=\"micro\")\n",
    "recall = recall_score(y_true, y_pred, average=\"micro\")\n",
    "\n",
    "# Model size\n",
    "total_model_size = sum(os.path.getsize(path) for path in model_paths.values()) / 1e6\n",
    "\n",
    "# Print all metrics\n",
    "print(\"\\n=== Final BirdCLEF ONNX Inference Summary ===\")\n",
    "print(f\"Subset Accuracy:              {subset_acc:.2f}%\")\n",
    "print(f\"Per-label Accuracy:           {per_label_acc:.2f}%\")\n",
    "print(f\"F1 Score (micro):             {f1:.4f}\")\n",
    "print(f\"Precision (micro):            {precision:.4f}\")\n",
    "print(f\"Recall (micro):               {recall:.4f}\")\n",
    "print(f\"Total ONNX Model Size:        {total_model_size:.2f} MB\")\n",
    "print(f\"Latency (single sample):\")\n",
    "print(f\"  Median:                   {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"  95th percentile:          {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"  99th percentile:          {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"  Throughput (1x):          {len(latencies) / np.sum(latencies):.2f} FPS\")\n",
    "print(f\" Batch Throughput ({batch_size}x):    {batch_fps:.2f} FPS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8d569de-7e0d-4d89-913f-dc0d78322115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_birdclef_inference_speed(sessions, test_manifest, num_trials=100, batch_size=8, num_batches=25):\n",
    "    print(f\"Execution providers: {[s.get_providers()[0] for s in sessions.values()]}\")\n",
    "    \n",
    "    # === Single Sample Latency Benchmark ===\n",
    "    sample = test_manifest.iloc[0]\n",
    "    emb, ma, m, wav = preprocess_sample(sample)\n",
    "    emb_np = emb.cpu().numpy()\n",
    "    ma_np = ma.cpu().numpy()\n",
    "    m_np = m.cpu().numpy()\n",
    "    wav_np = wav.cpu().numpy()\n",
    "\n",
    "    # Warm-up\n",
    "    _ = sessions[\"embedding\"].run(None, {\"embedding_input\": emb_np})\n",
    "    _ = sessions[\"resnet\"].run(None, {\"mel_aug_input\": ma_np})\n",
    "    _ = sessions[\"effnet\"].run(None, {\"mel_input\": m_np})\n",
    "    _ = sessions[\"rawaudio\"].run(None, {\"wav_input\": wav_np})\n",
    "    fusion_input = np.concatenate([\n",
    "        sessions[\"embedding\"].run(None, {\"embedding_input\": emb_np})[0],\n",
    "        sessions[\"resnet\"].run(None, {\"mel_aug_input\": ma_np})[0],\n",
    "        sessions[\"effnet\"].run(None, {\"mel_input\": m_np})[0],\n",
    "        sessions[\"rawaudio\"].run(None, {\"wav_input\": wav_np})[0]\n",
    "    ], axis=1)\n",
    "    _ = sessions[\"meta\"].run(None, {\"fusion_input\": fusion_input})\n",
    "\n",
    "    latencies = []\n",
    "    for _ in range(num_trials):\n",
    "        start = time.time()\n",
    "        p1 = sessions[\"embedding\"].run(None, {\"embedding_input\": emb_np})[0]\n",
    "        p2 = sessions[\"resnet\"].run(None, {\"mel_aug_input\": ma_np})[0]\n",
    "        p3 = sessions[\"effnet\"].run(None, {\"mel_input\": m_np})[0]\n",
    "        p4 = sessions[\"rawaudio\"].run(None, {\"wav_input\": wav_np})[0]\n",
    "        fusion_input = np.concatenate([p1, p2, p3, p4], axis=1)\n",
    "        _ = sessions[\"meta\"].run(None, {\"fusion_input\": fusion_input})\n",
    "        latencies.append(time.time() - start)\n",
    "\n",
    "    latencies = np.array(latencies)\n",
    "    print(f\"\\nSingle Sample Latency:\")\n",
    "    print(f\"  • Median:           {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "    print(f\"  • 95th percentile:  {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "    print(f\"  • 99th percentile:  {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "    print(f\"  • Throughput:       {len(latencies) / np.sum(latencies):.2f} FPS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8a74aa56-a8ed-4ead-87f8-dfcbefd8c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def run_batch_throughput(sessions, test_manifest, batch_size=8, num_batches=25):\n",
    "    sample_indices = np.random.choice(len(test_manifest), size=batch_size * num_batches, replace=True)\n",
    "    batches = [sample_indices[i:i + batch_size] for i in range(0, len(sample_indices), batch_size)]\n",
    "\n",
    "    def safe_squeeze(p_list):\n",
    "        p = p_list[0]\n",
    "        return np.squeeze(p, axis=1) if p.ndim == 3 else p\n",
    "\n",
    "    # Warm-up batch\n",
    "    warmup_batch = batches[0]\n",
    "    embs, mas, ms, wavs = [], [], [], []\n",
    "    for idx in warmup_batch:\n",
    "        sample = test_manifest.iloc[idx]\n",
    "        emb, ma, m, wav = preprocess_sample(sample)\n",
    "        embs.append(emb.cpu().numpy())\n",
    "        mas.append(ma.cpu().numpy())\n",
    "        ms.append(m.cpu().numpy())\n",
    "        wavs.append(wav.cpu().numpy())\n",
    "\n",
    "    p1 = safe_squeeze(sessions[\"embedding\"].run(None, {\"embedding_input\": np.concatenate(embs)}))\n",
    "    p2 = safe_squeeze(sessions[\"resnet\"].run(None, {\"mel_aug_input\": np.concatenate(mas)}))\n",
    "    p3 = safe_squeeze(sessions[\"effnet\"].run(None, {\"mel_input\": np.concatenate(ms)}))\n",
    "    p4 = safe_squeeze(sessions[\"rawaudio\"].run(None, {\"wav_input\": np.concatenate(wavs)}))\n",
    "    fusion_input = np.concatenate([p1, p2, p3, p4], axis=1)\n",
    "    _ = sessions[\"meta\"].run(None, {\"fusion_input\": fusion_input})\n",
    "\n",
    "    # Timed batches\n",
    "    batch_times = []\n",
    "    for batch in tqdm(batches[1:], desc=\"Measuring batch throughput\"):\n",
    "        embs, mas, ms, wavs = [], [], [], []\n",
    "        for idx in batch:\n",
    "            sample = test_manifest.iloc[idx]\n",
    "            emb, ma, m, wav = preprocess_sample(sample)\n",
    "            embs.append(emb.cpu().numpy())\n",
    "            mas.append(ma.cpu().numpy())\n",
    "            ms.append(m.cpu().numpy())\n",
    "            wavs.append(wav.cpu().numpy())\n",
    "\n",
    "        start_time = time.time()\n",
    "        p1 = safe_squeeze(sessions[\"embedding\"].run(None, {\"embedding_input\": np.concatenate(embs)}))\n",
    "        p2 = safe_squeeze(sessions[\"resnet\"].run(None, {\"mel_aug_input\": np.concatenate(mas)}))\n",
    "        p3 = safe_squeeze(sessions[\"effnet\"].run(None, {\"mel_input\": np.concatenate(ms)}))\n",
    "        p4 = safe_squeeze(sessions[\"rawaudio\"].run(None, {\"wav_input\": np.concatenate(wavs)}))\n",
    "        fusion_input = np.concatenate([p1, p2, p3, p4], axis=1)\n",
    "        _ = sessions[\"meta\"].run(None, {\"fusion_input\": fusion_input})\n",
    "        batch_times.append(time.time() - start_time)\n",
    "\n",
    "    total_samples = batch_size * (len(batches) - 1)\n",
    "    batch_fps = total_samples / np.sum(batch_times)\n",
    "    print(f\"\\n Batch Throughput ({batch_size}x): {batch_fps:.2f} FPS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3f6d241f-6a0c-4e48-814e-bd16df18e187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution providers: ['CPUExecutionProvider', 'CPUExecutionProvider', 'CPUExecutionProvider', 'CPUExecutionProvider', 'CPUExecutionProvider']\n",
      "\n",
      "Single Sample Latency:\n",
      "  • Median:           109.84 ms\n",
      "  • 95th percentile:  135.36 ms\n",
      "  • 99th percentile:  146.56 ms\n",
      "  • Throughput:       8.91 FPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring batch throughput: 100%|██████████| 24/24 [00:07<00:00,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Batch Throughput (8x): 42.84 FPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark_birdclef_inference_speed(sessions, test_manifest)\n",
    "run_batch_throughput(sessions, test_manifest, batch_size=8, num_batches=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9c3f7a87-ed53-42d1-a9bb-992bbc1510ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized embedding: saved to onnx_optimized/embedding_optimized.onnx\n",
      "Optimized resnet: saved to onnx_optimized/resnet_optimized.onnx\n",
      "Optimized effnet: saved to onnx_optimized/effnet_optimized.onnx\n",
      "Optimized rawaudio: saved to onnx_optimized/rawaudio_optimized.onnx\n",
      "Optimized meta: saved to onnx_optimized/meta_optimized.onnx\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import os\n",
    "\n",
    "# Directory setup\n",
    "original_dir = \"onnx_exports\"\n",
    "optimized_dir = \"onnx_optimized\"\n",
    "os.makedirs(optimized_dir, exist_ok=True)\n",
    "\n",
    "# Model names and paths\n",
    "model_names = {\n",
    "    \"embedding\":  \"embedding_classifier.onnx\",\n",
    "    \"resnet\":     \"resnet50_multilabel.onnx\",\n",
    "    \"effnet\":     \"efficientnet_b3_lora.onnx\",\n",
    "    \"rawaudio\":   \"raw_audio_cnn.onnx\",\n",
    "    \"meta\":       \"meta_mlp.onnx\"\n",
    "}\n",
    "\n",
    "# Apply graph optimizations and save optimized models\n",
    "for name, filename in model_names.items():\n",
    "    input_path = os.path.join(original_dir, filename)\n",
    "    output_path = os.path.join(optimized_dir, f\"{name}_optimized.onnx\")\n",
    "\n",
    "    session_options = ort.SessionOptions()\n",
    "    session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "    session_options.optimized_model_filepath = output_path\n",
    "\n",
    "    _ = ort.InferenceSession(input_path, sess_options=session_options, providers=[\"CPUExecutionProvider\"])\n",
    "    print(f\"Optimized {name}: saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "00b64df6-6a29-4abd-bda5-2defd73f96fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution providers: ['CPUExecutionProvider', 'CPUExecutionProvider', 'CPUExecutionProvider', 'CPUExecutionProvider', 'CPUExecutionProvider']\n",
      "\n",
      "Single Sample Latency:\n",
      "  • Median:           113.44 ms\n",
      "  • 95th percentile:  141.97 ms\n",
      "  • 99th percentile:  146.43 ms\n",
      "  • Throughput:       8.77 FPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring batch throughput: 100%|██████████| 24/24 [00:07<00:00,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Batch Throughput (8x): 42.13 FPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load optimized ONNX sessions for benchmarking\n",
    "sessions_optimized = {}\n",
    "for name in model_names:\n",
    "    path = os.path.join(optimized_dir, f\"{name}_optimized.onnx\")\n",
    "    sessions_optimized[name] = ort.InferenceSession(path, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "# Benchmark single-sample latency\n",
    "benchmark_birdclef_inference_speed(sessions_optimized, test_manifest)\n",
    "\n",
    "# Benchmark batch throughput\n",
    "run_batch_throughput(sessions_optimized, test_manifest, batch_size=8, num_batches=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f6f021a-f0b5-4737-ae3b-5f7bf55e45f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 02:13:42 [INFO] Start auto tuning.\n",
      "2025-05-11 02:13:42 [INFO] Quantize model without tuning!\n",
      "2025-05-11 02:13:42 [INFO] Quantize the model with default configuration without evaluating the model.                To perform the tuning process, please either provide an eval_func or provide an                    eval_dataloader an eval_metric.\n",
      "2025-05-11 02:13:42 [INFO] Adaptor has 5 recipes.\n",
      "2025-05-11 02:13:42 [INFO] 0 recipes specified by user.\n",
      "2025-05-11 02:13:42 [INFO] 3 recipes require future tuning.\n",
      "2025-05-11 02:13:43 [INFO] *** Initialize auto tuning\n",
      "2025-05-11 02:13:43 [INFO] {\n",
      "2025-05-11 02:13:43 [INFO]     'PostTrainingQuantConfig': {\n",
      "2025-05-11 02:13:43 [INFO]         'AccuracyCriterion': {\n",
      "2025-05-11 02:13:43 [INFO]             'criterion': 'relative',\n",
      "2025-05-11 02:13:43 [INFO]             'higher_is_better': True,\n",
      "2025-05-11 02:13:43 [INFO]             'tolerable_loss': 0.01,\n",
      "2025-05-11 02:13:43 [INFO]             'absolute': None,\n",
      "2025-05-11 02:13:43 [INFO]             'keys': <bound method AccuracyCriterion.keys of <neural_compressor.config.AccuracyCriterion object at 0x75e158363dd0>>,\n",
      "2025-05-11 02:13:43 [INFO]             'relative': 0.01\n",
      "2025-05-11 02:13:43 [INFO]         },\n",
      "2025-05-11 02:13:43 [INFO]         'approach': 'post_training_dynamic_quant',\n",
      "2025-05-11 02:13:43 [INFO]         'backend': 'default',\n",
      "2025-05-11 02:13:43 [INFO]         'calibration_sampling_size': [\n",
      "2025-05-11 02:13:43 [INFO]             100\n",
      "2025-05-11 02:13:43 [INFO]         ],\n",
      "2025-05-11 02:13:43 [INFO]         'device': 'cpu',\n",
      "2025-05-11 02:13:43 [INFO]         'domain': 'auto',\n",
      "2025-05-11 02:13:43 [INFO]         'example_inputs': 'Not printed here due to large size tensors...',\n",
      "2025-05-11 02:13:43 [INFO]         'excluded_precisions': [\n",
      "2025-05-11 02:13:43 [INFO]         ],\n",
      "2025-05-11 02:13:43 [INFO]         'framework': 'onnxruntime',\n",
      "2025-05-11 02:13:43 [INFO]         'inputs': [\n",
      "2025-05-11 02:13:43 [INFO]         ],\n",
      "2025-05-11 02:13:43 [INFO]         'model_name': '',\n",
      "2025-05-11 02:13:43 [INFO]         'op_name_dict': None,\n",
      "2025-05-11 02:13:43 [INFO]         'op_type_dict': None,\n",
      "2025-05-11 02:13:43 [INFO]         'outputs': [\n",
      "2025-05-11 02:13:43 [INFO]         ],\n",
      "2025-05-11 02:13:43 [INFO]         'quant_format': 'default',\n",
      "2025-05-11 02:13:43 [INFO]         'quant_level': 'auto',\n",
      "2025-05-11 02:13:43 [INFO]         'recipes': {\n",
      "2025-05-11 02:13:43 [INFO]             'smooth_quant': False,\n",
      "2025-05-11 02:13:43 [INFO]             'smooth_quant_args': {\n",
      "2025-05-11 02:13:43 [INFO]             },\n",
      "2025-05-11 02:13:43 [INFO]             'layer_wise_quant': False,\n",
      "2025-05-11 02:13:43 [INFO]             'layer_wise_quant_args': {\n",
      "2025-05-11 02:13:43 [INFO]             },\n",
      "2025-05-11 02:13:43 [INFO]             'fast_bias_correction': False,\n",
      "2025-05-11 02:13:43 [INFO]             'weight_correction': False,\n",
      "2025-05-11 02:13:43 [INFO]             'gemm_to_matmul': True,\n",
      "2025-05-11 02:13:43 [INFO]             'graph_optimization_level': None,\n",
      "2025-05-11 02:13:43 [INFO]             'first_conv_or_matmul_quantization': True,\n",
      "2025-05-11 02:13:43 [INFO]             'last_conv_or_matmul_quantization': True,\n",
      "2025-05-11 02:13:43 [INFO]             'pre_post_process_quantization': True,\n",
      "2025-05-11 02:13:43 [INFO]             'add_qdq_pair_to_weight': False,\n",
      "2025-05-11 02:13:43 [INFO]             'optypes_to_exclude_output_quant': [\n",
      "2025-05-11 02:13:43 [INFO]             ],\n",
      "2025-05-11 02:13:43 [INFO]             'dedicated_qdq_pair': False,\n",
      "2025-05-11 02:13:43 [INFO]             'rtn_args': {\n",
      "2025-05-11 02:13:43 [INFO]             },\n",
      "2025-05-11 02:13:43 [INFO]             'awq_args': {\n",
      "2025-05-11 02:13:43 [INFO]             },\n",
      "2025-05-11 02:13:43 [INFO]             'gptq_args': {\n",
      "2025-05-11 02:13:43 [INFO]             },\n",
      "2025-05-11 02:13:43 [INFO]             'teq_args': {\n",
      "2025-05-11 02:13:43 [INFO]             },\n",
      "2025-05-11 02:13:43 [INFO]             'autoround_args': {\n",
      "2025-05-11 02:13:43 [INFO]             }\n",
      "2025-05-11 02:13:43 [INFO]         },\n",
      "2025-05-11 02:13:43 [INFO]         'reduce_range': None,\n",
      "2025-05-11 02:13:43 [INFO]         'TuningCriterion': {\n",
      "2025-05-11 02:13:43 [INFO]             'max_trials': 100,\n",
      "2025-05-11 02:13:43 [INFO]             'objective': [\n",
      "2025-05-11 02:13:43 [INFO]                 'performance'\n",
      "2025-05-11 02:13:43 [INFO]             ],\n",
      "2025-05-11 02:13:43 [INFO]             'strategy': 'basic',\n",
      "2025-05-11 02:13:43 [INFO]             'strategy_kwargs': None,\n",
      "2025-05-11 02:13:43 [INFO]             'timeout': 0\n",
      "2025-05-11 02:13:43 [INFO]         },\n",
      "2025-05-11 02:13:43 [INFO]         'use_bf16': True,\n",
      "2025-05-11 02:13:43 [INFO]         'ni_workload_name': 'quantization'\n",
      "2025-05-11 02:13:43 [INFO]     }\n",
      "2025-05-11 02:13:43 [INFO] }\n",
      "2025-05-11 02:13:43 [WARNING] [Strategy] Please install `mpi4py` correctly if using distributed tuning; otherwise, ignore this warning.\n",
      "2025-05-11 02:13:43 [WARNING] The model is automatically detected as a non-NLP model. You can use 'domain' argument in 'PostTrainingQuantConfig' to overwrite it\n",
      "2025-05-11 02:13:43 [WARNING] Graph optimization level is automatically set to ENABLE_BASIC. You can use 'recipe' argument in 'PostTrainingQuantConfig'to overwrite it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 02:13:43 [INFO] Do not evaluate the baseline and quantize the model with default configuration.\n",
      "2025-05-11 02:13:43 [INFO] Quantize the model with default config.\n",
      "2025-05-11 02:13:44 [INFO] |******Mixed Precision Statistics******|\n",
      "2025-05-11 02:13:44 [INFO] +-----------------------+-------+------+\n",
      "2025-05-11 02:13:44 [INFO] |        Op Type        | Total | INT8 |\n",
      "2025-05-11 02:13:44 [INFO] +-----------------------+-------+------+\n",
      "2025-05-11 02:13:44 [INFO] |         MatMul        |   4   |  4   |\n",
      "2025-05-11 02:13:44 [INFO] | DynamicQuantizeLinear |   4   |  4   |\n",
      "2025-05-11 02:13:44 [INFO] +-----------------------+-------+------+\n",
      "2025-05-11 02:13:44 [INFO] Pass quantize model elapsed time: 603.21 ms\n",
      "2025-05-11 02:13:44 [INFO] Save tuning history to /home/jovyan/work/nc_workspace/2025-05-11_02-13-40/./history.snapshot.\n",
      "2025-05-11 02:13:44 [INFO] [Strategy] Found the model meets accuracy requirements, ending the tuning process.\n",
      "2025-05-11 02:13:44 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n",
      "2025-05-11 02:13:44 [INFO] Save deploy yaml to /home/jovyan/work/nc_workspace/2025-05-11_02-13-40/deploy.yaml\n",
      "2025-05-11 02:13:44 [INFO] Start auto tuning.\n",
      "2025-05-11 02:13:44 [INFO] Quantize model without tuning!\n",
      "2025-05-11 02:13:44 [INFO] Quantize the model with default configuration without evaluating the model.                To perform the tuning process, please either provide an eval_func or provide an                    eval_dataloader an eval_metric.\n",
      "2025-05-11 02:13:44 [INFO] Adaptor has 5 recipes.\n",
      "2025-05-11 02:13:44 [INFO] 0 recipes specified by user.\n",
      "2025-05-11 02:13:44 [INFO] 3 recipes require future tuning.\n",
      "2025-05-11 02:13:44 [INFO] *** Initialize auto tuning\n",
      "2025-05-11 02:13:44 [INFO] {\n",
      "2025-05-11 02:13:44 [INFO]     'PostTrainingQuantConfig': {\n",
      "2025-05-11 02:13:44 [INFO]         'AccuracyCriterion': {\n",
      "2025-05-11 02:13:44 [INFO]             'criterion': 'relative',\n",
      "2025-05-11 02:13:44 [INFO]             'higher_is_better': True,\n",
      "2025-05-11 02:13:44 [INFO]             'tolerable_loss': 0.01,\n",
      "2025-05-11 02:13:44 [INFO]             'absolute': None,\n",
      "2025-05-11 02:13:44 [INFO]             'keys': <bound method AccuracyCriterion.keys of <neural_compressor.config.AccuracyCriterion object at 0x75e158363dd0>>,\n",
      "2025-05-11 02:13:44 [INFO]             'relative': 0.01\n",
      "2025-05-11 02:13:44 [INFO]         },\n",
      "2025-05-11 02:13:44 [INFO]         'approach': 'post_training_dynamic_quant',\n",
      "2025-05-11 02:13:44 [INFO]         'backend': 'default',\n",
      "2025-05-11 02:13:44 [INFO]         'calibration_sampling_size': [\n",
      "2025-05-11 02:13:44 [INFO]             100\n",
      "2025-05-11 02:13:44 [INFO]         ],\n",
      "2025-05-11 02:13:44 [INFO]         'device': 'cpu',\n",
      "2025-05-11 02:13:44 [INFO]         'domain': 'auto',\n",
      "2025-05-11 02:13:44 [INFO]         'example_inputs': 'Not printed here due to large size tensors...',\n",
      "2025-05-11 02:13:44 [INFO]         'excluded_precisions': [\n",
      "2025-05-11 02:13:44 [INFO]         ],\n",
      "2025-05-11 02:13:44 [INFO]         'framework': 'onnxruntime',\n",
      "2025-05-11 02:13:44 [INFO]         'inputs': [\n",
      "2025-05-11 02:13:44 [INFO]         ],\n",
      "2025-05-11 02:13:44 [INFO]         'model_name': '',\n",
      "2025-05-11 02:13:44 [INFO]         'op_name_dict': None,\n",
      "2025-05-11 02:13:44 [INFO]         'op_type_dict': None,\n",
      "2025-05-11 02:13:44 [INFO]         'outputs': [\n",
      "2025-05-11 02:13:44 [INFO]         ],\n",
      "2025-05-11 02:13:44 [INFO]         'quant_format': 'default',\n",
      "2025-05-11 02:13:44 [INFO]         'quant_level': 'auto',\n",
      "2025-05-11 02:13:44 [INFO]         'recipes': {\n",
      "2025-05-11 02:13:44 [INFO]             'smooth_quant': False,\n",
      "2025-05-11 02:13:44 [INFO]             'smooth_quant_args': {\n",
      "2025-05-11 02:13:44 [INFO]             },\n",
      "2025-05-11 02:13:44 [INFO]             'layer_wise_quant': False,\n",
      "2025-05-11 02:13:44 [INFO]             'layer_wise_quant_args': {\n",
      "2025-05-11 02:13:44 [INFO]             },\n",
      "2025-05-11 02:13:44 [INFO]             'fast_bias_correction': False,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: onnx_quantized/embedding_quantized.onnx\n",
      "Quantizing resnet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 02:13:44 [INFO]             'weight_correction': False,\n",
      "2025-05-11 02:13:44 [INFO]             'gemm_to_matmul': True,\n",
      "2025-05-11 02:13:44 [INFO]             'graph_optimization_level': None,\n",
      "2025-05-11 02:13:44 [INFO]             'first_conv_or_matmul_quantization': True,\n",
      "2025-05-11 02:13:44 [INFO]             'last_conv_or_matmul_quantization': True,\n",
      "2025-05-11 02:13:44 [INFO]             'pre_post_process_quantization': True,\n",
      "2025-05-11 02:13:44 [INFO]             'add_qdq_pair_to_weight': False,\n",
      "2025-05-11 02:13:44 [INFO]             'optypes_to_exclude_output_quant': [\n",
      "2025-05-11 02:13:44 [INFO]             ],\n",
      "2025-05-11 02:13:44 [INFO]             'dedicated_qdq_pair': False,\n",
      "2025-05-11 02:13:44 [INFO]             'rtn_args': {\n",
      "2025-05-11 02:13:44 [INFO]             },\n",
      "2025-05-11 02:13:44 [INFO]             'awq_args': {\n",
      "2025-05-11 02:13:44 [INFO]             },\n",
      "2025-05-11 02:13:44 [INFO]             'gptq_args': {\n",
      "2025-05-11 02:13:44 [INFO]             },\n",
      "2025-05-11 02:13:44 [INFO]             'teq_args': {\n",
      "2025-05-11 02:13:44 [INFO]             },\n",
      "2025-05-11 02:13:44 [INFO]             'autoround_args': {\n",
      "2025-05-11 02:13:44 [INFO]             }\n",
      "2025-05-11 02:13:44 [INFO]         },\n",
      "2025-05-11 02:13:44 [INFO]         'reduce_range': None,\n",
      "2025-05-11 02:13:44 [INFO]         'TuningCriterion': {\n",
      "2025-05-11 02:13:44 [INFO]             'max_trials': 100,\n",
      "2025-05-11 02:13:44 [INFO]             'objective': [\n",
      "2025-05-11 02:13:44 [INFO]                 'performance'\n",
      "2025-05-11 02:13:44 [INFO]             ],\n",
      "2025-05-11 02:13:44 [INFO]             'strategy': 'basic',\n",
      "2025-05-11 02:13:44 [INFO]             'strategy_kwargs': None,\n",
      "2025-05-11 02:13:44 [INFO]             'timeout': 0\n",
      "2025-05-11 02:13:44 [INFO]         },\n",
      "2025-05-11 02:13:44 [INFO]         'use_bf16': True,\n",
      "2025-05-11 02:13:44 [INFO]         'ni_workload_name': 'quantization'\n",
      "2025-05-11 02:13:44 [INFO]     }\n",
      "2025-05-11 02:13:44 [INFO] }\n",
      "2025-05-11 02:13:44 [WARNING] [Strategy] Please install `mpi4py` correctly if using distributed tuning; otherwise, ignore this warning.\n",
      "2025-05-11 02:13:44 [WARNING] The model is automatically detected as a non-NLP model. You can use 'domain' argument in 'PostTrainingQuantConfig' to overwrite it\n",
      "2025-05-11 02:13:44 [WARNING] Graph optimization level is automatically set to ENABLE_BASIC. You can use 'recipe' argument in 'PostTrainingQuantConfig'to overwrite it\n",
      "2025-05-11 02:13:45 [INFO] Do not evaluate the baseline and quantize the model with default configuration.\n",
      "2025-05-11 02:13:45 [INFO] Quantize the model with default config.\n",
      "2025-05-11 02:13:48 [INFO] |******Mixed Precision Statistics******|\n",
      "2025-05-11 02:13:48 [INFO] +-----------------------+-------+------+\n",
      "2025-05-11 02:13:48 [INFO] |        Op Type        | Total | INT8 |\n",
      "2025-05-11 02:13:48 [INFO] +-----------------------+-------+------+\n",
      "2025-05-11 02:13:48 [INFO] |          Conv         |   53  |  53  |\n",
      "2025-05-11 02:13:48 [INFO] |         MatMul        |   1   |  1   |\n",
      "2025-05-11 02:13:48 [INFO] | DynamicQuantizeLinear |   50  |  50  |\n",
      "2025-05-11 02:13:48 [INFO] +-----------------------+-------+------+\n",
      "2025-05-11 02:13:48 [INFO] Pass quantize model elapsed time: 2962.33 ms\n",
      "2025-05-11 02:13:48 [INFO] Save tuning history to /home/jovyan/work/nc_workspace/2025-05-11_02-13-40/./history.snapshot.\n",
      "2025-05-11 02:13:48 [INFO] [Strategy] Found the model meets accuracy requirements, ending the tuning process.\n",
      "2025-05-11 02:13:48 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n",
      "2025-05-11 02:13:48 [INFO] Save deploy yaml to /home/jovyan/work/nc_workspace/2025-05-11_02-13-40/deploy.yaml\n",
      "2025-05-11 02:13:48 [INFO] Start auto tuning.\n",
      "2025-05-11 02:13:48 [INFO] Quantize model without tuning!\n",
      "2025-05-11 02:13:48 [INFO] Quantize the model with default configuration without evaluating the model.                To perform the tuning process, please either provide an eval_func or provide an                    eval_dataloader an eval_metric.\n",
      "2025-05-11 02:13:48 [INFO] Adaptor has 5 recipes.\n",
      "2025-05-11 02:13:48 [INFO] 0 recipes specified by user.\n",
      "2025-05-11 02:13:48 [INFO] 3 recipes require future tuning.\n",
      "2025-05-11 02:13:48 [INFO] *** Initialize auto tuning\n",
      "2025-05-11 02:13:48 [INFO] {\n",
      "2025-05-11 02:13:48 [INFO]     'PostTrainingQuantConfig': {\n",
      "2025-05-11 02:13:48 [INFO]         'AccuracyCriterion': {\n",
      "2025-05-11 02:13:48 [INFO]             'criterion': 'relative',\n",
      "2025-05-11 02:13:48 [INFO]             'higher_is_better': True,\n",
      "2025-05-11 02:13:48 [INFO]             'tolerable_loss': 0.01,\n",
      "2025-05-11 02:13:48 [INFO]             'absolute': None,\n",
      "2025-05-11 02:13:48 [INFO]             'keys': <bound method AccuracyCriterion.keys of <neural_compressor.config.AccuracyCriterion object at 0x75e158363dd0>>,\n",
      "2025-05-11 02:13:48 [INFO]             'relative': 0.01\n",
      "2025-05-11 02:13:48 [INFO]         },\n",
      "2025-05-11 02:13:48 [INFO]         'approach': 'post_training_dynamic_quant',\n",
      "2025-05-11 02:13:48 [INFO]         'backend': 'default',\n",
      "2025-05-11 02:13:48 [INFO]         'calibration_sampling_size': [\n",
      "2025-05-11 02:13:48 [INFO]             100\n",
      "2025-05-11 02:13:48 [INFO]         ],\n",
      "2025-05-11 02:13:48 [INFO]         'device': 'cpu',\n",
      "2025-05-11 02:13:48 [INFO]         'domain': 'auto',\n",
      "2025-05-11 02:13:48 [INFO]         'example_inputs': 'Not printed here due to large size tensors...',\n",
      "2025-05-11 02:13:48 [INFO]         'excluded_precisions': [\n",
      "2025-05-11 02:13:48 [INFO]         ],\n",
      "2025-05-11 02:13:48 [INFO]         'framework': 'onnxruntime',\n",
      "2025-05-11 02:13:48 [INFO]         'inputs': [\n",
      "2025-05-11 02:13:48 [INFO]         ],\n",
      "2025-05-11 02:13:48 [INFO]         'model_name': '',\n",
      "2025-05-11 02:13:48 [INFO]         'op_name_dict': None,\n",
      "2025-05-11 02:13:48 [INFO]         'op_type_dict': None,\n",
      "2025-05-11 02:13:48 [INFO]         'outputs': [\n",
      "2025-05-11 02:13:48 [INFO]         ],\n",
      "2025-05-11 02:13:48 [INFO]         'quant_format': 'default',\n",
      "2025-05-11 02:13:48 [INFO]         'quant_level': 'auto',\n",
      "2025-05-11 02:13:48 [INFO]         'recipes': {\n",
      "2025-05-11 02:13:48 [INFO]             'smooth_quant': False,\n",
      "2025-05-11 02:13:48 [INFO]             'smooth_quant_args': {\n",
      "2025-05-11 02:13:48 [INFO]             },\n",
      "2025-05-11 02:13:48 [INFO]             'layer_wise_quant': False,\n",
      "2025-05-11 02:13:48 [INFO]             'layer_wise_quant_args': {\n",
      "2025-05-11 02:13:48 [INFO]             },\n",
      "2025-05-11 02:13:48 [INFO]             'fast_bias_correction': False,\n",
      "2025-05-11 02:13:48 [INFO]             'weight_correction': False,\n",
      "2025-05-11 02:13:48 [INFO]             'gemm_to_matmul': True,\n",
      "2025-05-11 02:13:48 [INFO]             'graph_optimization_level': None,\n",
      "2025-05-11 02:13:48 [INFO]             'first_conv_or_matmul_quantization': True,\n",
      "2025-05-11 02:13:48 [INFO]             'last_conv_or_matmul_quantization': True,\n",
      "2025-05-11 02:13:48 [INFO]             'pre_post_process_quantization': True,\n",
      "2025-05-11 02:13:48 [INFO]             'add_qdq_pair_to_weight': False,\n",
      "2025-05-11 02:13:48 [INFO]             'optypes_to_exclude_output_quant': [\n",
      "2025-05-11 02:13:48 [INFO]             ],\n",
      "2025-05-11 02:13:48 [INFO]             'dedicated_qdq_pair': False,\n",
      "2025-05-11 02:13:48 [INFO]             'rtn_args': {\n",
      "2025-05-11 02:13:48 [INFO]             },\n",
      "2025-05-11 02:13:48 [INFO]             'awq_args': {\n",
      "2025-05-11 02:13:48 [INFO]             },\n",
      "2025-05-11 02:13:48 [INFO]             'gptq_args': {\n",
      "2025-05-11 02:13:48 [INFO]             },\n",
      "2025-05-11 02:13:48 [INFO]             'teq_args': {\n",
      "2025-05-11 02:13:48 [INFO]             },\n",
      "2025-05-11 02:13:48 [INFO]             'autoround_args': {\n",
      "2025-05-11 02:13:48 [INFO]             }\n",
      "2025-05-11 02:13:48 [INFO]         },\n",
      "2025-05-11 02:13:48 [INFO]         'reduce_range': None,\n",
      "2025-05-11 02:13:48 [INFO]         'TuningCriterion': {\n",
      "2025-05-11 02:13:48 [INFO]             'max_trials': 100,\n",
      "2025-05-11 02:13:48 [INFO]             'objective': [\n",
      "2025-05-11 02:13:48 [INFO]                 'performance'\n",
      "2025-05-11 02:13:48 [INFO]             ],\n",
      "2025-05-11 02:13:48 [INFO]             'strategy': 'basic',\n",
      "2025-05-11 02:13:48 [INFO]             'strategy_kwargs': None,\n",
      "2025-05-11 02:13:48 [INFO]             'timeout': 0\n",
      "2025-05-11 02:13:48 [INFO]         },\n",
      "2025-05-11 02:13:48 [INFO]         'use_bf16': True,\n",
      "2025-05-11 02:13:48 [INFO]         'ni_workload_name': 'quantization'\n",
      "2025-05-11 02:13:48 [INFO]     }\n",
      "2025-05-11 02:13:48 [INFO] }\n",
      "2025-05-11 02:13:48 [WARNING] [Strategy] Please install `mpi4py` correctly if using distributed tuning; otherwise, ignore this warning.\n",
      "2025-05-11 02:13:48 [WARNING] The model is automatically detected as a non-NLP model. You can use 'domain' argument in 'PostTrainingQuantConfig' to overwrite it\n",
      "2025-05-11 02:13:48 [WARNING] Graph optimization level is automatically set to ENABLE_BASIC. You can use 'recipe' argument in 'PostTrainingQuantConfig'to overwrite it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: onnx_quantized/resnet_quantized.onnx\n",
      "Quantizing effnet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 02:13:49 [INFO] Do not evaluate the baseline and quantize the model with default configuration.\n",
      "2025-05-11 02:13:49 [INFO] Quantize the model with default config.\n",
      "2025-05-11 02:13:52 [INFO] |******Mixed Precision Statistics******|\n",
      "2025-05-11 02:13:52 [INFO] +-----------------------+-------+------+\n",
      "2025-05-11 02:13:52 [INFO] |        Op Type        | Total | INT8 |\n",
      "2025-05-11 02:13:52 [INFO] +-----------------------+-------+------+\n",
      "2025-05-11 02:13:52 [INFO] |          Conv         |  284  | 284  |\n",
      "2025-05-11 02:13:52 [INFO] |         MatMul        |   1   |  1   |\n",
      "2025-05-11 02:13:52 [INFO] | DynamicQuantizeLinear |  208  | 208  |\n",
      "2025-05-11 02:13:52 [INFO] +-----------------------+-------+------+\n",
      "2025-05-11 02:13:52 [INFO] Pass quantize model elapsed time: 3292.43 ms\n",
      "2025-05-11 02:13:52 [INFO] Save tuning history to /home/jovyan/work/nc_workspace/2025-05-11_02-13-40/./history.snapshot.\n",
      "2025-05-11 02:13:52 [INFO] [Strategy] Found the model meets accuracy requirements, ending the tuning process.\n",
      "2025-05-11 02:13:52 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n",
      "2025-05-11 02:13:52 [INFO] Save deploy yaml to /home/jovyan/work/nc_workspace/2025-05-11_02-13-40/deploy.yaml\n",
      "2025-05-11 02:13:52 [INFO] Start auto tuning.\n",
      "2025-05-11 02:13:52 [INFO] Quantize model without tuning!\n",
      "2025-05-11 02:13:52 [INFO] Quantize the model with default configuration without evaluating the model.                To perform the tuning process, please either provide an eval_func or provide an                    eval_dataloader an eval_metric.\n",
      "2025-05-11 02:13:52 [INFO] Adaptor has 5 recipes.\n",
      "2025-05-11 02:13:52 [INFO] 0 recipes specified by user.\n",
      "2025-05-11 02:13:52 [INFO] 3 recipes require future tuning.\n",
      "2025-05-11 02:13:52 [INFO] *** Initialize auto tuning\n",
      "2025-05-11 02:13:52 [INFO] {\n",
      "2025-05-11 02:13:52 [INFO]     'PostTrainingQuantConfig': {\n",
      "2025-05-11 02:13:52 [INFO]         'AccuracyCriterion': {\n",
      "2025-05-11 02:13:52 [INFO]             'criterion': 'relative',\n",
      "2025-05-11 02:13:52 [INFO]             'higher_is_better': True,\n",
      "2025-05-11 02:13:52 [INFO]             'tolerable_loss': 0.01,\n",
      "2025-05-11 02:13:52 [INFO]             'absolute': None,\n",
      "2025-05-11 02:13:52 [INFO]             'keys': <bound method AccuracyCriterion.keys of <neural_compressor.config.AccuracyCriterion object at 0x75e158363dd0>>,\n",
      "2025-05-11 02:13:52 [INFO]             'relative': 0.01\n",
      "2025-05-11 02:13:52 [INFO]         },\n",
      "2025-05-11 02:13:52 [INFO]         'approach': 'post_training_dynamic_quant',\n",
      "2025-05-11 02:13:52 [INFO]         'backend': 'default',\n",
      "2025-05-11 02:13:52 [INFO]         'calibration_sampling_size': [\n",
      "2025-05-11 02:13:52 [INFO]             100\n",
      "2025-05-11 02:13:52 [INFO]         ],\n",
      "2025-05-11 02:13:52 [INFO]         'device': 'cpu',\n",
      "2025-05-11 02:13:52 [INFO]         'domain': 'auto',\n",
      "2025-05-11 02:13:52 [INFO]         'example_inputs': 'Not printed here due to large size tensors...',\n",
      "2025-05-11 02:13:52 [INFO]         'excluded_precisions': [\n",
      "2025-05-11 02:13:52 [INFO]         ],\n",
      "2025-05-11 02:13:52 [INFO]         'framework': 'onnxruntime',\n",
      "2025-05-11 02:13:52 [INFO]         'inputs': [\n",
      "2025-05-11 02:13:52 [INFO]         ],\n",
      "2025-05-11 02:13:52 [INFO]         'model_name': '',\n",
      "2025-05-11 02:13:52 [INFO]         'op_name_dict': None,\n",
      "2025-05-11 02:13:52 [INFO]         'op_type_dict': None,\n",
      "2025-05-11 02:13:52 [INFO]         'outputs': [\n",
      "2025-05-11 02:13:52 [INFO]         ],\n",
      "2025-05-11 02:13:52 [INFO]         'quant_format': 'default',\n",
      "2025-05-11 02:13:52 [INFO]         'quant_level': 'auto',\n",
      "2025-05-11 02:13:52 [INFO]         'recipes': {\n",
      "2025-05-11 02:13:52 [INFO]             'smooth_quant': False,\n",
      "2025-05-11 02:13:52 [INFO]             'smooth_quant_args': {\n",
      "2025-05-11 02:13:52 [INFO]             },\n",
      "2025-05-11 02:13:52 [INFO]             'layer_wise_quant': False,\n",
      "2025-05-11 02:13:52 [INFO]             'layer_wise_quant_args': {\n",
      "2025-05-11 02:13:52 [INFO]             },\n",
      "2025-05-11 02:13:52 [INFO]             'fast_bias_correction': False,\n",
      "2025-05-11 02:13:52 [INFO]             'weight_correction': False,\n",
      "2025-05-11 02:13:52 [INFO]             'gemm_to_matmul': True,\n",
      "2025-05-11 02:13:52 [INFO]             'graph_optimization_level': None,\n",
      "2025-05-11 02:13:52 [INFO]             'first_conv_or_matmul_quantization': True,\n",
      "2025-05-11 02:13:52 [INFO]             'last_conv_or_matmul_quantization': True,\n",
      "2025-05-11 02:13:52 [INFO]             'pre_post_process_quantization': True,\n",
      "2025-05-11 02:13:52 [INFO]             'add_qdq_pair_to_weight': False,\n",
      "2025-05-11 02:13:52 [INFO]             'optypes_to_exclude_output_quant': [\n",
      "2025-05-11 02:13:52 [INFO]             ],\n",
      "2025-05-11 02:13:52 [INFO]             'dedicated_qdq_pair': False,\n",
      "2025-05-11 02:13:52 [INFO]             'rtn_args': {\n",
      "2025-05-11 02:13:52 [INFO]             },\n",
      "2025-05-11 02:13:52 [INFO]             'awq_args': {\n",
      "2025-05-11 02:13:52 [INFO]             },\n",
      "2025-05-11 02:13:52 [INFO]             'gptq_args': {\n",
      "2025-05-11 02:13:52 [INFO]             },\n",
      "2025-05-11 02:13:52 [INFO]             'teq_args': {\n",
      "2025-05-11 02:13:52 [INFO]             },\n",
      "2025-05-11 02:13:52 [INFO]             'autoround_args': {\n",
      "2025-05-11 02:13:52 [INFO]             }\n",
      "2025-05-11 02:13:52 [INFO]         },\n",
      "2025-05-11 02:13:52 [INFO]         'reduce_range': None,\n",
      "2025-05-11 02:13:52 [INFO]         'TuningCriterion': {\n",
      "2025-05-11 02:13:52 [INFO]             'max_trials': 100,\n",
      "2025-05-11 02:13:52 [INFO]             'objective': [\n",
      "2025-05-11 02:13:52 [INFO]                 'performance'\n",
      "2025-05-11 02:13:52 [INFO]             ],\n",
      "2025-05-11 02:13:52 [INFO]             'strategy': 'basic',\n",
      "2025-05-11 02:13:52 [INFO]             'strategy_kwargs': None,\n",
      "2025-05-11 02:13:52 [INFO]             'timeout': 0\n",
      "2025-05-11 02:13:52 [INFO]         },\n",
      "2025-05-11 02:13:52 [INFO]         'use_bf16': True,\n",
      "2025-05-11 02:13:52 [INFO]         'ni_workload_name': 'quantization'\n",
      "2025-05-11 02:13:52 [INFO]     }\n",
      "2025-05-11 02:13:52 [INFO] }\n",
      "2025-05-11 02:13:52 [WARNING] [Strategy] Please install `mpi4py` correctly if using distributed tuning; otherwise, ignore this warning.\n",
      "2025-05-11 02:13:52 [WARNING] The model is automatically detected as a non-NLP model. You can use 'domain' argument in 'PostTrainingQuantConfig' to overwrite it\n",
      "2025-05-11 02:13:52 [WARNING] Graph optimization level is automatically set to ENABLE_BASIC. You can use 'recipe' argument in 'PostTrainingQuantConfig'to overwrite it\n",
      "2025-05-11 02:13:53 [INFO] Do not evaluate the baseline and quantize the model with default configuration.\n",
      "2025-05-11 02:13:53 [INFO] Quantize the model with default config.\n",
      "2025-05-11 02:13:53 [INFO] |******Mixed Precision Statistics******|\n",
      "2025-05-11 02:13:53 [INFO] +-----------------------+-------+------+\n",
      "2025-05-11 02:13:53 [INFO] |        Op Type        | Total | INT8 |\n",
      "2025-05-11 02:13:53 [INFO] +-----------------------+-------+------+\n",
      "2025-05-11 02:13:53 [INFO] |          Conv         |   4   |  4   |\n",
      "2025-05-11 02:13:53 [INFO] |         MatMul        |   1   |  1   |\n",
      "2025-05-11 02:13:53 [INFO] | DynamicQuantizeLinear |   5   |  5   |\n",
      "2025-05-11 02:13:53 [INFO] +-----------------------+-------+------+\n",
      "2025-05-11 02:13:53 [INFO] Pass quantize model elapsed time: 31.11 ms\n",
      "2025-05-11 02:13:53 [INFO] Save tuning history to /home/jovyan/work/nc_workspace/2025-05-11_02-13-40/./history.snapshot.\n",
      "2025-05-11 02:13:53 [INFO] [Strategy] Found the model meets accuracy requirements, ending the tuning process.\n",
      "2025-05-11 02:13:53 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n",
      "2025-05-11 02:13:53 [INFO] Save deploy yaml to /home/jovyan/work/nc_workspace/2025-05-11_02-13-40/deploy.yaml\n",
      "2025-05-11 02:13:53 [INFO] Start auto tuning.\n",
      "2025-05-11 02:13:53 [INFO] Quantize model without tuning!\n",
      "2025-05-11 02:13:53 [INFO] Quantize the model with default configuration without evaluating the model.                To perform the tuning process, please either provide an eval_func or provide an                    eval_dataloader an eval_metric.\n",
      "2025-05-11 02:13:53 [INFO] Adaptor has 5 recipes.\n",
      "2025-05-11 02:13:53 [INFO] 0 recipes specified by user.\n",
      "2025-05-11 02:13:53 [INFO] 3 recipes require future tuning.\n",
      "2025-05-11 02:13:53 [INFO] *** Initialize auto tuning\n",
      "2025-05-11 02:13:53 [INFO] {\n",
      "2025-05-11 02:13:53 [INFO]     'PostTrainingQuantConfig': {\n",
      "2025-05-11 02:13:53 [INFO]         'AccuracyCriterion': {\n",
      "2025-05-11 02:13:53 [INFO]             'criterion': 'relative',\n",
      "2025-05-11 02:13:53 [INFO]             'higher_is_better': True,\n",
      "2025-05-11 02:13:53 [INFO]             'tolerable_loss': 0.01,\n",
      "2025-05-11 02:13:53 [INFO]             'absolute': None,\n",
      "2025-05-11 02:13:53 [INFO]             'keys': <bound method AccuracyCriterion.keys of <neural_compressor.config.AccuracyCriterion object at 0x75e158363dd0>>,\n",
      "2025-05-11 02:13:53 [INFO]             'relative': 0.01\n",
      "2025-05-11 02:13:53 [INFO]         },\n",
      "2025-05-11 02:13:53 [INFO]         'approach': 'post_training_dynamic_quant',\n",
      "2025-05-11 02:13:53 [INFO]         'backend': 'default',\n",
      "2025-05-11 02:13:53 [INFO]         'calibration_sampling_size': [\n",
      "2025-05-11 02:13:53 [INFO]             100\n",
      "2025-05-11 02:13:53 [INFO]         ],\n",
      "2025-05-11 02:13:53 [INFO]         'device': 'cpu',\n",
      "2025-05-11 02:13:53 [INFO]         'domain': 'auto',\n",
      "2025-05-11 02:13:53 [INFO]         'example_inputs': 'Not printed here due to large size tensors...',\n",
      "2025-05-11 02:13:53 [INFO]         'excluded_precisions': [\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: onnx_quantized/effnet_quantized.onnx\n",
      "Quantizing rawaudio...\n",
      "Saved: onnx_quantized/rawaudio_quantized.onnx\n",
      "Quantizing meta...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 02:13:53 [INFO]         ],\n",
      "2025-05-11 02:13:53 [INFO]         'framework': 'onnxruntime',\n",
      "2025-05-11 02:13:53 [INFO]         'inputs': [\n",
      "2025-05-11 02:13:53 [INFO]         ],\n",
      "2025-05-11 02:13:53 [INFO]         'model_name': '',\n",
      "2025-05-11 02:13:53 [INFO]         'op_name_dict': None,\n",
      "2025-05-11 02:13:53 [INFO]         'op_type_dict': None,\n",
      "2025-05-11 02:13:53 [INFO]         'outputs': [\n",
      "2025-05-11 02:13:53 [INFO]         ],\n",
      "2025-05-11 02:13:53 [INFO]         'quant_format': 'default',\n",
      "2025-05-11 02:13:53 [INFO]         'quant_level': 'auto',\n",
      "2025-05-11 02:13:53 [INFO]         'recipes': {\n",
      "2025-05-11 02:13:53 [INFO]             'smooth_quant': False,\n",
      "2025-05-11 02:13:53 [INFO]             'smooth_quant_args': {\n",
      "2025-05-11 02:13:53 [INFO]             },\n",
      "2025-05-11 02:13:53 [INFO]             'layer_wise_quant': False,\n",
      "2025-05-11 02:13:53 [INFO]             'layer_wise_quant_args': {\n",
      "2025-05-11 02:13:53 [INFO]             },\n",
      "2025-05-11 02:13:53 [INFO]             'fast_bias_correction': False,\n",
      "2025-05-11 02:13:53 [INFO]             'weight_correction': False,\n",
      "2025-05-11 02:13:53 [INFO]             'gemm_to_matmul': True,\n",
      "2025-05-11 02:13:53 [INFO]             'graph_optimization_level': None,\n",
      "2025-05-11 02:13:53 [INFO]             'first_conv_or_matmul_quantization': True,\n",
      "2025-05-11 02:13:53 [INFO]             'last_conv_or_matmul_quantization': True,\n",
      "2025-05-11 02:13:53 [INFO]             'pre_post_process_quantization': True,\n",
      "2025-05-11 02:13:53 [INFO]             'add_qdq_pair_to_weight': False,\n",
      "2025-05-11 02:13:53 [INFO]             'optypes_to_exclude_output_quant': [\n",
      "2025-05-11 02:13:53 [INFO]             ],\n",
      "2025-05-11 02:13:53 [INFO]             'dedicated_qdq_pair': False,\n",
      "2025-05-11 02:13:53 [INFO]             'rtn_args': {\n",
      "2025-05-11 02:13:53 [INFO]             },\n",
      "2025-05-11 02:13:53 [INFO]             'awq_args': {\n",
      "2025-05-11 02:13:53 [INFO]             },\n",
      "2025-05-11 02:13:53 [INFO]             'gptq_args': {\n",
      "2025-05-11 02:13:53 [INFO]             },\n",
      "2025-05-11 02:13:53 [INFO]             'teq_args': {\n",
      "2025-05-11 02:13:53 [INFO]             },\n",
      "2025-05-11 02:13:53 [INFO]             'autoround_args': {\n",
      "2025-05-11 02:13:53 [INFO]             }\n",
      "2025-05-11 02:13:53 [INFO]         },\n",
      "2025-05-11 02:13:53 [INFO]         'reduce_range': None,\n",
      "2025-05-11 02:13:53 [INFO]         'TuningCriterion': {\n",
      "2025-05-11 02:13:53 [INFO]             'max_trials': 100,\n",
      "2025-05-11 02:13:53 [INFO]             'objective': [\n",
      "2025-05-11 02:13:53 [INFO]                 'performance'\n",
      "2025-05-11 02:13:53 [INFO]             ],\n",
      "2025-05-11 02:13:53 [INFO]             'strategy': 'basic',\n",
      "2025-05-11 02:13:53 [INFO]             'strategy_kwargs': None,\n",
      "2025-05-11 02:13:53 [INFO]             'timeout': 0\n",
      "2025-05-11 02:13:53 [INFO]         },\n",
      "2025-05-11 02:13:53 [INFO]         'use_bf16': True,\n",
      "2025-05-11 02:13:53 [INFO]         'ni_workload_name': 'quantization'\n",
      "2025-05-11 02:13:53 [INFO]     }\n",
      "2025-05-11 02:13:53 [INFO] }\n",
      "2025-05-11 02:13:53 [WARNING] [Strategy] Please install `mpi4py` correctly if using distributed tuning; otherwise, ignore this warning.\n",
      "2025-05-11 02:13:53 [WARNING] The model is automatically detected as a non-NLP model. You can use 'domain' argument in 'PostTrainingQuantConfig' to overwrite it\n",
      "2025-05-11 02:13:53 [WARNING] Graph optimization level is automatically set to ENABLE_BASIC. You can use 'recipe' argument in 'PostTrainingQuantConfig'to overwrite it\n",
      "2025-05-11 02:13:53 [INFO] Do not evaluate the baseline and quantize the model with default configuration.\n",
      "2025-05-11 02:13:53 [INFO] Quantize the model with default config.\n",
      "2025-05-11 02:13:53 [INFO] |******Mixed Precision Statistics******|\n",
      "2025-05-11 02:13:53 [INFO] +-----------------------+-------+------+\n",
      "2025-05-11 02:13:53 [INFO] |        Op Type        | Total | INT8 |\n",
      "2025-05-11 02:13:53 [INFO] +-----------------------+-------+------+\n",
      "2025-05-11 02:13:53 [INFO] |         MatMul        |   3   |  3   |\n",
      "2025-05-11 02:13:53 [INFO] | DynamicQuantizeLinear |   3   |  3   |\n",
      "2025-05-11 02:13:53 [INFO] +-----------------------+-------+------+\n",
      "2025-05-11 02:13:53 [INFO] Pass quantize model elapsed time: 113.73 ms\n",
      "2025-05-11 02:13:53 [INFO] Save tuning history to /home/jovyan/work/nc_workspace/2025-05-11_02-13-40/./history.snapshot.\n",
      "2025-05-11 02:13:53 [INFO] [Strategy] Found the model meets accuracy requirements, ending the tuning process.\n",
      "2025-05-11 02:13:53 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n",
      "2025-05-11 02:13:53 [INFO] Save deploy yaml to /home/jovyan/work/nc_workspace/2025-05-11_02-13-40/deploy.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: onnx_quantized/meta_quantized.onnx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from neural_compressor import quantization\n",
    "from neural_compressor.model.onnx_model import ONNXModel\n",
    "from neural_compressor.config import PostTrainingQuantConfig\n",
    "\n",
    "# Directories\n",
    "original_dir = \"onnx_exports\"\n",
    "quantized_dir = \"onnx_quantized\"\n",
    "os.makedirs(quantized_dir, exist_ok=True)\n",
    "\n",
    "# Model filenames\n",
    "model_names = {\n",
    "    \"embedding\":  \"embedding_classifier.onnx\",\n",
    "    \"resnet\":     \"resnet50_multilabel.onnx\",\n",
    "    \"effnet\":     \"efficientnet_b3_lora.onnx\",\n",
    "    \"rawaudio\":   \"raw_audio_cnn.onnx\",\n",
    "    \"meta\":       \"meta_mlp.onnx\"\n",
    "}\n",
    "\n",
    "# Dynamic Quantization config (no calibration dataset required)\n",
    "config_ptq = PostTrainingQuantConfig(approach=\"dynamic\")\n",
    "\n",
    "# Quantize each model\n",
    "for name, filename in model_names.items():\n",
    "    input_path = os.path.join(original_dir, filename)\n",
    "    output_path = os.path.join(quantized_dir, f\"{name}_quantized.onnx\")\n",
    "\n",
    "    print(f\"Quantizing {name}...\")\n",
    "    fp32_model = ONNXModel(input_path)\n",
    "    q_model = quantization.fit(model=fp32_model, conf=config_ptq)\n",
    "    q_model.save_model_to_file(output_path)\n",
    "    print(f\"Saved: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6299d760-325c-4421-9d9c-d45b375020d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution providers: ['CPUExecutionProvider', 'CPUExecutionProvider', 'CPUExecutionProvider', 'CPUExecutionProvider', 'CPUExecutionProvider']\n",
      "\n",
      "Single Sample Latency:\n",
      "  • Median:           199.78 ms\n",
      "  • 95th percentile:  254.92 ms\n",
      "  • 99th percentile:  296.44 ms\n",
      "  • Throughput:       4.86 FPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring batch throughput: 100%|██████████| 24/24 [00:28<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Batch Throughput (8x): 7.61 FPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "# Load quantized models into sessions\n",
    "sessions_quantized = {}\n",
    "for name in model_names:\n",
    "    path = os.path.join(quantized_dir, f\"{name}_quantized.onnx\")\n",
    "    sessions_quantized[name] = ort.InferenceSession(path, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "# Run benchmarks\n",
    "benchmark_birdclef_inference_speed(sessions_quantized, test_manifest)\n",
    "run_batch_throughput(sessions_quantized, test_manifest, batch_size=8, num_batches=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4ab230af-5d8d-4c76-a816-f326d4521289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Quantized Model Sizes on Disk:\n",
      "  Embedding : 6.99 MB\n",
      "  Resnet    : 24.13 MB\n",
      "  Effnet    : 17.14 MB\n",
      "  Rawaudio  : 0.20 MB\n",
      "  Meta      : 1.51 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "quantized_dir = \"onnx_quantized\"\n",
    "\n",
    "print(\" Quantized Model Sizes on Disk:\")\n",
    "for name in [\"embedding\", \"resnet\", \"effnet\", \"rawaudio\", \"meta\"]:\n",
    "    onnx_model_path = os.path.join(quantized_dir, f\"{name}_quantized.onnx\")\n",
    "    model_size = os.path.getsize(onnx_model_path) / 1e6  # in MB\n",
    "    print(f\"  {name.capitalize():<10}: {model_size:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f163beba-e96e-47e5-aa0c-d7382dd04cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmarking Quantized BirdCLEF Models...\n",
      "Execution providers: ['CPUExecutionProvider', 'CPUExecutionProvider', 'CPUExecutionProvider', 'CPUExecutionProvider', 'CPUExecutionProvider']\n",
      "\n",
      "Single Sample Latency:\n",
      "  • Median:           201.51 ms\n",
      "  • 95th percentile:  239.01 ms\n",
      "  • 99th percentile:  287.63 ms\n",
      "  • Throughput:       4.88 FPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring batch throughput: 100%|██████████| 24/24 [00:28<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Batch Throughput (8x): 7.65 FPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "# Load quantized models into sessions\n",
    "sessions_quantized = {}\n",
    "for name in [\"embedding\", \"resnet\", \"effnet\", \"rawaudio\", \"meta\"]:\n",
    "    path = os.path.join(quantized_dir, f\"{name}_quantized.onnx\")\n",
    "    sessions_quantized[name] = ort.InferenceSession(path, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "print(\"\\nBenchmarking Quantized BirdCLEF Models...\")\n",
    "benchmark_birdclef_inference_speed(sessions_quantized, test_manifest)\n",
    "run_batch_throughput(sessions_quantized, test_manifest, batch_size=8, num_batches=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "262dec1d-d97b-4361-908f-6ebc2e58d253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Quadro RTX 6000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3aa50b66-6f9b-4842-9a26-5a9a5a07c606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Execution device: CPU\n",
      " Providers used:\n",
      "  embedding: CPUExecutionProvider\n",
      "  resnet   : CPUExecutionProvider\n",
      "  effnet   : CPUExecutionProvider\n",
      "  rawaudio : CPUExecutionProvider\n",
      "  meta     : CPUExecutionProvider\n",
      "\n",
      "Benchmarking on CUDAExecutionProvider...\n",
      "Execution providers: ['CPUExecutionProvider', 'CPUExecutionProvider', 'CPUExecutionProvider', 'CPUExecutionProvider', 'CPUExecutionProvider']\n",
      "\n",
      "Single Sample Latency:\n",
      "  • Median:           116.02 ms\n",
      "  • 95th percentile:  144.12 ms\n",
      "  • 99th percentile:  150.50 ms\n",
      "  • Throughput:       8.57 FPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring batch throughput: 100%|██████████| 24/24 [00:07<00:00,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Batch Throughput (8x): 42.81 FPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Mapping of expected logical keys to actual ONNX filenames\n",
    "model_files = {\n",
    "    \"embedding\":  \"embedding_classifier.onnx\",\n",
    "    \"resnet\":     \"resnet50_multilabel.onnx\",\n",
    "    \"effnet\":     \"efficientnet_b3_lora.onnx\",\n",
    "    \"rawaudio\":   \"raw_audio_cnn.onnx\",\n",
    "    \"meta\":       \"meta_mlp.onnx\"\n",
    "}\n",
    "\n",
    "# Directory containing the ONNX models\n",
    "onnx_dir = \"onnx_exports\"  # Change to \"onnx_optimized\" or \"onnx_quantized\" if needed\n",
    "\n",
    "# Load ONNX models using CUDAExecutionProvider\n",
    "sessions_cuda = {}\n",
    "for key, filename in model_files.items():\n",
    "    model_path = os.path.join(onnx_dir, filename)\n",
    "    sessions_cuda[key] = ort.InferenceSession(model_path, providers=[\"CUDAExecutionProvider\"])\n",
    "\n",
    "# Confirm active execution providers\n",
    "print(f\" Execution device: {ort.get_device()}\")\n",
    "print(\" Providers used:\")\n",
    "for name, session in sessions_cuda.items():\n",
    "    print(f\"  {name:<9}: {session.get_providers()[0]}\")\n",
    "\n",
    "# Run benchmarks\n",
    "print(\"\\nBenchmarking on CUDAExecutionProvider...\")\n",
    "benchmark_birdclef_inference_speed(sessions_cuda, test_manifest)\n",
    "run_batch_throughput(sessions_cuda, test_manifest, batch_size=8, num_batches=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c075eab9-3c87-4de6-acc6-4b4f9958f9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ee044a-0c8e-4f28-b2e0-81bdc7a81bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
