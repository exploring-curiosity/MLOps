{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13bc88ec-08de-4350-8195-b0c9bf797eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL 1\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import timm\n",
    "from random import sample as rand_sample\n",
    "from sklearn.metrics import accuracy_score\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import onnx\n",
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10ebc686-6510-4234-abc7-5c2bc5df760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL 2\n",
    "DEVICE        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "FEATURE_BASE  = \"/mnt/BirdCLEF/features_sampled\"\n",
    "TEST_MANIFEST = os.path.join(FEATURE_BASE, \"manifest_test.csv\")\n",
    "TAXONOMY_CSV  = \"/mnt/BirdCLEF/taxonomy.csv\"\n",
    "TRAIN_META    = \"/mnt/BirdCLEF/train.csv\"\n",
    "DROPOUT       = 0.3\n",
    "\n",
    "CKPT_EMB    = \"/mnt/BirdCLEF/Models/best_emb_mlp.pt\"\n",
    "CKPT_RES    = \"/mnt/BirdCLEF/Models/best_resnet50.pt\"\n",
    "CKPT_EFF    = \"/mnt/BirdCLEF/Models/best_effb3_lora.pt\"\n",
    "CKPT_RAW    = \"/mnt/BirdCLEF/Models/best_rawcnn.pt\"\n",
    "CKPT_META   = \"/mnt/BirdCLEF/Models/best_meta_mlp.pt\"\n",
    "\n",
    "HIDDEN_DIMS    = [1024, 512]\n",
    "THRESHOLD   = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c88ada-8813-46a3-b6a7-9e3ed7364fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL 3\n",
    "tax = pd.read_csv(TAXONOMY_CSV)\n",
    "CLASSES = sorted(tax[\"primary_label\"].astype(str).tolist())\n",
    "NUM_CLASSES = len(CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "986a81d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL 4\n",
    "class MetaMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dims, dropout):\n",
    "        super().__init__()\n",
    "        layers, dims = [], [in_dim]+hidden_dims\n",
    "        for i in range(len(hidden_dims)):\n",
    "            layers += [\n",
    "                nn.Linear(dims[i], dims[i+1]),\n",
    "                nn.BatchNorm1d(dims[i+1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ]\n",
    "        layers.append(nn.Linear(dims[-1], NUM_CLASSES))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5812ec1c-2753-4325-b205-a99c5923b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL 5\n",
    "class EmbeddingClassifier(nn.Module):\n",
    "    def __init__(self, emb_dim, num_cls):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 2048), nn.BatchNorm1d(2048), nn.ReLU(), nn.Dropout(DROPOUT),\n",
    "            nn.Linear(2048, 1024),    nn.BatchNorm1d(1024), nn.ReLU(), nn.Dropout(DROPOUT),\n",
    "            nn.Linear(1024, 512),     nn.BatchNorm1d(512),  nn.ReLU(), nn.Dropout(DROPOUT),\n",
    "            nn.Linear(512, num_cls)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "def get_resnet50_multilabel(num_classes):\n",
    "    m = torch.hub.load('pytorch/vision:v0.14.0', 'resnet50', pretrained=False)\n",
    "    m.conv1 = nn.Conv2d(1, m.conv1.out_channels,\n",
    "                        kernel_size=m.conv1.kernel_size,\n",
    "                        stride=m.conv1.stride,\n",
    "                        padding=m.conv1.padding,\n",
    "                        bias=False)\n",
    "    m.fc    = nn.Linear(m.fc.in_features, num_classes)\n",
    "    return m\n",
    "\n",
    "TARGET_MODULES  = [\"conv_pw\",\"conv_dw\",\"conv_pwl\",\"conv_head\"]\n",
    "MODULES_TO_SAVE = [\"classifier\"]\n",
    "def build_efficientnetb3_lora(num_classes):\n",
    "    base = timm.create_model(\"efficientnet_b3\", pretrained=True)\n",
    "    # patch forward\n",
    "    orig_fwd = base.forward\n",
    "    def forward_patch(*args, input_ids=None, **kwargs):\n",
    "        x = input_ids if input_ids is not None else args[0]\n",
    "        return orig_fwd(x)\n",
    "    base.forward = forward_patch\n",
    "    # adapt stem & head\n",
    "    stem = base.conv_stem\n",
    "    base.conv_stem = nn.Conv2d(1, stem.out_channels,\n",
    "                               kernel_size=stem.kernel_size,\n",
    "                               stride=stem.stride,\n",
    "                               padding=stem.padding,\n",
    "                               bias=False)\n",
    "    base.classifier = nn.Linear(base.classifier.in_features, num_classes)\n",
    "    # LoRA\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=12, lora_alpha=24,\n",
    "        target_modules=TARGET_MODULES,\n",
    "        lora_dropout=0.1, bias=\"none\",\n",
    "        modules_to_save=MODULES_TO_SAVE,\n",
    "        task_type=\"FEATURE_EXTRACTION\",\n",
    "        inference_mode=False\n",
    "    )\n",
    "    return get_peft_model(base, lora_cfg)\n",
    "\n",
    "class RawAudioCNN(nn.Module):\n",
    "    def __init__(self, num_cls):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16,  kernel_size=15, stride=4, padding=7)\n",
    "        self.bn1   = nn.BatchNorm1d(16)\n",
    "        self.pool  = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(16,32,  kernel_size=15, stride=2, padding=7)\n",
    "        self.bn2   = nn.BatchNorm1d(32)\n",
    "        self.conv3 = nn.Conv1d(32,64,  kernel_size=15, stride=2, padding=7)\n",
    "        self.bn3   = nn.BatchNorm1d(64)\n",
    "        self.conv4 = nn.Conv1d(64,128, kernel_size=15, stride=2, padding=7)\n",
    "        self.bn4   = nn.BatchNorm1d(128)\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc          = nn.Linear(128, num_cls)\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # [B,T]â†’[B,1,T]\n",
    "        x = F.relu(self.bn1(self.conv1(x))); x = self.pool(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.global_pool(x).squeeze(-1)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1098d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jovyan/.cache/torch/hub/pytorch_vision_v0.14.0\n",
      "/opt/conda/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/tmp/ipykernel_13072/1809891512.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt, map_location=\"cpu\"))\n",
      "/tmp/ipykernel_13072/1809891512.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  meta_model.load_state_dict(torch.load(CKPT_META, map_location=DEVICE))\n"
     ]
    }
   ],
   "source": [
    "# === Step 1: Get emb_dim ===\n",
    "class EmbeddingDatasetForDim:\n",
    "    def __init__(self, manifest, base, key=\"embedding\"):\n",
    "        df = pd.read_csv(manifest)\n",
    "        df[\"emb_path\"] = df[\"emb_path\"].astype(str).apply(\n",
    "            lambda p: os.path.join(base, \"embeddings\", p.lstrip(os.sep))\n",
    "        )\n",
    "        first_sample_path = df.iloc[0].emb_path\n",
    "        arr = np.load(first_sample_path)[key]  # shape: (n_windows, emb_dim)\n",
    "        self.emb_dim = arr.shape[1]\n",
    "\n",
    "_emb_ds = EmbeddingDatasetForDim(TEST_MANIFEST, FEATURE_BASE)\n",
    "emb_dim = _emb_ds.emb_dim\n",
    "\n",
    "# === Step 2: Instantiate all models ===\n",
    "emb_model  = EmbeddingClassifier(emb_dim=emb_dim, num_cls=NUM_CLASSES).to(DEVICE)\n",
    "res_model  = get_resnet50_multilabel(NUM_CLASSES).to(DEVICE)\n",
    "eff_model  = build_efficientnetb3_lora(NUM_CLASSES).to(DEVICE)\n",
    "raw_model  = RawAudioCNN(NUM_CLASSES).to(DEVICE)\n",
    "meta_model = MetaMLP(NUM_CLASSES * 4, HIDDEN_DIMS, DROPOUT).to(DEVICE)\n",
    "\n",
    "# === Step 3: Load weights & freeze base models ===\n",
    "for model, ckpt in [\n",
    "    (emb_model, CKPT_EMB),\n",
    "    (res_model, CKPT_RES),\n",
    "    (eff_model, CKPT_EFF),\n",
    "    (raw_model, CKPT_RAW)\n",
    "]:\n",
    "    model.load_state_dict(torch.load(ckpt, map_location=\"cpu\"))\n",
    "    model.eval()\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "# === Step 4: Load meta model weights (not frozen) ===\n",
    "meta_model.load_state_dict(torch.load(CKPT_META, map_location=DEVICE))\n",
    "meta_model.eval()\n",
    "\n",
    "# === Step 5: Compile all models (PyTorch 2.x) ===\n",
    "emb_model  = torch.compile(emb_model)\n",
    "res_model  = torch.compile(res_model)\n",
    "eff_model  = torch.compile(eff_model)\n",
    "raw_model  = torch.compile(raw_model)\n",
    "meta_model = torch.compile(meta_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e010cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 7\n",
    "def preprocess_sample(sample):\n",
    "    \n",
    "    emb_path = os.path.join(FEATURE_BASE, \"embeddings\", sample.emb_path.lstrip(os.sep))\n",
    "    emb_arr = np.load(emb_path)[\"embedding\"].mean(axis=0).astype(np.float32)\n",
    "    emb = torch.from_numpy(emb_arr).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    ma_path = os.path.join(FEATURE_BASE, \"mel_aug\", sample.mel_aug_path.lstrip(os.sep))\n",
    "    ma_arr = np.load(ma_path)[\"mel\"].astype(np.float32)\n",
    "    ma = torch.from_numpy(ma_arr).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    m_path = os.path.join(FEATURE_BASE, \"mel\", sample.mel_path.lstrip(os.sep))\n",
    "    m_arr = np.load(m_path)[\"mel\"].astype(np.float32)\n",
    "    m = torch.from_numpy(m_arr).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    wav_path = os.path.join(FEATURE_BASE, \"denoised\", sample.audio_path.lstrip(os.sep))\n",
    "    wav, sr = torchaudio.load(wav_path)\n",
    "    wav = wav.float().squeeze(0)\n",
    "    T = sr * 10\n",
    "    if wav.size(0) < T:\n",
    "        wav = F.pad(wav, (0, T - wav.size(0)))\n",
    "    else:\n",
    "        wav = wav[:T]\n",
    "    wav = (wav - wav.mean()) / wav.std().clamp_min(1e-6)\n",
    "    wav = wav.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    return emb, ma, m, wav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13c85fe8-4fe1-4ad4-a2b9-dc830fa8f779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 0: Load sample for inference ===\n",
    "test_manifest = pd.read_csv(TEST_MANIFEST)\n",
    "sample = test_manifest.iloc[10]  # Or any other row for testing\n",
    "\n",
    "emb, ma, m, wav = preprocess_sample(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d206c30-8429-42a5-bd26-14c62d3e5393",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL 9\n",
    "with torch.no_grad():\n",
    "    p1 = torch.sigmoid(emb_model(emb))     # [1,NUM_CLASSES]\n",
    "    p2 = torch.sigmoid(res_model(ma))      # [1,NUM_CLASSES]\n",
    "    p3 = torch.sigmoid(eff_model(m))       # [1,NUM_CLASSES]\n",
    "    p4 = torch.sigmoid(raw_model(wav))     # [1,NUM_CLASSES]\n",
    "\n",
    "    feat   = torch.cat([p1,p2,p3,p4], dim=1)\n",
    "    logits = meta_model(feat)\n",
    "    probs  = torch.sigmoid(logits)[0].cpu().numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94f70b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Sizes on Disk:\n",
      "  Embedding MLP:    26.48 MB\n",
      "  ResNet50:         91.57 MB\n",
      "  EfficientNetB3:   63.92 MB\n",
      "  RawAudioCNN:      0.73 MB\n",
      "  Meta MLP:         5.66 MB\n"
     ]
    }
   ],
   "source": [
    "#CELL 10\n",
    "def get_model_size(path):\n",
    "    size_mb = os.path.getsize(path) / (1024 ** 2)\n",
    "    return f\"{size_mb:.2f} MB\"\n",
    "\n",
    "print(\"\\nModel Sizes on Disk:\")\n",
    "print(f\"  Embedding MLP:    {get_model_size(CKPT_EMB)}\")\n",
    "print(f\"  ResNet50:         {get_model_size(CKPT_RES)}\")\n",
    "print(f\"  EfficientNetB3:   {get_model_size(CKPT_EFF)}\")\n",
    "print(f\"  RawAudioCNN:      {get_model_size(CKPT_RAW)}\")\n",
    "print(f\"  Meta MLP:         {get_model_size(CKPT_META)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f84749a1-daab-401f-89ec-36d3919bf0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multi-label Predictions (Threshold â‰¥ 0.5):\n",
      "126247         |Confidence: 0.635\n"
     ]
    }
   ],
   "source": [
    "#CELL 11\n",
    "ml_preds = [(CLASSES[i], float(probs[i]))\n",
    "            for i in range(NUM_CLASSES) if probs[i] >= THRESHOLD]\n",
    "\n",
    "print(f\"\\nMulti-label Predictions (Threshold â‰¥ {THRESHOLD}):\")\n",
    "if ml_preds:\n",
    "    for label, score in ml_preds:\n",
    "        print(f\"{label:<15}|Confidence: {score:.3f}\")\n",
    "else:\n",
    "    print(\" No predictions met the threshold.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbcd23fa-a8c6-4383-8473-c336a12a63d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Primaryâ€‘label (topâ€‘1) prediction:\n",
      "  â†’ 126247: 0.635\n"
     ]
    }
   ],
   "source": [
    "#CELL 12\n",
    "primary_idx   = int(probs.argmax())\n",
    "primary_label = CLASSES[primary_idx]\n",
    "primary_score = float(probs[primary_idx])\n",
    "\n",
    "print(f\"\\nPrimaryâ€‘label (topâ€‘1) prediction:\")\n",
    "print(f\"  â†’ {primary_label}: {primary_score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b71bd568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5961\n"
     ]
    }
   ],
   "source": [
    "#Cell 13\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "for _, sample in test_manifest.iterrows():\n",
    "    \n",
    "    emb, ma, m, wav = preprocess_sample(sample)\n",
    "\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        p1 = torch.sigmoid(emb_model(emb))\n",
    "        p2 = torch.sigmoid(res_model(ma))\n",
    "        p3 = torch.sigmoid(eff_model(m))\n",
    "        p4 = torch.sigmoid(raw_model(wav))\n",
    "\n",
    "        feat = torch.cat([p1, p2, p3, p4], dim=1)\n",
    "        logits = meta_model(feat)\n",
    "        probs = torch.sigmoid(logits)[0].cpu().numpy()\n",
    "\n",
    "    # Thresholded prediction\n",
    "    pred_labels = (probs >= THRESHOLD).astype(int)\n",
    "\n",
    "    # Ground truth (one-hot)\n",
    "    gt = np.zeros(NUM_CLASSES, dtype=int)\n",
    "    labels = sample.primary_label if isinstance(sample.primary_label, list) else [sample.primary_label]\n",
    "    for lbl in labels:\n",
    "        if lbl in CLASSES:\n",
    "            gt[CLASSES.index(lbl)] = 1\n",
    "\n",
    "    all_preds.append(pred_labels)\n",
    "    all_labels.append(gt)\n",
    "\n",
    "# Compute accuracy\n",
    "y_pred = np.stack(all_preds)\n",
    "y_true = np.stack(all_labels)\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b44a1aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Inference Latency Stats over 100 Random Samples ===\n",
      "Median Latency:      20.91 ms\n",
      "95th Percentile:     26.80 ms\n",
      "99th Percentile:     28.82 ms\n",
      "Average Latency:     22.20 ms\n",
      "Throughput:          45.05 FPS\n"
     ]
    }
   ],
   "source": [
    "#Cell 14\n",
    "N_TRIALS = 100\n",
    "sample_indices = rand_sample(range(len(test_manifest)), N_TRIALS)\n",
    "latencies = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in sample_indices:\n",
    "        sample = test_manifest.iloc[idx]\n",
    "        emb, ma, m, wav = preprocess_sample(sample)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        p1 = torch.sigmoid(emb_model(emb))\n",
    "        p2 = torch.sigmoid(res_model(ma))\n",
    "        p3 = torch.sigmoid(eff_model(m))\n",
    "        p4 = torch.sigmoid(raw_model(wav))\n",
    "\n",
    "        feat = torch.cat([p1, p2, p3, p4], dim=1)\n",
    "        logits = meta_model(feat)\n",
    "        _ = torch.sigmoid(logits)[0].cpu().numpy()\n",
    "\n",
    "        end = time.time()\n",
    "        latencies.append(end - start)\n",
    "\n",
    "latencies = np.array(latencies)\n",
    "\n",
    "print(f\"\\n=== Inference Latency Stats over {N_TRIALS} Random Samples ===\")\n",
    "print(f\"Median Latency:      {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"95th Percentile:     {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"99th Percentile:     {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Average Latency:     {np.mean(latencies) * 1000:.2f} ms\")\n",
    "print(f\"Throughput:          {N_TRIALS / np.sum(latencies):.2f} FPS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f09f5537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Batch Inference Timing (256 samples Ã— 43 batches) ===\n",
      "Median Latency:      19.35 ms\n",
      "95th Percentile:     21.26 ms\n",
      "99th Percentile:     63.34 ms\n",
      "Average Latency:     21.27 ms\n",
      "Throughput:          12036.27 FPS\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "num_batches = 50\n",
    "batch_size = 256\n",
    "\n",
    "# Cap total trials to available samples\n",
    "num_trials = min(num_batches * batch_size, len(test_manifest))\n",
    "sample_indices = rand_sample(range(len(test_manifest)), num_trials)\n",
    "\n",
    "# Group into batches\n",
    "batches = [sample_indices[i:i + batch_size] for i in range(0, len(sample_indices), batch_size)]\n",
    "\n",
    "# Warm-up batch\n",
    "warmup_batch = batches[0]\n",
    "embs, mas, ms, wavs = [], [], [], []\n",
    "for idx in warmup_batch:\n",
    "    sample = test_manifest.iloc[idx]\n",
    "    emb, ma, m, wav = preprocess_sample(sample)\n",
    "    embs.append(emb)\n",
    "    mas.append(ma)\n",
    "    ms.append(m)\n",
    "    wavs.append(wav)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = meta_model(torch.cat([\n",
    "        torch.sigmoid(emb_model(torch.cat(embs))),\n",
    "        torch.sigmoid(res_model(torch.cat(mas))),\n",
    "        torch.sigmoid(eff_model(torch.cat(ms))),\n",
    "        torch.sigmoid(raw_model(torch.cat(wavs)))\n",
    "    ], dim=1))\n",
    "\n",
    "# Timed batches\n",
    "batch_times = []\n",
    "for batch in batches[1:]:\n",
    "    embs, mas, ms, wavs = [], [], [], []\n",
    "    for idx in batch:\n",
    "        sample = test_manifest.iloc[idx]\n",
    "        emb, ma, m, wav = preprocess_sample(sample)\n",
    "        embs.append(emb)\n",
    "        mas.append(ma)\n",
    "        ms.append(m)\n",
    "        wavs.append(wav)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "\n",
    "        p1 = torch.sigmoid(emb_model(torch.cat(embs)))\n",
    "        p2 = torch.sigmoid(res_model(torch.cat(mas)))\n",
    "        p3 = torch.sigmoid(eff_model(torch.cat(ms)))\n",
    "        p4 = torch.sigmoid(raw_model(torch.cat(wavs)))\n",
    "\n",
    "        feat = torch.cat([p1, p2, p3, p4], dim=1)\n",
    "        _ = torch.sigmoid(meta_model(feat))\n",
    "\n",
    "        batch_times.append(time.time() - start_time)\n",
    "\n",
    "# Report\n",
    "batch_times = np.array(batch_times)\n",
    "print(f\"\\n=== Batch Inference Timing ({batch_size} samples Ã— {len(batch_times)} batches) ===\")\n",
    "print(f\"Median Latency:      {np.percentile(batch_times, 50) * 1000:.2f} ms\")\n",
    "print(f\"95th Percentile:     {np.percentile(batch_times, 95) * 1000:.2f} ms\")\n",
    "print(f\"99th Percentile:     {np.percentile(batch_times, 99) * 1000:.2f} ms\")\n",
    "print(f\"Average Latency:     {np.mean(batch_times) * 1000:.2f} ms\")\n",
    "print(f\"Throughput:          {(len(batch_times) * batch_size) / np.sum(batch_times):.2f} FPS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79ee258d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Performance Summary: BirdCLEF Fusion Model (EAGER) ===\n",
      "Accuracy: 59.61% (6570/11022 correct)\n",
      "Model Size on Disk (total): 197.51 MB\n",
      "Inference Latency (single sample, median): 26.70 ms\n",
      "Inference Latency (single sample, 95th percentile): 29.41 ms\n",
      "Inference Latency (single sample, 99th percentile): 38.15 ms\n",
      "Inference Throughput (single sample): 36.71 FPS\n",
      "Batch Throughput: 1184.37 FPS\n"
     ]
    }
   ],
   "source": [
    "#CELL 16.a\n",
    "print(\"\\n=== Final Performance Summary: BirdCLEF Fusion Model (EAGER) ===\")\n",
    "\n",
    "# Accuracy summary\n",
    "total = len(test_manifest)\n",
    "correct = int((y_pred == y_true).all(axis=1).sum())  # Subset accuracy match\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Accuracy: {accuracy:.2f}% ({correct}/{total} correct)\")\n",
    "\n",
    "# Model size summary\n",
    "model_ckpts = {\n",
    "    \"Embedding MLP\": CKPT_EMB,\n",
    "    \"ResNet50\": CKPT_RES,\n",
    "    \"EffNetB3 + LoRA\": CKPT_EFF,\n",
    "    \"RawAudioCNN\": CKPT_RAW,\n",
    "    \"Meta MLP\": CKPT_META\n",
    "}\n",
    "total_model_size = sum(os.path.getsize(p) for p in model_ckpts.values())\n",
    "print(f\"Model Size on Disk (total): {total_model_size / 1e6:.2f} MB\")\n",
    "\n",
    "# Latency summary (single sample)\n",
    "num_trials = len(latencies)\n",
    "print(f\"Inference Latency (single sample, median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 95th percentile): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 99th percentile): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Inference Throughput (single sample): {num_trials / np.sum(latencies):.2f} FPS\")\n",
    "\n",
    "# Batch throughput (from Cell 15)\n",
    "batch_fps = (len(batch_times) * batch_size) / np.sum(batch_times)\n",
    "print(f\"Batch Throughput: {batch_fps:.2f} FPS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7234f11-36c3-4403-aab6-5557b0b55b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Performance Summary: BirdCLEF Fusion Model (Compile) ===\n",
      "Accuracy: 59.61% (6570/11022 correct)\n",
      "Model Size on Disk (total): 197.51 MB\n",
      "Inference Latency (single sample, median): 20.91 ms\n",
      "Inference Latency (single sample, 95th percentile): 26.80 ms\n",
      "Inference Latency (single sample, 99th percentile): 28.82 ms\n",
      "Inference Throughput (single sample): 45.05 FPS\n",
      "Batch Throughput: 12036.27 FPS\n"
     ]
    }
   ],
   "source": [
    "#CELL 16.b\n",
    "print(\"\\n=== Final Performance Summary: BirdCLEF Fusion Model (Compile) ===\")\n",
    "\n",
    "# Accuracy summary\n",
    "total = len(test_manifest)\n",
    "correct = int((y_pred == y_true).all(axis=1).sum())  # Subset accuracy match\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Accuracy: {accuracy:.2f}% ({correct}/{total} correct)\")\n",
    "\n",
    "# Model size summary\n",
    "model_ckpts = {\n",
    "    \"Embedding MLP\": CKPT_EMB,\n",
    "    \"ResNet50\": CKPT_RES,\n",
    "    \"EffNetB3 + LoRA\": CKPT_EFF,\n",
    "    \"RawAudioCNN\": CKPT_RAW,\n",
    "    \"Meta MLP\": CKPT_META\n",
    "}\n",
    "total_model_size = sum(os.path.getsize(p) for p in model_ckpts.values())\n",
    "print(f\"Model Size on Disk (total): {total_model_size / 1e6:.2f} MB\")\n",
    "\n",
    "# Latency summary (single sample)\n",
    "num_trials = len(latencies)\n",
    "print(f\"Inference Latency (single sample, median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 95th percentile): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 99th percentile): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Inference Throughput (single sample): {num_trials / np.sum(latencies):.2f} FPS\")\n",
    "\n",
    "# Batch throughput (from Cell 15)\n",
    "batch_fps = (len(batch_times) * batch_size) / np.sum(batch_times)\n",
    "print(f\"Batch Throughput: {batch_fps:.2f} FPS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9358564-185b-4ca6-8648-bb6e8f32a16f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
